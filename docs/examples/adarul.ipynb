{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADARUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-16T14:02:18.606749736Z",
     "start_time": "2023-11-16T14:02:16.904318303Z"
    }
   },
   "outputs": [],
   "source": [
    "import rul_datasets\n",
    "import rul_adapt\n",
    "import pytorch_lightning as pl\n",
    "import omegaconf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce original configurations\n",
    "\n",
    "You can reproduce the original experiments by Ragab et al. by using the `get_adarul` constructor function.\n",
    "\n",
    "Additional kwargs for the trainer, e.g. accelerator=\"gpu\" for training on a GPU, can be passed to the function as a dictionary.\n",
    "The first dictionary is used for the pre-training trainer and the second one for the main trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T14:02:18.834709633Z",
     "start_time": "2023-11-16T14:02:18.608371867Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42, workers=True)  # makes is reproducible\n",
    "pre_training, main_training = rul_adapt.construct.get_adarul(\n",
    "    3, 1, {\"max_epochs\": 1}, {\"max_epochs\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns two tuples.\n",
    "The first contains everything needed for pre-training, the second everything needed for the main training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T14:03:51.238690150Z",
     "start_time": "2023-11-16T14:02:18.834977822Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name               | Type                     | Params\n",
      "----------------------------------------------------------------\n",
      "0 | train_loss         | MeanSquaredError         | 0     \n",
      "1 | val_loss           | MeanSquaredError         | 0     \n",
      "2 | test_loss          | MeanSquaredError         | 0     \n",
      "3 | evaluator          | AdaptionEvaluator        | 0     \n",
      "4 | _feature_extractor | ActivationDropoutWrapper | 62.5 K\n",
      "5 | _regressor         | FullyConnectedHead       | 6.3 K \n",
      "----------------------------------------------------------------\n",
      "68.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "68.7 K    Total params\n",
      "0.275     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8955d63606284c038e7c46f4ac715bf4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26e2b87dbf144b80824762f64fa5f380"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "625a04b373054424896fb9fde921e80f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "pre_dm, pre_approach, pre_trainer = pre_training\n",
    "pre_trainer.fit(pre_approach, pre_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-training, we can use the pre-trained networks to initialize the main training.\n",
    "The networks of the pre-training approach, i.e. `feature_extractor` and `regressor`, can be accessed as properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T14:04:03.035152695Z",
     "start_time": "2023-11-16T14:03:51.238597478Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                     | Type                     | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | gan_loss                 | BCEWithLogitsLoss        | 0     \n",
      "1 | evaluator                | AdaptionEvaluator        | 0     \n",
      "2 | _feature_extractor       | ActivationDropoutWrapper | 62.5 K\n",
      "3 | _regressor               | FullyConnectedHead       | 6.3 K \n",
      "4 | _domain_disc             | FullyConnectedHead       | 6.3 K \n",
      "5 | frozen_feature_extractor | ActivationDropoutWrapper | 62.5 K\n",
      "----------------------------------------------------------------------\n",
      "75.0 K    Trainable params\n",
      "62.5 K    Non-trainable params\n",
      "137 K     Total params\n",
      "0.550     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe5a6f05c6e9431094ba9afc33e674f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60e18cd8aeb3447e81275989fb8fc445"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6cd1bb27a4624a71975daca2658abec1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "455a25a04c3045f8886cc83e655a2b42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0             DataLoader 1\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "    test/source/rmse        24.635229110717773\n",
      "    test/source/score       1310.8724365234375\n",
      "    test/target/rmse                                 31.754472732543945\n",
      "    test/target/score                                 2988.716064453125\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'test/source/rmse/dataloader_idx_0': 24.635229110717773,\n  'test/source/score/dataloader_idx_0': 1310.8724365234375},\n {'test/target/rmse/dataloader_idx_1': 31.754472732543945,\n  'test/target/score/dataloader_idx_1': 2988.716064453125}]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm, approach, domain_disc, trainer = main_training\n",
    "approach.set_model(pre_approach.feature_extractor, pre_approach.regressor, domain_disc)\n",
    "trainer.fit(approach, dm)\n",
    "trainer.test(approach, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to see the hyperparameters, you can use the `get_adarul_config` function.\n",
    "This returns an `omegaconf.DictConfig` which you can modify.\n",
    "Afterwards, you can pass the config to `adarul_from_config` to receive the training-ready approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T14:04:03.083226295Z",
     "start_time": "2023-11-16T14:04:03.034510227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dm:\n",
      "  source:\n",
      "    _target_: rul_datasets.CmapssReader\n",
      "    fd: 3\n",
      "    window_size: 30\n",
      "    max_rul: 130\n",
      "    operation_condition_aware_scaling: true\n",
      "  target:\n",
      "    fd: 1\n",
      "    percent_broken: 1.0\n",
      "  batch_size: 10\n",
      "feature_extractor:\n",
      "  _convert_: all\n",
      "  _target_: rul_adapt.model.ActivationDropoutWrapper\n",
      "  wrapped:\n",
      "    _target_: rul_adapt.model.LstmExtractor\n",
      "    input_channels: 14\n",
      "    units:\n",
      "    - 32\n",
      "    - 32\n",
      "    - 32\n",
      "    bidirectional: true\n",
      "  dropout: 0.5\n",
      "regressor:\n",
      "  _convert_: all\n",
      "  _target_: rul_adapt.model.FullyConnectedHead\n",
      "  input_channels: 64\n",
      "  act_func_on_last_layer: false\n",
      "  units:\n",
      "  - 64\n",
      "  - 32\n",
      "  - 1\n",
      "  dropout: 0.5\n",
      "domain_disc:\n",
      "  _convert_: all\n",
      "  _target_: rul_adapt.model.FullyConnectedHead\n",
      "  input_channels: 64\n",
      "  act_func_on_last_layer: false\n",
      "  units:\n",
      "  - 64\n",
      "  - 32\n",
      "  - 1\n",
      "adarul_pre:\n",
      "  _target_: rul_adapt.approach.SupervisedApproach\n",
      "  lr: 0.0001\n",
      "  loss_type: mse\n",
      "  optim_type: adam\n",
      "  rul_scale: 130\n",
      "adarul:\n",
      "  _target_: rul_adapt.approach.AdaRulApproach\n",
      "  lr: 0.0001\n",
      "  max_rul: 130\n",
      "  num_disc_updates: 35\n",
      "  num_gen_updates: 1\n",
      "trainer_pre:\n",
      "  _target_: pytorch_lightning.Trainer\n",
      "  max_epochs: 5\n",
      "  gradient_clip_val: 1.0\n",
      "trainer:\n",
      "  _target_: pytorch_lightning.Trainer\n",
      "  max_epochs: 20\n",
      "  limit_train_batches: 36\n"
     ]
    }
   ],
   "source": [
    "three2one_config = rul_adapt.construct.get_adarul_config(3, 1)\n",
    "print(omegaconf.OmegaConf.to_yaml(three2one_config, resolve=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your own experiments\n",
    "\n",
    "You can use the ADARUL implementation to run your own experiments with different hyperparameters or on different datasets.\n",
    "Here we build an approach with an CNN feature extractor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T14:09:36.657065211Z",
     "start_time": "2023-11-16T14:09:30.286988058Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/tilman/Programming/rul-adapt/docs/examples/lightning_logs/version_24/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name               | Type               | Params\n",
      "----------------------------------------------------------\n",
      "0 | train_loss         | MeanSquaredError   | 0     \n",
      "1 | val_loss           | MeanSquaredError   | 0     \n",
      "2 | test_loss          | MeanSquaredError   | 0     \n",
      "3 | evaluator          | AdaptionEvaluator  | 0     \n",
      "4 | _feature_extractor | CnnExtractor       | 5.3 K \n",
      "5 | _regressor         | FullyConnectedHead | 81    \n",
      "----------------------------------------------------------\n",
      "5.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.4 K     Total params\n",
      "0.022     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c2a29c85a3449e3b0a6237034fe7477"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                     | Type               | Params\n",
      "----------------------------------------------------------------\n",
      "0 | gan_loss                 | BCEWithLogitsLoss  | 0     \n",
      "1 | evaluator                | AdaptionEvaluator  | 0     \n",
      "2 | _feature_extractor       | CnnExtractor       | 5.3 K \n",
      "3 | _regressor               | FullyConnectedHead | 81    \n",
      "4 | _domain_disc             | FullyConnectedHead | 81    \n",
      "5 | frozen_feature_extractor | CnnExtractor       | 5.3 K \n",
      "----------------------------------------------------------------\n",
      "5.5 K     Trainable params\n",
      "5.3 K     Non-trainable params\n",
      "10.8 K    Total params\n",
      "0.043     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "796994b72921489fb85490c2bdf876a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b8111c44ecd4039b2b94ff0cbd1b926"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e0491af3e2b42e1835d5b206a64d712"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41a242a13d4843359f25b3ec0c4993d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0             DataLoader 1\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "    test/source/rmse         65.23387908935547\n",
      "    test/source/score          70147.4921875\n",
      "    test/target/rmse                                  66.29417419433594\n",
      "    test/target/score                                   64386.578125\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'test/source/rmse/dataloader_idx_0': 65.23387908935547,\n  'test/source/score/dataloader_idx_0': 70147.4921875},\n {'test/target/rmse/dataloader_idx_1': 66.29417419433594,\n  'test/target/score/dataloader_idx_1': 64386.578125}]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = rul_datasets.CmapssReader(3)\n",
    "target = source.get_compatible(1, percent_broken=0.8)\n",
    "\n",
    "pre_dm = rul_datasets.RulDataModule(source, batch_size=32)\n",
    "dm = rul_datasets.DomainAdaptionDataModule(\n",
    "    pre_dm, rul_datasets.RulDataModule(target, batch_size=32),\n",
    ")\n",
    "\n",
    "feature_extractor = rul_adapt.model.CnnExtractor(\n",
    "    input_channels=14,\n",
    "    units=[16, 16, 16],\n",
    "    seq_len=30,\n",
    "    fc_units=8,\n",
    ")\n",
    "regressor = rul_adapt.model.FullyConnectedHead(\n",
    "    input_channels=8,\n",
    "    units=[8, 1],\n",
    "    act_func_on_last_layer=False,\n",
    ")\n",
    "domain_disc = rul_adapt.model.FullyConnectedHead(\n",
    "    input_channels=8,\n",
    "    units=[8, 1],\n",
    "    act_func_on_last_layer=False,\n",
    ")\n",
    "\n",
    "pre_approach = rul_adapt.approach.SupervisedApproach(\n",
    "    lr=0.001, loss_type=\"mse\", optim_type=\"adam\", rul_scale=source.max_rul\n",
    ")\n",
    "pre_approach.set_model(feature_extractor, regressor)\n",
    "pre_trainer = pl.Trainer(max_epochs=1)\n",
    "trainer.fit(pre_approach, pre_dm)\n",
    "\n",
    "approach = rul_adapt.approach.AdaRulApproach(\n",
    "    lr=0.001,\n",
    "    max_rul=source.max_rul,\n",
    "    num_disc_updates=35,\n",
    "    num_gen_updates=1,\n",
    ")\n",
    "approach.set_model(\n",
    "    pre_approach.feature_extractor, pre_approach.regressor, domain_disc\n",
    ")\n",
    "trainer = pl.Trainer(max_epochs=1)\n",
    "trainer.fit(approach, dm)\n",
    "trainer.test(approach, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
