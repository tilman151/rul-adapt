{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RUL Adapt","text":"<p>This library contains a collection of unsupervised domain adaption algorithms for RUL estimation. They are provided as LightningModules to be used in PyTorch Lightning.</p> <p>Currently, five approaches are implemented, including their original hyperparameters:</p> <ul> <li>LSTM-DANN by Da Costa et al. (2020)</li> <li>ADARUL by Ragab et al. (2020)</li> <li>LatentAlign by Zhang et al. (2021)</li> <li>TBiGRU by Cao et al. (2021)</li> <li>Consistency-DANN by Siahpour et al. (2022)</li> </ul> <p>Three approaches are implemented without their original hyperparameters:</p> <ul> <li>ConditionalDANN by Cheng et al. (2021)</li> <li>ConditionalMMD by Cheng et al. (2021)</li> <li>PseudoLabels as used by Wang et al. (2022)</li> </ul> <p>This includes the following general approaches adapted for RUL estimation:</p> <ul> <li>Domain Adaption Neural Networks (DANN) by Ganin et al. (2016)</li> <li>Multi-Kernel Maximum Mean Discrepancy (MMD) by Long et al. (2015)</li> </ul> <p>Each approach has an example notebook which can be found in the examples folder.</p>"},{"location":"#installation","title":"Installation","text":"<p>This library is pip-installable. Simply type:</p> <pre><code>pip install rul-adapt\n</code></pre>"},{"location":"#contribution","title":"Contribution","text":"<p>Contributions are always welcome. Whether you want to fix a bug, add a feature or a new approach, just open an issue and a PR.</p>"},{"location":"gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the code reference pages.\"\"\"\n</pre> \"\"\"Generate the code reference pages.\"\"\" In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>nav = mkdocs_gen_files.Nav()\n</pre> nav = mkdocs_gen_files.Nav() In\u00a0[\u00a0]: Copied! <pre>for path in sorted(Path(\"rul_adapt\").rglob(\"*.py\")):\n    module_path = path.with_suffix(\"\")\n    doc_path = path.with_suffix(\".md\")\n    full_doc_path = Path(\"api\", doc_path)\n\n    parts = list(module_path.parts)\n\n    if parts[-1] == \"__init__\":\n        parts = parts[:-1]\n        doc_path = doc_path.with_name(\"index.md\")\n        full_doc_path = full_doc_path.with_name(\"index.md\")\n    elif parts[-1] == \"__main__\":\n        continue\n\n    if len(parts) &gt; 1:\n        nav[parts[1:]] = doc_path.as_posix()\n\n        with mkdocs_gen_files.open(full_doc_path, mode=\"wt\") as fd:\n            identifier = \".\".join(parts)\n            print(\"::: \" + identifier, file=fd)\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, Path(\"../\") / path)\n</pre> for path in sorted(Path(\"rul_adapt\").rglob(\"*.py\")):     module_path = path.with_suffix(\"\")     doc_path = path.with_suffix(\".md\")     full_doc_path = Path(\"api\", doc_path)      parts = list(module_path.parts)      if parts[-1] == \"__init__\":         parts = parts[:-1]         doc_path = doc_path.with_name(\"index.md\")         full_doc_path = full_doc_path.with_name(\"index.md\")     elif parts[-1] == \"__main__\":         continue      if len(parts) &gt; 1:         nav[parts[1:]] = doc_path.as_posix()          with mkdocs_gen_files.open(full_doc_path, mode=\"wt\") as fd:             identifier = \".\".join(parts)             print(\"::: \" + identifier, file=fd)      mkdocs_gen_files.set_edit_path(full_doc_path, Path(\"../\") / path) In\u00a0[\u00a0]: Copied! <pre>with mkdocs_gen_files.open(\"api/SUMMARY.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> with mkdocs_gen_files.open(\"api/SUMMARY.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>approach<ul> <li>abstract</li> <li>adarul</li> <li>cnn_dann</li> <li>conditional</li> <li>consistency</li> <li>dann</li> <li>evaluation</li> <li>latent_align</li> <li>mmd</li> <li>pseudo_labels</li> <li>supervised</li> <li>tbigru</li> </ul> </li> <li>construct<ul> <li>adarul<ul> <li>functional</li> </ul> </li> <li>cnn_dann<ul> <li>functional</li> </ul> </li> <li>consistency<ul> <li>functional</li> </ul> </li> <li>latent_align<ul> <li>functional</li> </ul> </li> <li>lstm_dann<ul> <li>functional</li> </ul> </li> <li>tbigru<ul> <li>functional</li> </ul> </li> </ul> </li> <li>loss<ul> <li>adaption</li> <li>alignment</li> <li>conditional</li> <li>rul</li> <li>utils</li> </ul> </li> <li>model<ul> <li>cnn</li> <li>head</li> <li>rnn</li> <li>two_stage</li> <li>wrapper</li> </ul> </li> <li>utils</li> </ul>"},{"location":"api/rul_adapt/utils/","title":"utils","text":""},{"location":"api/rul_adapt/utils/#rul_adapt.utils.OptimizerFactory","title":"<code>OptimizerFactory</code>","text":"<p>Factory for creating optimizers and schedulers.</p> <p>After initialization, the factory can be called to create an optimizer with an optional scheduler.</p>"},{"location":"api/rul_adapt/utils/#rul_adapt.utils.OptimizerFactory.__call__","title":"<code>__call__(parameters)</code>","text":"<p>Create an optimizer with an optional scheduler for the given parameters.</p> <p>The object returned by this method is a lightning optimizer config and can be the return value of <code>configure_optimizers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Iterable[Parameter]</code> <p>The model parameters to optimize.</p> required <p>Returns:</p> Type Description <code>OptimizerLRSchedulerConfig</code> <p>A lightning optimizer config.</p>"},{"location":"api/rul_adapt/utils/#rul_adapt.utils.OptimizerFactory.__init__","title":"<code>__init__(optim_type='adam', lr=0.001, scheduler_type=None, **kwargs)</code>","text":"<p>Create a new factory to efficiently create optimizers and schedulers.</p> <p>The factory creates an optimizer of the specified <code>optim_type</code> and adds an optional scheduler of the specified <code>scheduler_type</code>. Additional keyword arguments for the optimizer can be passed by adding the 'optim_' prefix and for the scheduler by adding the 'scheduler_' prefix. The factory will ignore any other keyword arguments.</p> <p>Available optimizers are 'adam', 'sgd' and 'rmsprop'. Available schedulers are 'step', 'cosine', 'linear' and 'lambda'.</p> <p>Parameters:</p> Name Type Description Default <code>optim_type</code> <code>str</code> <p>The type of optimizer to create.</p> <code>'adam'</code> <code>lr</code> <code>float</code> <p>The learning rate to use.</p> <code>0.001</code> <code>scheduler_type</code> <code>Optional[str]</code> <p>The optional type of scheduler to create.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the optimizer and scheduler.</p> <code>{}</code>"},{"location":"api/rul_adapt/utils/#rul_adapt.utils.get_loss","title":"<code>get_loss(loss_type)</code>","text":"<p>Get a loss instance by specifying a string.</p>"},{"location":"api/rul_adapt/utils/#rul_adapt.utils.pairwise","title":"<code>pairwise(iterable)</code>","text":"<p>s -&gt; (s0,s1), (s1,s2), (s2, s3), ...</p>"},{"location":"api/rul_adapt/utils/#rul_adapt.utils.str2callable","title":"<code>str2callable(cls, restriction='')</code>","text":"<p>Dynamically import a callable from a string.</p>"},{"location":"api/rul_adapt/approach/","title":"approach","text":""},{"location":"api/rul_adapt/approach/abstract/","title":"abstract","text":"<p>A module for the abstract base class of all approaches.</p>"},{"location":"api/rul_adapt/approach/abstract/#rul_adapt.approach.abstract.AdaptionApproach","title":"<code>AdaptionApproach</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>This abstract class is the base of all adaption approaches.</p> <p>It defines that there needs to be a <code>feature_extractor</code>, a <code>regressor</code>. These members can be accessed via read-only properties. The <code>feature_extractor</code> and <code>regressor</code> are trainable neural networks.</p> <p>All child classes are supposed to implement their own constructors. The <code>feature_extractor</code> and <code>regressor</code> should explicitly not be arguments of the constructor and should be set by calling set_model. This way, the approach can be initialized with all hyperparameters first and afterward supplied with the networks. This is useful for initializing the networks with pre-trained weights.</p> <p>Because models are constructed outside the approach, the default checkpointing mechanism of PyTorch Lightning fails to load checkpoints of AdaptionApproaches. We extended the checkpointing mechanism by implementing the <code>on_save_checkpoint</code> and <code>on_load_checkpoint</code> callbacks to make it work. If a subclass uses an additional model, besides feature extractor and regressor, that is not initialized in the constructor, the subclass needs to implement the <code>CHECKPOINT_MODELS</code> class variable. This variable is a list of model names to be included in the checkpoint. For example, if your approach has an additional model <code>self._domain_disc</code>, the <code>CHECKPOINT_MODELS</code> variable should be set to <code>['_domain_disc']</code>. Otherwise, loading a checkpoint of this approach will fail.</p>"},{"location":"api/rul_adapt/approach/abstract/#rul_adapt.approach.abstract.AdaptionApproach.feature_extractor","title":"<code>feature_extractor: nn.Module</code>  <code>property</code>","text":"<p>The feature extraction network.</p>"},{"location":"api/rul_adapt/approach/abstract/#rul_adapt.approach.abstract.AdaptionApproach.regressor","title":"<code>regressor: nn.Module</code>  <code>property</code>","text":"<p>The RUL regression network.</p>"},{"location":"api/rul_adapt/approach/abstract/#rul_adapt.approach.abstract.AdaptionApproach.set_model","title":"<code>set_model(feature_extractor, regressor, *args, **kwargs)</code>","text":"<p>Set the feature extractor and regressor for this approach.</p> <p>Child classes can override this function to add additional models to an approach. The <code>args</code> and <code>kwargs</code> making this possible are ignored in this function.</p> <p>Parameters:</p> Name Type Description Default <code>feature_extractor</code> <code>Module</code> <p>The feature extraction network.</p> required <code>regressor</code> <code>Module</code> <p>The RUL regression network.</p> required"},{"location":"api/rul_adapt/approach/adarul/","title":"adarul","text":"<p>The Adversarial Domain Adaption for Remaining Useful Life (ADARUL) approach pre-trains a feature extractor and regressor on the source domain in a supervised fashion. Afterwards the feature extractor is adapted by feeding it the target features and training it adversarial against a domain discriminator. The discriminator is trained to distinguish the source features fed to a frozen version of the pre-trained feature extractor and the target features fed to the adapted feature extractor.</p> <p>The approach was first introduced by Ragab et al. and evaluated on the CMAPSS dataset.</p>"},{"location":"api/rul_adapt/approach/adarul/#rul_adapt.approach.adarul.AdaRulApproach","title":"<code>AdaRulApproach</code>","text":"<p>             Bases: <code>AdaptionApproach</code></p> <p>The ADARUL approach uses a GAN setup to adapt a feature extractor. This approach should only be used with a pre-trained feature extractor.</p> <p>The regressor and domain discriminator need the same number of input units as the feature extractor has output units. The discriminator is not allowed to have an activation function on its last layer for it to work with its loss.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt import model\n&gt;&gt;&gt; from rul_adapt import approach\n&gt;&gt;&gt; feat_ex = model.CnnExtractor(1, [16, 16, 1], 10, fc_units=16)\n&gt;&gt;&gt; reg = model.FullyConnectedHead(16, [1])\n&gt;&gt;&gt; disc = model.FullyConnectedHead(16, [8, 1], act_func_on_last_layer=False)\n&gt;&gt;&gt; pre = approach.SupervisedApproach(\"mse\", 125, lr=0.001)\n&gt;&gt;&gt; pre.set_model(feat_ex, reg)\n&gt;&gt;&gt; main = approach.AdaRulApproach(5, 1, 125, lr=0.001)\n&gt;&gt;&gt; main.set_model(pre.feature_extractor, pre.regressor, disc)\n</code></pre>"},{"location":"api/rul_adapt/approach/adarul/#rul_adapt.approach.adarul.AdaRulApproach.domain_disc","title":"<code>domain_disc</code>  <code>property</code>","text":"<p>The domain discriminator network.</p>"},{"location":"api/rul_adapt/approach/adarul/#rul_adapt.approach.adarul.AdaRulApproach.__init__","title":"<code>__init__(num_disc_updates, num_gen_updates, max_rul=None, rul_score_mode='phm08', evaluate_degraded_only=False, **optim_kwargs)</code>","text":"<p>Create a new ADARUL approach.</p> <p>The discriminator is first trained for <code>num_disc_updates</code> batches. Afterward, the feature extractor (generator) is trained for <code>num_gen_updates</code>. This cycle repeats until the epoch ends.</p> <p>The regressor is supposed to output a value between [0, 1] which is then scaled by <code>max_rul</code>.</p> <p>For more information about the possible optimizer keyword arguments, see here.</p> <p>Parameters:</p> Name Type Description Default <code>max_rul</code> <code>Optional[int]</code> <p>Maximum RUL value of the training data.</p> <code>None</code> <code>num_disc_updates</code> <code>int</code> <p>Number of batches to update discriminator with.</p> required <code>num_gen_updates</code> <code>int</code> <p>Number of batches to update generator with.</p> required <code>rul_score_mode</code> <code>Literal['phm08', 'phm12']</code> <p>The mode for the val and test RUL score, either 'phm08'             or 'phm12'.</p> <code>'phm08'</code> <code>evaluate_degraded_only</code> <code>bool</code> <p>Whether to only evaluate the RUL score on degraded                     samples.</p> <code>False</code> <code>**optim_kwargs</code> <code>Any</code> <p>Keyword arguments for the optimizer, e.g. learning rate.</p> <code>{}</code>"},{"location":"api/rul_adapt/approach/adarul/#rul_adapt.approach.adarul.AdaRulApproach.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure an optimizer for the generator and discriminator respectively.</p>"},{"location":"api/rul_adapt/approach/adarul/#rul_adapt.approach.adarul.AdaRulApproach.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict the RUL values for a batch of input features.</p>"},{"location":"api/rul_adapt/approach/adarul/#rul_adapt.approach.adarul.AdaRulApproach.set_model","title":"<code>set_model(feature_extractor, regressor, domain_disc=None, *args, **kwargs)</code>","text":"<p>Set the feature extractor, regressor and domain discriminator for this approach. The discriminator is not allowed to have an activation function on its last layer and needs to use only a single output neuron.</p> <p>A frozen copy of the feature extractor is produced to be used for the real samples fed to the discriminator. The feature extractor should, therefore, be pre-trained.</p> <p>Parameters:</p> Name Type Description Default <code>feature_extractor</code> <code>Module</code> <p>The feature extraction network.</p> required <code>regressor</code> <code>Module</code> <p>The RUL regression network.</p> required <code>domain_disc</code> <code>Optional[Module]</code> <p>The domain discriminator network.</p> <code>None</code>"},{"location":"api/rul_adapt/approach/adarul/#rul_adapt.approach.adarul.AdaRulApproach.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one test step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for <code>dataloader_idx</code> zero are assumed to be from the source domain and for <code>dataloader_idx</code> one from the target domain. The metrics are written to the configured logger under the prefix <code>test</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/adarul/#rul_adapt.approach.adarul.AdaRulApproach.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Execute one training step.</p> <p>The <code>batch</code> argument is a list of three tensors representing the source features, source labels and target features. Each iteration either only the discriminator or only the generator is trained. The respective loss is logged.</p> <p>The real samples are source features passed though the frozen version of the feature extractor. The fake samples are the target features passed through the adapted feature extractor. The discriminator predicts if a sample came from the source or target domain.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list of a source feature, source label and target feature tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:     Either the discriminator or generator loss.</p>"},{"location":"api/rul_adapt/approach/adarul/#rul_adapt.approach.adarul.AdaRulApproach.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one validation step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for <code>dataloader_idx</code> zero are assumed to be from the source domain and for <code>dataloader_idx</code> one from the target domain. The metrics are written to the configured logger under the prefix <code>val</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/cnn_dann/","title":"cnn_dann","text":""},{"location":"api/rul_adapt/approach/cnn_dann/#rul_adapt.approach.cnn_dann.init_weights","title":"<code>init_weights(feature_extractor, regressor)</code>","text":"<p>Initialize the weights of the feature extractor and regressor in-place.</p> <p>For the weight matrices the Xavier uniform initialization is used. The biases are initialized to zero. This function works only for the networks returned by a call to rul_adapt.construct.get_cnn_dann.</p> <p>Parameters:</p> Name Type Description Default <code>feature_extractor</code> <code>CnnExtractor</code> <p>The feature extractor network to be initialized.</p> required <code>regressor</code> <code>DropoutPrefix</code> <p>The regressor network to be initialized.</p> required"},{"location":"api/rul_adapt/approach/conditional/","title":"conditional","text":"<p>The Conditional Adaption approaches are derived from the [MMD] [ rul_adapt.approach.mmd] and DANN approaches. They apply their respective adaption loss not only to the whole data but also separately to subsets of the data with a ConditionalAdaptionLoss. Fuzzy sets with rectangular membership functions define these subsets.</p> <p>Both variants were introduced by Cheng et al. in 2021.</p>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalDannApproach","title":"<code>ConditionalDannApproach</code>","text":"<p>             Bases: <code>AdaptionApproach</code></p> <p>The conditional DANN approach uses a marginal and several conditional domain discriminators. The features are produced by a shared feature extractor. The loss in the domain discriminators is binary cross-entropy.</p> <p>The regressor and domain discriminators need the same number of input units as the feature extractor has output units. The discriminators are not allowed to have an activation function on their last layer and need to use only a single output neuron because BCEWithLogitsLoss is used.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt import model\n&gt;&gt;&gt; from rul_adapt import approach\n&gt;&gt;&gt; feat_ex = model.CnnExtractor(1, [16, 16, 1], 10, fc_units=16)\n&gt;&gt;&gt; reg = model.FullyConnectedHead(16, [1])\n&gt;&gt;&gt; disc = model.FullyConnectedHead(16, [8, 1], act_func_on_last_layer=False)\n&gt;&gt;&gt; cond_dann = approach.ConditionalDannApproach(1.0, 0.5, [(0, 1)])\n&gt;&gt;&gt; cond_dann.set_model(feat_ex, reg, disc)\n</code></pre>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalDannApproach.__init__","title":"<code>__init__(dann_factor, dynamic_adaptive_factor, fuzzy_sets, loss_type='mae', rul_score_mode='phm08', evaluate_degraded_only=False, **optim_kwargs)</code>","text":"<p>Create a new conditional DANN approach.</p> <p>The strength of the domain discriminator's influence on the feature extractor is controlled by the <code>dann_factor</code>. The higher it is, the stronger the influence. The <code>dynamic_adaptive_factor</code> controls the balance between the marginal and conditional DANN loss.</p> <p>The domain discriminator is set by the <code>set_model</code> function together with the feature extractor and regressor. For more information, see the approach module page.</p> <p>For more information about the possible optimizer keyword arguments, see here.</p> <p>Parameters:</p> Name Type Description Default <code>dann_factor</code> <code>float</code> <p>Strength of the domain DANN loss.</p> required <code>dynamic_adaptive_factor</code> <code>float</code> <p>Balance between the marginal and conditional DANN                      loss.</p> required <code>fuzzy_sets</code> <code>List[Tuple[float, float]]</code> <p>Fuzzy sets for the conditional DANN loss.</p> required <code>loss_type</code> <code>Literal['mse', 'rmse', 'mae']</code> <p>The type of regression loss, either 'mse', 'rmse' or 'mae'.</p> <code>'mae'</code> <code>rul_score_mode</code> <code>Literal['phm08', 'phm12']</code> <p>The mode for the val and test RUL score, either 'phm08'             or 'phm12'.</p> <code>'phm08'</code> <code>**optim_kwargs</code> <code>Any</code> <p>Keyword arguments for the optimizer, e.g. learning rate.</p> <code>{}</code>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalDannApproach.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure an Adam optimizer.</p>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalDannApproach.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict the RUL values for a batch of input features.</p>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalDannApproach.set_model","title":"<code>set_model(feature_extractor, regressor, domain_disc=None, *args, **kwargs)</code>","text":"<p>Set the feature extractor, regressor, and domain discriminator for this approach.</p> <p>The discriminator is not allowed to have an activation function on its last layer and needs to use only a single output neuron. It is wrapped by a DomainAdversarialLoss.</p> <p>A copy of the discriminator is used for each conditional loss governing a fuzzy set.</p> <p>Parameters:</p> Name Type Description Default <code>feature_extractor</code> <code>Module</code> <p>The feature extraction network.</p> required <code>regressor</code> <code>Module</code> <p>The RUL regression network.</p> required <code>domain_disc</code> <code>Optional[Module]</code> <p>The domain discriminator network.          Copied for each fuzzy set.</p> <code>None</code>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalDannApproach.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one test step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>test</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalDannApproach.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Execute one training step.</p> <p>The <code>batch</code> argument is a list of three tensors representing the source features, source labels and target features. Both types of features are fed to the feature extractor. Then the regression loss for the source domain, the MMD loss and the conditional MMD loss are computed. The regression, MMD, conditional MMD and combined loss are logged.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list of a source feature, source label and target feature tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:     The combined loss.</p>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalDannApproach.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one validation step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>val</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalMmdApproach","title":"<code>ConditionalMmdApproach</code>","text":"<p>             Bases: <code>AdaptionApproach</code></p> <p>The conditional MMD uses a combination of a marginal and conditional MML loss to adapt a feature extractor to be used with the source regressor.</p> <p>The regressor needs the same number of input units as the feature extractor has output units.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt import model\n&gt;&gt;&gt; from rul_adapt import approach\n&gt;&gt;&gt; feat_ex = model.CnnExtractor(1, [16, 16, 1], 10, fc_units=16)\n&gt;&gt;&gt; reg = model.FullyConnectedHead(16, [1])\n&gt;&gt;&gt; cond_mmd = approach.ConditionalMmdApproach(0.01, 5, 0.5, [(0, 1)])\n&gt;&gt;&gt; cond_mmd.set_model(feat_ex, reg)\n</code></pre>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalMmdApproach.__init__","title":"<code>__init__(mmd_factor, num_mmd_kernels, dynamic_adaptive_factor, fuzzy_sets, loss_type='mae', rul_score_mode='phm08', evaluate_degraded_only=False, **optim_kwargs)</code>","text":"<p>Create a new conditional MMD approach.</p> <p>The strength of the influence of the MMD loss on the feature extractor is controlled by the <code>mmd_factor</code>. The higher it is, the stronger the influence. The dynamic adaptive factor controls the balance between the marginal MMD and conditional MMD losses.</p> <p>For more information about the possible optimizer keyword arguments, see here.</p> <p>Parameters:</p> Name Type Description Default <code>mmd_factor</code> <code>float</code> <p>The strength of the MMD loss' influence.</p> required <code>num_mmd_kernels</code> <code>int</code> <p>The number of kernels for the MMD loss.</p> required <code>dynamic_adaptive_factor</code> <code>float</code> <p>The balance between marginal and conditional MMD.</p> required <code>fuzzy_sets</code> <code>List[Tuple[float, float]]</code> <p>The fuzzy sets for the conditional MMD loss.</p> required <code>loss_type</code> <code>Literal['mse', 'rmse', 'mae']</code> <p>The type of regression loss, either 'mse', 'rmse' or 'mae'.</p> <code>'mae'</code> <code>rul_score_mode</code> <code>Literal['phm08', 'phm12']</code> <p>The mode for the val and test RUL score, either 'phm08'             or 'phm12'.</p> <code>'phm08'</code> <code>evaluate_degraded_only</code> <code>bool</code> <p>Whether to only evaluate the RUL score on degraded                     samples.</p> <code>False</code> <code>**optim_kwargs</code> <code>Any</code> <p>Keyword arguments for the optimizer, e.g. learning rate.</p> <code>{}</code>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalMmdApproach.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure an Adam optimizer.</p>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalMmdApproach.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict the RUL values for a batch of input features.</p>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalMmdApproach.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one test step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>test</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalMmdApproach.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Execute one training step.</p> <p>The <code>batch</code> argument is a list of three tensors representing the source features, source labels and target features. Both types of features are fed to the feature extractor. Then the regression loss for the source domain, the MMD loss and the conditional MMD loss are computed. The regression, MMD, conditional MMD and combined loss are logged.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list of a source feature, source label and target feature tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:     The combined loss.</p>"},{"location":"api/rul_adapt/approach/conditional/#rul_adapt.approach.conditional.ConditionalMmdApproach.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one validation step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>val</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/consistency/","title":"consistency","text":"<p>The Consistency DANN approach uses a consistency loss in tandem with a DANN loss. First, the network is pre-trained in a supervised fashion on the source domain. The pre-trained weights are then used to initialize the main training stage. The consistency loss penalizes the weights of the feature extractor moving away from the pre-trained version. This way the feature extractor weights stay close to the pre-trained weights.</p> <pre><code># pre-training stage\n\nSource --&gt; PreFeatEx --&gt; Source Feats --&gt; Regressor  --&gt; RUL Prediction\n\n# main training stage\n\n   ------- PreTrainFeatEx --&gt; PreTrain Source Feats --&gt; Consistency Loss\n   |\n   |\nSource --&gt; FeatEx --&gt; Source Feats -----------&gt; Regressor  --&gt; RUL Prediction\n        ^         |                 |\n        |         |                 v\nTarget --         --&gt; Target Feats --&gt;  GRL --&gt; DomainDisc --&gt; Domain Prediction\n</code></pre> <p>This version of DANN was introduced by Siahpour et al..</p>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.ConsistencyApproach","title":"<code>ConsistencyApproach</code>","text":"<p>             Bases: <code>AdaptionApproach</code></p> <p>The Consistency DANN approach introduces a consistency loss that keeps the weights of the feature extractor close to the ones of a pre-trained version. This approach should only be used with a pre-trained feature extractor. Otherwise, the consistency loss would serve no purpose.</p> <p>The regressor and domain discriminator need the same number of input units as the feature extractor has output units. The discriminator is not allowed to have an activation function on its last layer for it to work with the DANN loss.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt import model\n&gt;&gt;&gt; from rul_adapt import approach\n&gt;&gt;&gt; feat_ex = model.CnnExtractor(1, [16, 16, 1], 10, fc_units=16)\n&gt;&gt;&gt; reg = model.FullyConnectedHead(16, [1])\n&gt;&gt;&gt; disc = model.FullyConnectedHead(16, [8, 1], act_func_on_last_layer=False)\n&gt;&gt;&gt; pre = approach.SupervisedApproach(\"rmse\")\n&gt;&gt;&gt; pre.set_model(feat_ex, reg, disc)\n&gt;&gt;&gt; main = approach.ConsistencyApproach(1.0, 100)\n&gt;&gt;&gt; main.set_model(pre.feature_extractor, pre.regressor, disc)\n</code></pre>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.ConsistencyApproach.dann_factor","title":"<code>dann_factor</code>  <code>property</code>","text":"<p>Return the influency of the DANN loss based on the current epoch.</p> <p>It is calculated as: <code>2 / (1 + math.exp(-10 * current_epoch / max_epochs)) - 1</code></p>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.ConsistencyApproach.domain_disc","title":"<code>domain_disc</code>  <code>property</code>","text":"<p>The domain discriminator network.</p>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.ConsistencyApproach.__init__","title":"<code>__init__(consistency_factor, max_epochs, loss_type='rmse', rul_score_mode='phm08', evaluate_degraded_only=False, **optim_kwargs)</code>","text":"<p>Create a new consistency DANN approach.</p> <p>The consistency factor is the strength of the consistency loss' influence. The influence of the DANN loss is increased during the training process. It starts at zero and reaches one at <code>max_epochs</code>.</p> <p>The domain discriminator is set by the <code>set_model</code> function together with the feature extractor and regressor. For more information, see the approach module page.</p> <p>For more information about the possible optimizer keyword arguments, see here.</p> <p>Parameters:</p> Name Type Description Default <code>consistency_factor</code> <code>float</code> <p>The strength of the consistency loss' influence.</p> required <code>max_epochs</code> <code>int</code> <p>The number of epochs after which the DANN loss' influence is         maximal.</p> required <code>loss_type</code> <code>Literal['mse', 'mae', 'rmse']</code> <p>The type of regression loss, either 'mse', 'rmse' or 'mae'.</p> <code>'rmse'</code> <code>rul_score_mode</code> <code>Literal['phm08', 'phm12']</code> <p>The mode for the val and test RUL score, either 'phm08'             or 'phm12'.</p> <code>'phm08'</code> <code>evaluate_degraded_only</code> <code>bool</code> <p>Whether to only evaluate the RUL score on degraded                     samples.</p> <code>False</code> <code>**optim_kwargs</code> <code>Any</code> <p>Keyword arguments for the optimizer, e.g. learning rate.</p> <code>{}</code>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.ConsistencyApproach.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure an optimizer to train the feature extractor, regressor and domain discriminator.</p>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.ConsistencyApproach.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict the RUL values for a batch of input features.</p>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.ConsistencyApproach.set_model","title":"<code>set_model(feature_extractor, regressor, domain_disc=None, *args, **kwargs)</code>","text":"<p>Set the feature extractor, regressor and domain discriminator for this approach. The discriminator is not allowed to have an activation function on its last layer and needs to use only a single output neuron. It is wrapped by a DomainAdversarialLoss.</p> <p>A frozen copy of the feature extractor is produced to be used for the consistency loss. The feature extractor should, therefore, be pre-trained.</p> <p>Parameters:</p> Name Type Description Default <code>feature_extractor</code> <code>Module</code> <p>The pre-trained feature extraction network.</p> required <code>regressor</code> <code>Module</code> <p>The optionally pre-trained RUL regression network.</p> required <code>domain_disc</code> <code>Optional[Module]</code> <p>The domain discriminator network.</p> <code>None</code>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.ConsistencyApproach.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one test step. The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>test</code>. Args:     batch: A list containing a feature and a label tensor.     batch_idx: The index of the current batch.     dataloader_idx: The index of the current dataloader (0: source, 1: target).</p>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.ConsistencyApproach.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Execute one training step.</p> <p>The <code>batch</code> argument is a list of three tensors representing the source features, source labels and target features. Both types of features are fed to the feature extractor. Then the regression loss for the source domain and the DANN loss between domains is computed. Afterwards the consistency loss is calculated. The regression, DANN, consistency and combined loss are logged.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list of a source feature, source label and target feature tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:     The combined loss.</p>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.ConsistencyApproach.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one validation step. The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>val</code>. Args:     batch: A list containing a feature and a label tensor.     batch_idx: The index of the current batch.     dataloader_idx: The index of the current dataloader (0: source, 1: target).</p>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.StdExtractor","title":"<code>StdExtractor</code>","text":"<p>This extractor can be used to extract the per-feature standard deviation from windows of data. It can be used to pre-process datasets like FEMTO and XJTU-SY with the help of the RulDataModule.</p> <p>Examples:</p> <p>Extract the std of the horizontal acceleration and produce windows of size 30. <pre><code>&gt;&gt;&gt; import rul_datasets\n&gt;&gt;&gt; import rul_adapt\n&gt;&gt;&gt; fd1 = rul_datasets.XjtuSyReader(fd=1)\n&gt;&gt;&gt; extractor = rul_adapt.approach.consistency.StdExtractor([0])\n&gt;&gt;&gt; dm = rul_datasets.RulDataModule(fd1, 32, extractor, window_size=30)\n</code></pre></p>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.StdExtractor.__call__","title":"<code>__call__(inputs, targets)</code>","text":"<p>Extract features from the input data.</p> <p>The input is expected to have a shape of <code>[num_windows, window_size, num_features]</code>. The output will have a shape of <code>[num_windows, len(self.channels)]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>ndarray</code> <p>The input data.</p> required <p>Returns:     The features extracted from the input data.</p>"},{"location":"api/rul_adapt/approach/consistency/#rul_adapt.approach.consistency.StdExtractor.__init__","title":"<code>__init__(channels)</code>","text":"<p>Create a new feature extractor for standard deviations.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>List[int]</code> <p>The list of channel indices to extract features from.</p> required"},{"location":"api/rul_adapt/approach/dann/","title":"dann","text":"<p>The Domain Adversarial Neural Network (DANN) approach uses a domain discriminator trained on distinguishing the source and target features produced by a shared feature extractor. A Gradient Reversal Layer (GRL) is used to train the feature extractor on making its source and target outputs indistinguishable.</p> <pre><code>Source --&gt; FeatEx --&gt; Source Feats -----------&gt; Regressor  --&gt; RUL Prediction\n        ^         |                 |\n        |         |                 v\nTarget --         --&gt; Target Feats --&gt;  GRL --&gt; DomainDisc --&gt; Domain Prediction\n</code></pre> <p>It was originally introduced by Ganin et al. for image classification.</p> Used In <ul> <li>da Costa et al. (2020). Remaining useful lifetime prediction via deep domain adaptation. Reliability Engineering &amp; System Safety, 195, 106682. 10.1016/J.RESS.2019.106682</li> <li>Krokotsch et al. (2020). A Novel Evaluation Framework for Unsupervised Domain Adaption on Remaining Useful Lifetime Estimation. 2020 IEEE International Conference on Prognostics and Health Management (ICPHM). 10.1109/ICPHM49022.2020.9187058</li> </ul>"},{"location":"api/rul_adapt/approach/dann/#rul_adapt.approach.dann.DannApproach","title":"<code>DannApproach</code>","text":"<p>             Bases: <code>AdaptionApproach</code></p> <p>The DANN approach introduces a domain discriminator that is trained on distinguishing source and target features as a binary classification problem. The features are produced by a shared feature extractor. The loss in the domain discriminator is binary cross-entropy.</p> <p>The regressor and domain discriminator need the same number of input units as the feature extractor has output units. The discriminator is not allowed to have an activation function on its last layer and needs to use only a single output neuron because BCEWithLogitsLoss is used.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt import model\n&gt;&gt;&gt; from rul_adapt import approach\n&gt;&gt;&gt; feat_ex = model.CnnExtractor(1, [16, 16, 1], 10, fc_units=16)\n&gt;&gt;&gt; reg = model.FullyConnectedHead(16, [1])\n&gt;&gt;&gt; disc = model.FullyConnectedHead(16, [8, 1], act_func_on_last_layer=False)\n&gt;&gt;&gt; dann = approach.DannApproach(1.0)\n&gt;&gt;&gt; dann.set_model(feat_ex, reg, disc)\n</code></pre>"},{"location":"api/rul_adapt/approach/dann/#rul_adapt.approach.dann.DannApproach.domain_disc","title":"<code>domain_disc</code>  <code>property</code>","text":"<p>The domain discriminator network.</p>"},{"location":"api/rul_adapt/approach/dann/#rul_adapt.approach.dann.DannApproach.__init__","title":"<code>__init__(dann_factor, loss_type='mae', rul_score_mode='phm08', evaluate_degraded_only=False, **optim_kwargs)</code>","text":"<p>Create a new DANN approach.</p> <p>The strength of the domain discriminator's influence on the feature extractor is controlled by the <code>dann_factor</code>. The higher it is, the stronger the influence.</p> <p>Possible options for the regression loss are <code>mae</code>, <code>mse</code> and <code>rmse</code>.</p> <p>The domain discriminator is set by the <code>set_model</code> function together with the feature extractor and regressor. For more information, see the approach module page.</p> <p>For more information about the possible optimizer keyword arguments, see here.</p> <p>Parameters:</p> Name Type Description Default <code>dann_factor</code> <code>float</code> <p>Strength of the domain DANN loss.</p> required <code>loss_type</code> <code>Literal['mae', 'mse', 'rmse']</code> <p>Type of regression loss.</p> <code>'mae'</code> <code>rul_score_mode</code> <code>Literal['phm08', 'phm12']</code> <p>The mode for the val and test RUL score, either 'phm08'             or 'phm12'.</p> <code>'phm08'</code> <code>evaluate_degraded_only</code> <code>bool</code> <p>Whether to only evaluate the RUL score on degraded                     samples.</p> <code>False</code> <code>**optim_kwargs</code> <code>Any</code> <p>Keyword arguments for the optimizer, e.g. learning rate.</p> <code>{}</code>"},{"location":"api/rul_adapt/approach/dann/#rul_adapt.approach.dann.DannApproach.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure an optimizer for the whole model.</p>"},{"location":"api/rul_adapt/approach/dann/#rul_adapt.approach.dann.DannApproach.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict the RUL values for a batch of input features.</p>"},{"location":"api/rul_adapt/approach/dann/#rul_adapt.approach.dann.DannApproach.set_model","title":"<code>set_model(feature_extractor, regressor, domain_disc=None, *args, **kwargs)</code>","text":"<p>Set the feature extractor, regressor, and domain discriminator for this approach.</p> <p>The discriminator is not allowed to have an activation function on its last layer and needs to use only a single output neuron. It is wrapped by a DomainAdversarialLoss.</p> <p>Parameters:</p> Name Type Description Default <code>feature_extractor</code> <code>Module</code> <p>The feature extraction network.</p> required <code>regressor</code> <code>Module</code> <p>The RUL regression network.</p> required <code>domain_disc</code> <code>Optional[Module]</code> <p>The domain discriminator network.</p> <code>None</code>"},{"location":"api/rul_adapt/approach/dann/#rul_adapt.approach.dann.DannApproach.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one test step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>test</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/dann/#rul_adapt.approach.dann.DannApproach.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Execute one training step.</p> <p>The <code>batch</code> argument is a list of three tensors representing the source features, source labels and target features. Both types of features are fed to the feature extractor. Then the regression loss for the source domain and the DANN loss between domains is computed. The regression, DANN and combined loss are logged.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list of a source feature, source label and target feature tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:     The combined loss.</p>"},{"location":"api/rul_adapt/approach/dann/#rul_adapt.approach.dann.DannApproach.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one validation step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>val</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/evaluation/","title":"evaluation","text":""},{"location":"api/rul_adapt/approach/latent_align/","title":"latent_align","text":"<p>The latent space alignment approach uses several auxiliary losses to align the latent space of the source and target domain produced by a shared feature extractor:</p> <ul> <li>Healthy State Alignment: Pushes the healthy data of both domains into a single   compact cluster</li> <li>Degradation Direction Alignment: Minimizes the angle between degraded data   points with the healthy cluster as origin</li> <li>Degradation Level Alignment: Aligns the distance of degraded data points from the   healthy cluster to the number of time steps in degradation</li> <li>Degradation Fusion: Uses a   MMD loss to align the   distribution of both domains</li> </ul> <p>Which features are considered in the healthy state and which in degradation is either determined by taking the first few steps of each time series or by using a first-time-to-predict estimation. The first variant is used for CMAPSS, the second for XJTU-SY.</p> <p>The approach was introduced by Zhang et al. in 2021. For applying the approach on raw vibration data, i.e. XJTU-SY, it uses a windowing scheme and first-point-to-predict estimation introduced by Li et al. in 2020.</p>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignApproach","title":"<code>LatentAlignApproach</code>","text":"<p>             Bases: <code>AdaptionApproach</code></p> <p>The latent alignment approach introduces four latent space alignment losses to align the latent space of a shared feature extractor to both source and target domain.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt import model, approach\n&gt;&gt;&gt; feat_ex = model.CnnExtractor(1, [16, 16, 1], 10, fc_units=16)\n&gt;&gt;&gt; reg = model.FullyConnectedHead(16, [1])\n&gt;&gt;&gt; latent_align = approach.LatentAlignApproach(0.1, 0.1, 0.1, 0.1, lr=0.001)\n&gt;&gt;&gt; latent_align.set_model(feat_ex, reg)\n</code></pre>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignApproach.__init__","title":"<code>__init__(alpha_healthy, alpha_direction, alpha_level, alpha_fusion, loss_type='mse', rul_score_mode='phm08', evaluate_degraded_only=False, labels_as_percentage=False, **optim_kwargs)</code>","text":"<p>Create a new latent alignment approach.</p> <p>Each of the alphas controls the influence of the respective loss on the training. Commonly they are all set to the same value.</p> <p>For more information about the possible optimizer keyword arguments, see here.</p> <p>Parameters:</p> Name Type Description Default <code>alpha_healthy</code> <code>float</code> <p>The influence of the healthy state alignment loss.</p> required <code>alpha_direction</code> <code>float</code> <p>The influence of the degradation direction alignment loss.</p> required <code>alpha_level</code> <code>float</code> <p>The influence of the degradation level regularization loss.</p> required <code>alpha_fusion</code> <code>float</code> <p>The influence of the degradation fusion (MMD) loss.</p> required <code>loss_type</code> <code>Literal['mse', 'mae', 'rmse']</code> <p>The type of regression loss to use.</p> <code>'mse'</code> <code>rul_score_mode</code> <code>Literal['phm08', 'phm12']</code> <p>The mode for the val and test RUL score, either 'phm08'             or 'phm12'.</p> <code>'phm08'</code> <code>evaluate_degraded_only</code> <code>bool</code> <p>Whether to only evaluate the RUL score on degraded                     samples.</p> <code>False</code> <code>labels_as_percentage</code> <code>bool</code> <p>Whether to multiply labels by 100 to get percentages</p> <code>False</code> <code>**optim_kwargs</code> <code>Any</code> <p>Keyword arguments for the optimizer, e.g. learning rate.</p> <code>{}</code>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignApproach.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure an optimizer.</p>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignApproach.forward","title":"<code>forward(features)</code>","text":"<p>Predict the RUL values for a batch of input features.</p>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignApproach.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one test step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>test</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignApproach.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Execute one training step.</p> <p>The <code>batch</code> contains the following tensors in order:</p> <ul> <li>The source domain features.</li> <li>The steps in degradation for the source features.</li> <li>The RUL labels for the source features.</li> <li>The target domain features.</li> <li>The steps in degradation for the target features.</li> <li>The healthy state features for both domains.</li> </ul> <p>The easies way to produce such a batch is using the LatentAlignDataModule.</p> <p>The source, target and healthy features are passed through the feature extractor. Afterward, these high-level features are used to compute the alignment losses. The source domain RUL predictions are computed using the regressor and used to calculate the MSE loss. The losses are then combined. Each separate and the combined loss are logged.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, ...]</code> <p>The batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The combined loss.</p>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignApproach.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one validation step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>val</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignFttpApproach","title":"<code>LatentAlignFttpApproach</code>","text":"<p>             Bases: <code>AdaptionApproach</code></p> <p>This first-point-to-predict estimation approach trains a GAN on healthy state bearing data. The discriminator can be used afterward to compute a health indicator for each bearing.</p> <p>The feature extractor and regressor models are used as the discriminator. The regressor is not allowed to have an activation function on its last layer and needs to use only a single output neuron because BCEWithLogitsLoss is used. The generator receives noise with the shape [batch_size, 1, noise_dim]. The generator needs an output with enough elements so that it can be reshaped to the same shape as the real input data. The reshaping is done internally.</p> <p>Both generator and discriminator are trained at once by using a Gradient Reversal Layer between them.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt import model, approach\n&gt;&gt;&gt; feat_ex = model.CnnExtractor(1, [16, 16, 1], 10, fc_units=16)\n&gt;&gt;&gt; reg = model.FullyConnectedHead(16, [1])\n&gt;&gt;&gt; gen = model.CnnExtractor(1, [1], 10, padding=True)\n&gt;&gt;&gt; fttp_model = approach.LatentAlignFttpApproach(1e-4, 10)\n&gt;&gt;&gt; fttp_model.set_model(feat_ex, reg, gen)\n&gt;&gt;&gt; health_indicator = fttp_model(torch.randn(16, 1, 10)).std()\n</code></pre>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignFttpApproach.generator","title":"<code>generator</code>  <code>property</code>","text":"<p>The generator network.</p>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignFttpApproach.__init__","title":"<code>__init__(noise_dim, **optim_kwargs)</code>","text":"<p>Create a new FTTP estimation approach.</p> <p>The generator is set by the <code>set_model</code> function together with the feature extractor and regressor.</p> <p>For more information about the possible optimizer keyword arguments, see here.</p> <p>Parameters:</p> Name Type Description Default <code>noise_dim</code> <code>int</code> <p>The size of the last dimension of the noise tensor.</p> required <code>**optim_kwargs</code> <code>Any</code> <p>Keyword arguments for the optimizer, e.g. learning rate.</p> <code>{}</code>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignFttpApproach.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure an optimizer for the generator and discriminator.</p>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignFttpApproach.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict the health indicator for the given inputs.</p>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignFttpApproach.set_model","title":"<code>set_model(feature_extractor, regressor, generator=None, *args, **kwargs)</code>","text":"<p>Set the feature extractor, regressor (forming the discriminator) and generator for this approach.</p> <p>The regressor is not allowed to have an activation function on its last layer and needs to use only a single output neuron. The generator receives noise with the shape [batch_size, 1, noise_dim]. The generator needs an output with enough elements so that it can be reshaped to the same shape as the real input data. The reshaping is done internally.</p> <p>Parameters:</p> Name Type Description Default <code>feature_extractor</code> <code>Module</code> <p>The feature extraction network.</p> required <code>regressor</code> <code>Module</code> <p>The regressor functioning as the head of the discriminator.</p> required <code>generator</code> <code>Optional[Module]</code> <p>The generator network.</p> <code>None</code>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.LatentAlignFttpApproach.training_step","title":"<code>training_step(batch)</code>","text":"<p>Execute one training step.</p> <p>The batch is a tuple of the features and the labels. The labels are ignored. A noise tensor is passed to the generator to generate fake features. The discriminator classifies if the features are real or fake and the binary cross entropy loss is calculated. Real features receive the label zero and the fake features one.</p> <p>Both generator and discriminator are trained at once by using a Gradient Reversal Layer between them. At the end, the loss is logged.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>A tuple of feature and label tensors.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The classification loss.</p>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.extract_chunk_windows","title":"<code>extract_chunk_windows(features, window_size, chunk_size)</code>","text":"<p>Extract chunk windows from the given features of shape <code>[num_org_windows, org_window_size, num_features]</code>.</p> <p>A chunk window is a window that consists of <code>window_size</code> chunks. Each original window is split into chunks of size <code>chunk_size</code>. A chunk window is then formed by concatenating chunks from the same position inside <code>window_size</code> consecutive original windows. Therefore, each original window is represented by <code>org_window_size // chunk_size</code> chunk windows. The original window size must therefor be divisible by the chunk size.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>ndarray</code> <p>The features to extract the chunk windows from.</p> required <code>window_size</code> <code>int</code> <p>The number of consecutive original windows to form a chunk window          from.</p> required <code>chunk_size</code> <code>int</code> <p>The size of the chunks to extract from the original windows.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Chunk windows of shape <code>[num_windows, window_size * chunk_size, num_features]</code>.</p>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.get_first_time_to_predict","title":"<code>get_first_time_to_predict(fttp_model, features, window_size, chunk_size, healthy_index, threshold_coefficient)</code>","text":"<p>Get the first time step to predict for the given features.</p> <p>The features are pre-processed via the extract_chunk_windows function and fed in batches to the <code>fttp_model</code>. Each batch consists of the chunk windows that end in the same original feature window. The health indicator for the original window is calculated as the standard deviation of the predictions of the <code>fttp_model</code>.</p> <p>The first-time-to-predict is the first time step where the health indicator is larger than <code>threshold_coefficient</code> times the mean of the health indicator for the first <code>healthy_index</code> time steps. If the threshold is never exceeded, a RuntimeError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>fttp_model</code> <code>LatentAlignFttpApproach</code> <p>The model to use for the health indicator calculation.</p> required <code>features</code> <code>ndarray</code> <p>The features to calculate the first-time-to-predict for.</p> required <code>window_size</code> <code>int</code> <p>The size of the chunk windows to extract.</p> required <code>chunk_size</code> <code>int</code> <p>The size of the chunks for each chunk window to extract.</p> required <code>healthy_index</code> <code>int</code> <p>The index of the last healthy time step.</p> required <code>threshold_coefficient</code> <code>float</code> <p>The threshold coefficient for the health indicator.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The original window index of the first-time-to-predict.</p>"},{"location":"api/rul_adapt/approach/latent_align/#rul_adapt.approach.latent_align.get_health_indicator","title":"<code>get_health_indicator(fttp_model, features, window_size, chunk_size)</code>","text":"<p>Get the health indicator for the given features.</p> <p>The features are pre-processed via the extract_chunk_windows function and fed in batches to the <code>fttp_model</code>. Each batch consists of the chunk windows that end in the same original feature window. The health indicator for the original window is calculated as the standard deviation of the predictions of the <code>fttp_model</code>.</p> <p>The length of the returned health indicator array is shorter than the <code>features</code> array by <code>window_size - 1</code>, due to the chunk windowing. This means the first health indicator value belongs to the original window with the index <code>window_size - 1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fttp_model</code> <code>Module</code> <p>The model to use for the health indicator calculation.</p> required <code>features</code> <code>ndarray</code> <p>The features to calculate the health indicator for.</p> required <code>window_size</code> <code>int</code> <p>The size of the chunk windows to extract.</p> required <code>chunk_size</code> <code>int</code> <p>The size of the chunks for each chunk window to extract.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The health indicator for the original windows.</p>"},{"location":"api/rul_adapt/approach/mmd/","title":"mmd","text":"<p>The Maximum Mean Discrepancy (MMD) approach uses the distance measure of the same name to adapt a feature extractor. This implementation uses a multi-kernel variant of the MMD loss with bandwidths set via the median heuristic.</p> <pre><code>Source --&gt; FeatEx --&gt; Source Feats -----------&gt; Regressor  --&gt; RUL Prediction\n        ^         |                 |\n        |         |                 v\nTarget --         --&gt; Target Feats --&gt;  MMD Loss\n</code></pre> <p>It was first introduced by Long et al. as Deep Adaption Network (DAN) for image classification.</p> Used In <ul> <li>Cao et al. (2021). Transfer learning for remaining useful life prediction of multi-conditions bearings based on bidirectional-GRU network. Measurement: Journal of the International Measurement Confederation, 178. 10.1016/j.measurement.2021.109287</li> <li>Krokotsch et al. (2020). A Novel Evaluation Framework for Unsupervised Domain Adaption on Remaining Useful Lifetime Estimation. 2020 IEEE International Conference on Prognostics and Health Management (ICPHM). 10.1109/ICPHM49022.2020.9187058</li> </ul>"},{"location":"api/rul_adapt/approach/mmd/#rul_adapt.approach.mmd.MmdApproach","title":"<code>MmdApproach</code>","text":"<p>             Bases: <code>AdaptionApproach</code></p> <p>The MMD uses the Maximum Mean Discrepancy to adapt a feature extractor to be used with the source regressor.</p> <p>The regressor needs the same number of input units as the feature extractor has output units.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt import model\n&gt;&gt;&gt; from rul_adapt import approach\n&gt;&gt;&gt; feat_ex = model.CnnExtractor(1, [16, 16, 1], 10, fc_units=16)\n&gt;&gt;&gt; reg = model.FullyConnectedHead(16, [1])\n&gt;&gt;&gt; mmd = approach.MmdApproach(0.01)\n&gt;&gt;&gt; mmd.set_model(feat_ex, reg)\n</code></pre>"},{"location":"api/rul_adapt/approach/mmd/#rul_adapt.approach.mmd.MmdApproach.__init__","title":"<code>__init__(mmd_factor, num_mmd_kernels=5, loss_type='mse', rul_score_mode='phm08', evaluate_degraded_only=False, **optim_kwargs)</code>","text":"<p>Create a new MMD approach.</p> <p>The strength of the influence of the MMD loss on the feature extractor is controlled by the <code>mmd_factor</code>. The higher it is, the stronger the influence.</p> <p>For more information about the possible optimizer keyword arguments, see here.</p> <p>Parameters:</p> Name Type Description Default <code>mmd_factor</code> <code>float</code> <p>The strength of the MMD loss' influence.</p> required <code>num_mmd_kernels</code> <code>int</code> <p>The number of kernels for the MMD loss.</p> <code>5</code> <code>loss_type</code> <code>Literal['mse', 'rmse', 'mae']</code> <p>The type of regression loss, either 'mse', 'rmse' or 'mae'.</p> <code>'mse'</code> <code>rul_score_mode</code> <code>Literal['phm08', 'phm12']</code> <p>The mode for the val and test RUL score, either 'phm08'             or 'phm12'.</p> <code>'phm08'</code> <code>evaluate_degraded_only</code> <code>bool</code> <p>Whether to only evaluate the RUL score on degraded                     samples.</p> <code>False</code> <code>**optim_kwargs</code> <code>Any</code> <p>Keyword arguments for the optimizer, e.g. learning rate.</p> <code>{}</code>"},{"location":"api/rul_adapt/approach/mmd/#rul_adapt.approach.mmd.MmdApproach.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure an optimizer.</p>"},{"location":"api/rul_adapt/approach/mmd/#rul_adapt.approach.mmd.MmdApproach.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict the RUL values for a batch of input features.</p>"},{"location":"api/rul_adapt/approach/mmd/#rul_adapt.approach.mmd.MmdApproach.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one test step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>test</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/mmd/#rul_adapt.approach.mmd.MmdApproach.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Execute one training step.</p> <p>The <code>batch</code> argument is a list of three tensors representing the source features, source labels and target features. Both types of features are fed to the feature extractor. Then the regression loss for the source domain and the MMD loss between domains is computed. The regression, MMD and combined loss are logged.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list of a source feature, source label and target feature tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:     The combined loss.</p>"},{"location":"api/rul_adapt/approach/mmd/#rul_adapt.approach.mmd.MmdApproach.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx)</code>","text":"<p>Execute one validation step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>val</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> required"},{"location":"api/rul_adapt/approach/pseudo_labels/","title":"pseudo_labels","text":"<p>Pseudo labeling is a simple approach that takes a model trained on the source domain to label the target domain. Afterward, the model training is continued on the combined source and target data. This process is repeated until the validation loss converges.</p> Used In <ul> <li>Wang et al. (2022). Residual Life Prediction of Bearings Based on SENet-TCN and Transfer Learning. IEEE Access, 10, 10.1109/ACCESS.2022.3223387</li> </ul> <p>Examples:</p> <pre><code>import torch\nimport rul_datasets\nimport pytorch_lightning as pl\n\nfrom rul_adapt import model\nfrom rul_adapt import approach\n\nfeat_ex = model.CnnExtractor(14, [16], 30, fc_units=16)\nreg = model.FullyConnectedHead(16, [1])\n\nsupervised = approach.SupervisedApproach(0.001, \"rmse\", \"adam\")\nsupervised.set_model(feat_ex, reg)\n\nfd1 = rul_datasets.RulDataModule(rul_datasets.CmapssReader(1), 32)\nfd1.setup()\nfd3 = rul_datasets.RulDataModule(rul_datasets.CmapssReader(3), 32)\nfd3.setup()\n\ntrainer = pl.Trainer(max_epochs=10)\ntrainer.fit(supervised, fd3)\n\npseudo_labels = approach.generate_pseudo_labels(fd1, supervised)\npseudo_labels = [max(0, min(125, pl)) for pl in pseudo_labels]\napproach.patch_pseudo_labels(fd3, pseudo_labels)\n\ncombined_data = torch.utils.data.ConcatDataset(\n    [fd1.to_dataset(\"dev\"), fd3.to_dataset(\"dev\")]\n)\ncombined_dl = torch.utils.data.DataLoader(combined_data, batch_size=32)\n\ntrainer = pl.Trainer(max_epochs=10)\ntrainer.fit(supervised, train_dataloader=combined_dl)\n</code></pre>"},{"location":"api/rul_adapt/approach/pseudo_labels/#rul_adapt.approach.pseudo_labels.generate_pseudo_labels","title":"<code>generate_pseudo_labels(dm, model, inductive=False)</code>","text":"<p>Generate pseudo labels for the dev set of a data module.</p> <p>The pseudo labels are generated for the last timestep of each run. They are returned raw and may therefore contain values bigger than <code>max_rul</code> or negative values. It is recommended to clip them to zero and <code>max_rul</code> respectively before using them to patch a reader.</p> <p>The model is assumed to reside on the CPU where the calculation will be performed.</p> <p>Parameters:</p> Name Type Description Default <code>dm</code> <code>RulDataModule</code> <p>The data module to generate pseudo labels for.</p> required <code>model</code> <code>Module</code> <p>The model to use for generating the pseudo labels.</p> required <code>inductive</code> <code>bool</code> <p>Whether to generate pseudo labels for inductive adaption,        i.e., use 'test' instead of 'dev' split.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>A list of pseudo labels for the dev set of the data module.</p>"},{"location":"api/rul_adapt/approach/pseudo_labels/#rul_adapt.approach.pseudo_labels.get_max_rul","title":"<code>get_max_rul(reader)</code>","text":"<p>Resolve the maximum RUL of a reader to be comparable to floats.</p>"},{"location":"api/rul_adapt/approach/pseudo_labels/#rul_adapt.approach.pseudo_labels.patch_pseudo_labels","title":"<code>patch_pseudo_labels(dm, pseudo_labels, inductive=False)</code>","text":"<p>Patch a data module with pseudo labels in-place.</p> <p>The pseudo labels are used to replace the RUL targets of the dev set of the data module. The validation and test sets are not affected.</p> <p>It is not possible to patch the same data module multiple times. Instead, instantiate a fresh data module and patch that one.</p> <p>Parameters:</p> Name Type Description Default <code>dm</code> <code>RulDataModule</code> <p>The data module to patch.</p> required <code>pseudo_labels</code> <code>List[float]</code> <p>The pseudo labels to use for patching the data module.</p> required <code>inductive</code> <code>bool</code> <p>Whether to generate pseudo labels for inductive adaption,        i.e., use 'test' instead of 'dev' split.</p> <code>False</code>"},{"location":"api/rul_adapt/approach/supervised/","title":"supervised","text":"<p>The supervised approach trains solely on the labeled source domain. It can be used for pre-training or as a baseline to compare adaption approaches against.</p> <pre><code>Data --&gt; FeatureExtractor --&gt; Features --&gt; Regressor  --&gt; RUL Prediction\n</code></pre>"},{"location":"api/rul_adapt/approach/supervised/#rul_adapt.approach.supervised.SupervisedApproach","title":"<code>SupervisedApproach</code>","text":"<p>             Bases: <code>AdaptionApproach</code></p> <p>The supervised approach uses either MSE, MAE or RMSE loss to train a feature extractor and regressor in a supervised fashion on the source domain. It can be used either for pre-training or as a baseline to compare adaption approaches against.</p> <p>The regressor needs the same number of input units as the feature extractor has output units.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt import model\n&gt;&gt;&gt; from rul_adapt import approach\n&gt;&gt;&gt; feat_ex = model.CnnExtractor(1, [16, 16, 1], 10, fc_units=16)\n&gt;&gt;&gt; reg = model.FullyConnectedHead(16, [1])\n&gt;&gt;&gt; disc = model.FullyConnectedHead(16, [8, 1], act_func_on_last_layer=False)\n&gt;&gt;&gt; main = approach.SupervisedApproach(\"mse\")\n&gt;&gt;&gt; main.set_model(feat_ex, reg, disc)\n</code></pre>"},{"location":"api/rul_adapt/approach/supervised/#rul_adapt.approach.supervised.SupervisedApproach.__init__","title":"<code>__init__(loss_type, rul_scale=1, rul_score_mode='phm08', evaluate_degraded_only=False, **optim_kwargs)</code>","text":"<p>Create a supervised approach.</p> <p>The regressor output can be scaled with <code>rul_scale</code> to control its magnitude. By default, the RUL values are not scaled.</p> <p>For more information about the possible optimizer keyword arguments, see here.</p> <p>Parameters:</p> Name Type Description Default <code>rul_score_mode</code> <code>Literal['phm08', 'phm12']</code> <code>'phm08'</code> <code>loss_type</code> <code>Literal['mse', 'mae', 'rmse']</code> <p>Training loss function to use. Either 'mse', 'mae' or 'rmse'.</p> required <code>rul_scale</code> <code>int</code> <p>Scalar to multiply the RUL prediction with.</p> <code>1</code> <code>evaluate_degraded_only</code> <code>bool</code> <p>Whether to only evaluate the RUL score on degraded                     samples.</p> <code>False</code> <code>**optim_kwargs</code> <code>Any</code> <p>Keyword arguments for the optimizer, e.g. learning rate.</p> <code>{}</code>"},{"location":"api/rul_adapt/approach/supervised/#rul_adapt.approach.supervised.SupervisedApproach.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=-1)</code>","text":"<p>Execute one test step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. A RUL prediction is made from the features and the validation RMSE and RUL score are calculated. The metrics recorded for dataloader idx zero are assumed to be from the source domain and for dataloader idx one from the target domain. The metrics are written to the configured logger under the prefix <code>test</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list containing a feature and a label tensor.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the current dataloader (0: source, 1: target).</p> <code>-1</code>"},{"location":"api/rul_adapt/approach/supervised/#rul_adapt.approach.supervised.SupervisedApproach.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Execute one training step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. The features are used to predict RUL values that are compared against the labels with the specified training loss. The loss is then logged.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list of feature and label tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:     The training loss.</p>"},{"location":"api/rul_adapt/approach/supervised/#rul_adapt.approach.supervised.SupervisedApproach.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=-1)</code>","text":"<p>Execute one validation step.</p> <p>The <code>batch</code> argument is a list of two tensors representing features and labels. The features are used to predict RUL values that are compared against the labels with an RMSE loss. The loss is then logged.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tensor]</code> <p>A list of feature and label tensors.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required"},{"location":"api/rul_adapt/approach/tbigru/","title":"tbigru","text":"<p>The TBiGRU approach uses a feature selection mechanism to mine transferable features and a bearing running state detection to determine the first-time-to-predict. The training is done with an MMD approach.</p> <p>The feature selection uses a distance measure based on Dynamic Time Warping and the Wasserstein distance. From a set of 30 common vibration features the ones with the smallest distance between source and target domain are selected. These features serve as inputs to the network.</p> <p>The first-time-to-predict (FTTP) is used to generate the RUL labels for training. FTTP is the time step where the degradation can be detected for the first time. The RUL labels before this time step should be constant. The TBiGRU approach uses the moving average correlation (MAC) of the energy entropies of four levels of maximal overlap discrete wavelet transform (MODWT) decompositions to determine four running states of each bearing. The end of the steady running state marks the FTTP.</p> <p>TBiGRU was introduced by Cao et al. and evaluated on the FEMTO Bearing dataset.</p>"},{"location":"api/rul_adapt/approach/tbigru/#rul_adapt.approach.tbigru.VibrationFeatureExtractor","title":"<code>VibrationFeatureExtractor</code>","text":"<p>This class extracts 30 different features from a raw acceleration signal.</p> <p>The features are: RMS, kurtosis, peak2peak, standard deviation, skewness, margin factor, impulse factor, energy, median absolute, gini factor, maximum absolute, mean absolute, energies of the 16 bands resulting from wavelet packet decomposition, standard deviation of arccosh and arcsinh. If the input has n features, n*30 features are extracted. Additionally, it features a scaler that can be fit to scale all extracted features between [0, 1].</p>"},{"location":"api/rul_adapt/approach/tbigru/#rul_adapt.approach.tbigru.VibrationFeatureExtractor.__call__","title":"<code>__call__(features, targets)</code>","text":"<p>Extract the features from the input and optionally scale them.</p> <p>The features should have the shape <code>[num_windows, window_size, num_input_features]</code> and the targets <code>[num_windows]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>ndarray</code> <p>The input features.</p> required <code>targets</code> <code>ndarray</code> <p>The input targets.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>The extracted features and input targets.</p>"},{"location":"api/rul_adapt/approach/tbigru/#rul_adapt.approach.tbigru.VibrationFeatureExtractor.__init__","title":"<code>__init__(num_input_features, feature_idx=None)</code>","text":"<p>Create a new vibration feature extractor with the selected features.</p> <p>The features are sorted as f1_1, .., f1_j, ..., fi_j, where i is the index of the computed feature (between 0 and 30) and j is the index of the raw feature (between 0 and <code>num_input_features</code>).</p> <p>Parameters:</p> Name Type Description Default <code>num_input_features</code> <code>int</code> <p>The number of input features.</p> required <code>feature_idx</code> <code>Optional[List[int]]</code> <p>The indices of the features to compute.</p> <code>None</code>"},{"location":"api/rul_adapt/approach/tbigru/#rul_adapt.approach.tbigru.VibrationFeatureExtractor.fit","title":"<code>fit(features)</code>","text":"<p>Fit the internal scaler on a list of raw feature time series.</p> <p>The time series are passed through the feature extractor and then used to fit the internal min-max scaler. Each time series in the list should have the shape <code>[num_windows, window_size, num_input_features]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List[ndarray]</code> <p>The list of raw feature time series.</p> required <p>Returns:</p> Type Description <code>VibrationFeatureExtractor</code> <p>The feature extractor itself.</p>"},{"location":"api/rul_adapt/approach/tbigru/#rul_adapt.approach.tbigru.mac","title":"<code>mac(inputs, window_size, wavelet='dmey')</code>","text":"<p>Calculate the moving average correlation (MAC) of the energy entropies of four levels of maximal overlap discrete wavelet transform (MODWT) decompositions.</p> <p>The <code>wavelet</code> is a wavelet description that can be passed to <code>pywt</code>. The default wavelet was confirmed by the original authors. For more options call <code>pywt.wavelist</code>. The input signal should have the shape <code>[num_windows, window_size, num_features]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>ndarray</code> <p>The input acceleration signal.</p> required <code>window_size</code> <code>int</code> <p>The window size of the sliding window to calculate the average          over.</p> required <code>wavelet</code> <code>str</code> <p>The description of the wavelet, e.g. 'sym4'.</p> <code>'dmey'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The MAC of the input signal which is <code>window_size - 1</code> shorter.</p>"},{"location":"api/rul_adapt/approach/tbigru/#rul_adapt.approach.tbigru.modwpt","title":"<code>modwpt(inputs, wavelet, level)</code>","text":"<p>Apply Maximal Overlap Discrete Wavelet Packet Transformation (MODWT) of <code>level</code> to the input.</p> <p>The <code>wavelet</code> should be a string that can be passed to <code>pywt</code> to construct a wavelet function. For more options call <code>pywt.wavelist</code>. The implementation was inspired by this repository.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>ndarray</code> <p>An input signal of shape <code>[num_windows, window_size, num_features]</code>.</p> required <code>wavelet</code> <code>str</code> <p>The description of the wavelet function, e.g. 'sym4'.</p> required <code>level</code> <code>int</code> <p>The decomposition level.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The 2**level decompositions stacked in the last axis.</p>"},{"location":"api/rul_adapt/approach/tbigru/#rul_adapt.approach.tbigru.select_features","title":"<code>select_features(source, target, num_features)</code>","text":"<p>Select the most transferable features between source and target domain.</p> <p>30 features are considered: RMS, kurtosis, peak2peak, standard deviation, skewness, margin factor, impulse factor, energy, median absolute, gini factor, maximum absolute, mean absolute, energies of the 16 bands resulting from wavelet packet decomposition, standard deviation of arccosh and arcsinh. If the input has n raw features, n*30 features are extracted.</p> <p>The <code>dev</code> splits of both domains are used to calculate a distance metric based on Dynamic Time Warping and the Wasserstein Distance. The indices of the <code>num_feature</code> features with the lowest distances are returned.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>AbstractReader</code> <p>The reader of the source domain.</p> required <code>target</code> <code>AbstractReader</code> <p>The reader of the target domain.</p> required <code>num_features</code> <code>int</code> <p>The number of transferable features to return.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The indices of features ordered by transferability.</p>"},{"location":"api/rul_adapt/construct/","title":"construct","text":""},{"location":"api/rul_adapt/construct/adarul/","title":"adarul","text":""},{"location":"api/rul_adapt/construct/adarul/functional/","title":"functional","text":""},{"location":"api/rul_adapt/construct/adarul/functional/#rul_adapt.construct.adarul.functional.adarul_from_config","title":"<code>adarul_from_config(config, pre_trainer_kwargs=None, trainer_kwargs=None)</code>","text":"<p>Construct an ADARUL approach from a configuration.</p> <p>The configuration can be created by calling get_adarul_config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>The ADARUL config.</p> required <code>pre_trainer_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Overrides for the pre-training trainer class.</p> <code>None</code> <code>trainer_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Overrides for the main trainer class.</p> <code>None</code> <p>Returns:     pre: The data module, approach and trainer for the pre-training stage     main: The data module, approach, domain discriminator and trainer for           the main stage</p>"},{"location":"api/rul_adapt/construct/adarul/functional/#rul_adapt.construct.adarul.functional.get_adarul","title":"<code>get_adarul(source_fd, target_fd, pre_trainer_kwargs=None, trainer_kwargs=None)</code>","text":"<p>Construct an ADARUL approach with the original hyperparameters on CMAPSS.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import rul_adapt\n&gt;&gt;&gt; pre, main = rul_adapt.construct.get_adarul(, 3, 1)\n&gt;&gt;&gt; pre_dm, pre_approach, pre_trainer = pre\n&gt;&gt;&gt; dm, approach, domain_disc, trainer = main\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source_fd</code> <code>int</code> <p>The source FD of CMAPSS.</p> required <code>target_fd</code> <code>int</code> <p>The target FD of CMAPSS.</p> required <code>pre_trainer_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Overrides for the pre-training trainer class.</p> <code>None</code> <code>trainer_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Overrides for the main trainer class.</p> <code>None</code> <p>Returns:     pre: The data module, approach and trainer for the pre-training stage     main: The data module, approach, domain discriminator and trainer for           the main stage</p>"},{"location":"api/rul_adapt/construct/adarul/functional/#rul_adapt.construct.adarul.functional.get_adarul_config","title":"<code>get_adarul_config(source_fd, target_fd)</code>","text":"<p>Get a configuration for the ADARUL approach.</p> <p>The configuration can be modified and fed to adarul_from_config to create the approach.</p> <p>Parameters:</p> Name Type Description Default <code>source_fd</code> <code>int</code> <p>The source FD of CMAPSS.</p> required <code>target_fd</code> <code>int</code> <p>The target FD of CMAPSS.</p> required <p>Returns:     The ADARUL configuration.</p>"},{"location":"api/rul_adapt/construct/cnn_dann/","title":"cnn_dann","text":""},{"location":"api/rul_adapt/construct/cnn_dann/functional/","title":"functional","text":""},{"location":"api/rul_adapt/construct/cnn_dann/functional/#rul_adapt.construct.cnn_dann.functional.cnn_dann_from_config","title":"<code>cnn_dann_from_config(config, **trainer_kwargs)</code>","text":"<p>Construct a CNN-DANN approach from a configuration.</p> <p>The configuration can be created by calling get_cnn_dann_config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>The CNN-DANN configuration.</p> required <code>trainer_kwargs</code> <code>Any</code> <p>Overrides for the trainer class.</p> <code>{}</code> <p>Returns:     dm: The data module for adaption of two CMAPSS sub-datasets.     dann: The DANN approach with feature extractor, regressor and domain disc.     trainer: The trainer object.</p>"},{"location":"api/rul_adapt/construct/cnn_dann/functional/#rul_adapt.construct.cnn_dann.functional.get_cnn_dann","title":"<code>get_cnn_dann(source_fd, target_fd, **trainer_kwargs)</code>","text":"<p>Construct an CNN-DANN approach for CMAPSS with the original hyperparameters.</p> <p>The adaption tasks 1--&gt;4, 2--&gt;3, 3--&gt;2 and 4--&gt;1 are missing because they were not investigated in the paper.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import rul_adapt\n&gt;&gt;&gt; dm, dann, trainer = rul_adapt.construct.get_cnn_dann(3, 1)\n&gt;&gt;&gt; trainer.fit(dann, dm)\n&gt;&gt;&gt; trainer.test(dann, dm)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source_fd</code> <code>int</code> <p>The source FD of CMAPSS.</p> required <code>target_fd</code> <code>int</code> <p>The target FD of CMAPSS.</p> required <code>trainer_kwargs</code> <code>Any</code> <p>Overrides for the trainer class.</p> <code>{}</code> <p>Returns:     dm: The data module for adaption of two CMAPSS sub-datasets.     dann: The DANN approach with feature extractor, regressor and domain disc.     trainer: The trainer object.</p>"},{"location":"api/rul_adapt/construct/cnn_dann/functional/#rul_adapt.construct.cnn_dann.functional.get_cnn_dann_config","title":"<code>get_cnn_dann_config(source_fd, target_fd)</code>","text":"<p>Get a configuration for the CNN-DANN approach.</p> <p>The adaption tasks 1--&gt;4, 2--&gt;3, 3--&gt;2 and 4--&gt;1 are missing because they were not investigated in the paper. The configuration can be modified and fed to cnn_dann_from_config to create the approach.</p> <p>Parameters:</p> Name Type Description Default <code>source_fd</code> <code>int</code> <p>The source FD of CMAPSS.</p> required <code>target_fd</code> <code>int</code> <p>The target FD of CMAPSS.</p> required <p>Returns:     The CNN-DANN configuration.</p>"},{"location":"api/rul_adapt/construct/consistency/","title":"consistency","text":""},{"location":"api/rul_adapt/construct/consistency/functional/","title":"functional","text":""},{"location":"api/rul_adapt/construct/consistency/functional/#rul_adapt.construct.consistency.functional.consistency_dann_from_config","title":"<code>consistency_dann_from_config(config, pre_trainer_kwargs=None, trainer_kwargs=None)</code>","text":"<p>Construct a Consistency DANN approach from a configuration. The configuration can be created by calling get_consistency_dann_config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>The Consistency DANN config.</p> required <code>pre_trainer_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Overrides for the pre-training trainer class.</p> <code>None</code> <code>trainer_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Overrides for the main trainer class.</p> <code>None</code> <p>Returns:     pre: The data module, approach and trainer for the pre-training stage     main: The data module, approach, domain discriminator and trainer for           the main stage</p>"},{"location":"api/rul_adapt/construct/consistency/functional/#rul_adapt.construct.consistency.functional.get_consistency_dann","title":"<code>get_consistency_dann(dataset, source_fd, target_fd, pre_trainer_kwargs=None, trainer_kwargs=None)</code>","text":"<p>Construct a Consistency DANN approach with the original hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import rul_adapt\n&gt;&gt;&gt; pre, main = rul_adapt.construct.get_consistency_dann(\"cmapss\", 3, 1)\n&gt;&gt;&gt; pre_dm, pre_approach, pre_trainer = pre\n&gt;&gt;&gt; dm, approach, domain_disc, trainer = main\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Literal['cmapss', 'xjtu-sy']</code> <p>The name of the dataset, either <code>cmapss</code> or <code>xjtu-sy</code>.</p> required <code>source_fd</code> <code>int</code> <p>The source FD of CMAPSS.</p> required <code>target_fd</code> <code>int</code> <p>The target FD of CMAPSS.</p> required <code>pre_trainer_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Overrides for the pre-training trainer class.</p> <code>None</code> <code>trainer_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Overrides for the main trainer class.</p> <code>None</code> <p>Returns:     pre: The data module, approach and trainer for the pre-training stage     main: The data module, approach, domain discriminator and trainer for           the main stage</p>"},{"location":"api/rul_adapt/construct/consistency/functional/#rul_adapt.construct.consistency.functional.get_consistency_dann_config","title":"<code>get_consistency_dann_config(dataset, source_fd, target_fd)</code>","text":"<p>Get a configuration for the Consistency DANN approach.</p> <p>The configuration can be modified and fed to consistency_dann_from_config to create the approach.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Literal['cmapss', 'xjtu-sy']</code> <p>The name of the dataset, either <code>cmapss</code> or <code>xjtu-sy</code>.</p> required <code>source_fd</code> <code>int</code> <p>The source FD of CMAPSS.</p> required <code>target_fd</code> <code>int</code> <p>The target FD of CMAPSS.</p> required <p>Returns:     The Consistency DANN configuration.</p>"},{"location":"api/rul_adapt/construct/latent_align/","title":"latent_align","text":""},{"location":"api/rul_adapt/construct/latent_align/functional/","title":"functional","text":""},{"location":"api/rul_adapt/construct/latent_align/functional/#rul_adapt.construct.latent_align.functional.get_latent_align","title":"<code>get_latent_align(dataset, source_fd, target_fd, xjtu_sy_subtask=None, **trainer_kwargs)</code>","text":"<p>Construct a Latent Alignment approach for the selected dataset with the original hyperparameters.</p> <p>For the XJTU-SY task only FD001 and FD002 are available. The subtask controls if the bearing with the id 1 or 2 is used as the target data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import rul_adapt\n&gt;&gt;&gt; dm, latent, trainer = rul_adapt.construct.get_latent_align(\"cmapss\", 3, 1)\n&gt;&gt;&gt; trainer.fit(latent, dm)\n&gt;&gt;&gt; trainer.test(latent, dm)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Literal['cmapss', 'xjtu-sy']</code> <p>The dataset to use.</p> required <code>source_fd</code> <code>int</code> <p>The source FD.</p> required <code>target_fd</code> <code>int</code> <p>The target FD.</p> required <code>xjtu_sy_subtask</code> <code>Optional[int]</code> <p>The subtask for the XJTU-SY (either 1 or 2).</p> <code>None</code> <code>trainer_kwargs</code> <code>Any</code> <p>Overrides for the trainer class.</p> <code>{}</code> <p>Returns:     dm: The data module for adaption of the sub-datasets.     dann: The Latent Alignment approach with feature extractor and regressor.     trainer: The trainer object.</p>"},{"location":"api/rul_adapt/construct/latent_align/functional/#rul_adapt.construct.latent_align.functional.get_latent_align_config","title":"<code>get_latent_align_config(dataset, source_fd, target_fd, xjtu_sy_subtask=None)</code>","text":"<p>Get a configuration for the Latent Alignment approach.</p> <p>For the XJTU-SY task only FD001 and FD002 are available. The subtask controls if the bearing with the id 1 or 2 is used as the target data. The configuration can be modified and fed to latent_align_from_config to create the approach.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Literal['cmapss', 'xjtu-sy']</code> <p>The dataset to use.</p> required <code>source_fd</code> <code>int</code> <p>The source FD.</p> required <code>target_fd</code> <code>int</code> <p>The target FD.</p> required <code>xjtu_sy_subtask</code> <code>Optional[int]</code> <p>The subtask for the XJTU-SY (either 1 or 2).</p> <code>None</code> <p>Returns:     The Latent Alignment configuration.</p>"},{"location":"api/rul_adapt/construct/latent_align/functional/#rul_adapt.construct.latent_align.functional.latent_align_from_config","title":"<code>latent_align_from_config(config, **trainer_kwargs)</code>","text":"<p>Construct a Latent Alignment approach from a configuration.</p> <p>The configuration can be created by calling get_latent_align_config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>The Latent Alignment configuration.</p> required <code>trainer_kwargs</code> <code>Any</code> <p>Overrides for the trainer class.</p> <code>{}</code> <p>Returns:     dm: The data module for adaption of the sub-datasets.     dann: The Latent Alignment approach with feature extractor, regressor.     trainer: The trainer object.</p>"},{"location":"api/rul_adapt/construct/lstm_dann/","title":"lstm_dann","text":""},{"location":"api/rul_adapt/construct/lstm_dann/functional/","title":"functional","text":""},{"location":"api/rul_adapt/construct/lstm_dann/functional/#rul_adapt.construct.lstm_dann.functional.get_lstm_dann","title":"<code>get_lstm_dann(source_fd, target_fd, **trainer_kwargs)</code>","text":"<p>Construct an LSTM-DANN approach for CMAPSS with the original hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import rul_adapt\n&gt;&gt;&gt; dm, dann, trainer = rul_adapt.construct.get_lstm_dann(3, 1)\n&gt;&gt;&gt; trainer.fit(dann, dm)\n&gt;&gt;&gt; trainer.test(dann, dm)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source_fd</code> <code>int</code> <p>The source FD of CMAPSS.</p> required <code>target_fd</code> <code>int</code> <p>The target FD of CMAPSS.</p> required <code>trainer_kwargs</code> <code>Any</code> <p>Overrides for the trainer class.</p> <code>{}</code> <p>Returns:     dm: The data module for adaption of two CMAPSS sub-datasets.     dann: The DANN approach with feature extractor, regressor and domain disc.     trainer: The trainer object.</p>"},{"location":"api/rul_adapt/construct/lstm_dann/functional/#rul_adapt.construct.lstm_dann.functional.get_lstm_dann_config","title":"<code>get_lstm_dann_config(source_fd, target_fd)</code>","text":"<p>Get a configuration for the LSTM-DANN approach.</p> <p>The configuration can be modified and fed to lstm_dann_from_config to create the approach.</p> <p>Parameters:</p> Name Type Description Default <code>source_fd</code> <code>int</code> <p>The source FD of CMAPSS.</p> required <code>target_fd</code> <code>int</code> <p>The target FD of CMAPSS.</p> required <p>Returns:     The LSTM-DANN configuration.</p>"},{"location":"api/rul_adapt/construct/lstm_dann/functional/#rul_adapt.construct.lstm_dann.functional.lstm_dann_from_config","title":"<code>lstm_dann_from_config(config, **trainer_kwargs)</code>","text":"<p>Construct a LSTM-DANN approach from a configuration.</p> <p>The configuration can be created by calling get_lstm_dann_config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>The LSTM-DANN configuration.</p> required <code>trainer_kwargs</code> <code>Any</code> <p>Overrides for the trainer class.</p> <code>{}</code> <p>Returns:     dm: The data module for adaption of two CMAPSS sub-datasets.     dann: The DANN approach with feature extractor, regressor and domain disc.     trainer: The trainer object.</p>"},{"location":"api/rul_adapt/construct/tbigru/","title":"tbigru","text":""},{"location":"api/rul_adapt/construct/tbigru/functional/","title":"functional","text":""},{"location":"api/rul_adapt/construct/tbigru/functional/#rul_adapt.construct.tbigru.functional.get_tbigru","title":"<code>get_tbigru(source_fd, target_fd, **trainer_kwargs)</code>","text":"<p>Construct a TBiGRU approach for FEMTO with the original hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import rul_adapt\n&gt;&gt;&gt; dm, tbigru, trainer = rul_adapt.construct.get_tbigru(3, 1)\n&gt;&gt;&gt; trainer.fit(tbigru, dm)\n&gt;&gt;&gt; trainer.test(tbigru, dm)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source_fd</code> <code>int</code> <p>The source FD of FEMTO.</p> required <code>target_fd</code> <code>int</code> <p>The target FD of FEMTO.</p> required <code>trainer_kwargs</code> <code>Any</code> <p>Overrides for the trainer class.</p> <code>{}</code> <p>Returns:     dm: The data module for adaption of two FEMTO sub-datasets.     dann: The TBiGRU approach with feature extractor and regressor.     trainer: The trainer object.</p>"},{"location":"api/rul_adapt/construct/tbigru/functional/#rul_adapt.construct.tbigru.functional.get_tbigru_config","title":"<code>get_tbigru_config(source_fd, target_fd)</code>","text":"<p>Get a configuration for the TBiGRU approach.</p> <p>The configuration can be modified and fed to tbigru_from_config to create the approach.</p> <p>Parameters:</p> Name Type Description Default <code>source_fd</code> <code>int</code> <p>The source FD of FEMTO.</p> required <code>target_fd</code> <code>int</code> <p>The target FD of FEMTO.</p> required <p>Returns:     The TBiGRU configuration.</p>"},{"location":"api/rul_adapt/construct/tbigru/functional/#rul_adapt.construct.tbigru.functional.tbigru_from_config","title":"<code>tbigru_from_config(config, **trainer_kwargs)</code>","text":"<p>Construct a TBiGRU approach from a configuration. The configuration can be created by calling get_tbigru_config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>The TBiGRU configuration.</p> required <code>trainer_kwargs</code> <code>Any</code> <p>Overrides for the trainer class.</p> <code>{}</code> <p>Returns:     dm: The data module for adaption of two FEMTO sub-datasets.     dann: The TBiGRU approach with feature extractor and regressor.     trainer: The trainer object.</p>"},{"location":"api/rul_adapt/loss/","title":"loss","text":""},{"location":"api/rul_adapt/loss/adaption/","title":"adaption","text":"<p>A module for unsupervised domain adaption losses.</p>"},{"location":"api/rul_adapt/loss/adaption/#rul_adapt.loss.adaption.DomainAdversarialLoss","title":"<code>DomainAdversarialLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>The Domain Adversarial Neural Network Loss (DANN) uses a domain discriminator to measure the distance between two feature distributions.</p> <p>The domain discriminator is a neural network that is jointly trained on classifying its input as one of two domains. Its output should be a single unscaled score (logit) which is fed to a binary cross entropy loss.</p> <p>The domain discriminator is preceded by a GradientReversalLayer. This way, the discriminator is trained to separate the domains while the network generating the inputs is trained to marginalize the domain difference.</p>"},{"location":"api/rul_adapt/loss/adaption/#rul_adapt.loss.adaption.DomainAdversarialLoss.__init__","title":"<code>__init__(domain_disc)</code>","text":"<p>Create a new DANN loss module.</p> <p>Parameters:</p> Name Type Description Default <code>domain_disc</code> <code>Module</code> <p>The neural network to act as the domain discriminator.</p> required"},{"location":"api/rul_adapt/loss/adaption/#rul_adapt.loss.adaption.DomainAdversarialLoss.update","title":"<code>update(source, target)</code>","text":"<p>Calculate the DANN loss as the binary cross entropy of the discriminators prediction for the source and target features.</p> <p>The source features receive a domain label of zero and the target features a domain label of one.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Tensor</code> <p>The source features with domain label zero.</p> required <code>target</code> <code>Tensor</code> <p>The target features with domain label one.</p> required"},{"location":"api/rul_adapt/loss/adaption/#rul_adapt.loss.adaption.GradientReversalLayer","title":"<code>GradientReversalLayer</code>","text":"<p>             Bases: <code>Module</code></p> <p>The gradient reversal layer (GRL) acts as an identity function in the forward pass and scales the gradient by a negative scalar in the backward pass.</p> <pre><code>GRL(f(x)) = f(x)\nGRL`(f(x)) = -lambda * f`(x)\n</code></pre>"},{"location":"api/rul_adapt/loss/adaption/#rul_adapt.loss.adaption.GradientReversalLayer.__init__","title":"<code>__init__(grad_weight=1.0)</code>","text":"<p>Create a new Gradient Reversal Layer.</p> <p>Parameters:</p> Name Type Description Default <code>grad_weight</code> <code>float</code> <p>The scalar that weights the negative gradient.</p> <code>1.0</code>"},{"location":"api/rul_adapt/loss/adaption/#rul_adapt.loss.adaption.JointMaximumMeanDiscrepancyLoss","title":"<code>JointMaximumMeanDiscrepancyLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>The Joint Maximum Mean Discrepancy Loss (JMMD) is a distance measure between multiple pairs of arbitrary distributions.</p> <p>It is related to the MMD insofar as the distance of each distribution pair in a reproducing Hilbert kernel space (RHKS) is calculated and then multiplied before the discrepancy is computed.</p> <pre><code>joint_rhks(xs, ys) = prod(rhks(x, y) for x, y in zip(xs, xs))\n</code></pre> <p>For more information see MaximumMeanDiscrepancyLoss.</p>"},{"location":"api/rul_adapt/loss/adaption/#rul_adapt.loss.adaption.JointMaximumMeanDiscrepancyLoss.__init__","title":"<code>__init__()</code>","text":"<p>Create a new JMMD loss module.</p> <p>It features a single Gaussian kernel with a bandwidth chosen by the median heuristic.</p>"},{"location":"api/rul_adapt/loss/adaption/#rul_adapt.loss.adaption.JointMaximumMeanDiscrepancyLoss.update","title":"<code>update(source_features, target_features)</code>","text":"<p>Compute the JMMD loss between multiple feature distributions.</p> <p>Parameters:</p> Name Type Description Default <code>source_features</code> <code>List[Tensor]</code> <p>The list of source features of shape              <code>[batch_size, num_features]</code>.</p> required <code>target_features</code> <code>List[Tensor]</code> <p>The list of target features of shape              <code>[batch_size, num_features]</code>.</p> required <p>Returns:</p> Type Description <code>None</code> <p>scalar JMMD loss</p>"},{"location":"api/rul_adapt/loss/adaption/#rul_adapt.loss.adaption.MaximumMeanDiscrepancyLoss","title":"<code>MaximumMeanDiscrepancyLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>The Maximum Mean Discrepancy Loss (MMD) is a distance measure between two arbitrary distributions.</p> <p>The distance is defined as the dot product in a reproducing Hilbert kernel space (RHKS) and is zero if and only if the distributions are identical. The RHKS is the space of the linear combination of multiple Gaussian kernels with bandwidths derived by the median heuristic.</p> <p>The source and target feature batches are treated as samples from their respective distribution. The linear pairwise distances between the two batches are transformed into distances in the RHKS via the kernel trick:</p> <pre><code>rhks(x, y) = dot(to_rhks(x), to_rhks(y)) = multi_kernel(dot(x, y))\nmulti_kernel(distance) = mean([gaussian(distance, bw) for bw in bandwidths])\ngaussian(distance, bandwidth) = exp(-distance * bandwidth)\n</code></pre> <p>The n kernels will use bandwidths between <code>median / (2**(n/ 2))</code> and <code>median * ( 2**(n / 2))</code>, where <code>median</code> is the median of the linear distances.</p> <p>The MMD loss is then calculated as:</p> <pre><code>mean(rhks(source, source) + rhks(target, target) - 2 * rhks(source, target))\n</code></pre> <p>This version of MMD is biased, which is acceptable for training purposes.</p>"},{"location":"api/rul_adapt/loss/adaption/#rul_adapt.loss.adaption.MaximumMeanDiscrepancyLoss.__init__","title":"<code>__init__(num_kernels)</code>","text":"<p>Create a new MMD loss module with <code>n</code> kernels.</p> <p>The bandwidths of the Gaussian kernels are derived by the median heuristic.</p> <p>Parameters:</p> Name Type Description Default <code>num_kernels</code> <code>int</code> <p>Number of Gaussian kernels to use.</p> required"},{"location":"api/rul_adapt/loss/adaption/#rul_adapt.loss.adaption.MaximumMeanDiscrepancyLoss.update","title":"<code>update(source_features, target_features)</code>","text":"<p>Compute the MMD loss between source and target feature distributions.</p> <p>Parameters:</p> Name Type Description Default <code>source_features</code> <code>Tensor</code> <p>Source features with shape <code>[batch_size, num_features]</code></p> required <code>target_features</code> <code>Tensor</code> <p>Target features with shape <code>[batch_size, num_features]</code></p> required <p>Returns:</p> Type Description <code>None</code> <p>scalar MMD loss</p>"},{"location":"api/rul_adapt/loss/alignment/","title":"alignment","text":"<p>These losses are used to create a latent space that is conductive to RUL estimation. They are mainly used by the LatentAlignmentApproach.</p>"},{"location":"api/rul_adapt/loss/alignment/#rul_adapt.loss.alignment.DegradationDirectionAlignmentLoss","title":"<code>DegradationDirectionAlignmentLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>This loss is used to align the direction of the degradation data in relation to the healthy state data in the latent space.</p> <p>It computes the mean of the cosine of the pairwise-angle of the vectors from the healthy state cluster to each degradation data point. The healthy state cluster location is assumed to be the mean of the healthy state data in the latent space. The loss is negated in order to maximize the cosine by minimizing the loss.</p> <p>The loss is implemented as a torchmetrics.Metric. See their documentation for more information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt.loss.alignment import DegradationDirectionAlignmentLoss\n&gt;&gt;&gt; degradation_align = DegradationDirectionAlignmentLoss()\n&gt;&gt;&gt; degradation_align(torch.zeros(10, 5), torch.ones(10, 5))\ntensor(-1.0000)\n</code></pre>"},{"location":"api/rul_adapt/loss/alignment/#rul_adapt.loss.alignment.DegradationLevelRegularizationLoss","title":"<code>DegradationLevelRegularizationLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>This loss is used to regularize the degradation level of the data in the latent space in relation to the healthy state data.</p> <p>It computes the mean of the difference between the normalized distance of the degradation data points from the healthy state cluster and the normalized degradation steps. The healthy state cluster location is assumed to be the mean of the healthy state data in the latent space.</p> <p>The loss is implemented as a torchmetrics.Metric. See their documentation for more information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt.loss.alignment import DegradationLevelRegularizationLoss\n&gt;&gt;&gt; degradation_align = DegradationLevelRegularizationLoss()\n&gt;&gt;&gt; degradation_align(torch.zeros(10, 5),\n...                   torch.ones(10, 5),\n...                   torch.ones(10),\n...                   torch.ones(10, 5),\n...                   torch.ones(10))\ntensor(0.)\n</code></pre>"},{"location":"api/rul_adapt/loss/alignment/#rul_adapt.loss.alignment.HealthyStateAlignmentLoss","title":"<code>HealthyStateAlignmentLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>This loss is used to align the healthy state of the data in the latent space.</p> <p>It computes the mean of the variance of each latent feature which is supposed to be minimized. This way a single compact cluster of healthy state data should be formed.</p> <p>The loss is implemented as a torchmetrics.Metric. See their documentation for more information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rul_adapt.loss.alignment import HealthyStateAlignmentLoss\n&gt;&gt;&gt; healthy_align = HealthyStateAlignmentLoss()\n&gt;&gt;&gt; healthy_align(torch.zeros(10, 5))\ntensor(0.)\n</code></pre>"},{"location":"api/rul_adapt/loss/conditional/","title":"conditional","text":"<p>A module for conditional unsupervised domain adaption losses.</p>"},{"location":"api/rul_adapt/loss/conditional/#rul_adapt.loss.conditional.ConditionalAdaptionLoss","title":"<code>ConditionalAdaptionLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>The Conditional Adaptions loss is a combination of multiple losses, each of which is only applied to a subset of the incoming data.</p> <p>The subsets are defined by fuzzy sets with a rectangular membership function. The prediction for each sample is checked against the fuzzy sets, and the corresponding loss is applied to the sample. The combined loss can be set as the sum of all components or their mean.</p>"},{"location":"api/rul_adapt/loss/conditional/#rul_adapt.loss.conditional.ConditionalAdaptionLoss.__init__","title":"<code>__init__(adaption_losses, fuzzy_sets, mean_over_sets=True)</code>","text":"<p>Create a new Conditional Adaption loss over fuzzy sets.</p> <p>The fuzzy sets' boundaries are inclusive on the left and exclusive on the right. The left boundary is supposed to be smaller than the right boundary.</p> <p>This loss should not be used as a way to accumulate its value over multiple batches.</p> <p>Parameters:</p> Name Type Description Default <code>adaption_losses</code> <code>Sequence[Metric]</code> <p>The list of losses to be applied to the subsets.</p> required <code>fuzzy_sets</code> <code>List[Tuple[float, float]]</code> <p>The fuzzy sets to be used for subset membership.</p> required <code>mean_over_sets</code> <code>bool</code> <p>Whether to take the mean or the sum of the losses.</p> <code>True</code>"},{"location":"api/rul_adapt/loss/conditional/#rul_adapt.loss.conditional.ConditionalAdaptionLoss.compute","title":"<code>compute()</code>","text":"<p>Compute the loss as either the sum or mean of all subset losses.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The combined loss.</p>"},{"location":"api/rul_adapt/loss/conditional/#rul_adapt.loss.conditional.ConditionalAdaptionLoss.update","title":"<code>update(source, source_preds, target, target_preds)</code>","text":"<p>Update the loss with the given data.</p> <p>The predictions for the source and target data are checked against the fuzzy sets to determine membership.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Tensor</code> <p>The source features.</p> required <code>source_preds</code> <code>Tensor</code> <p>The predictions for the source features.</p> required <code>target</code> <code>Tensor</code> <p>The target features.</p> required <code>target_preds</code> <code>Tensor</code> <p>The predictions for the target features.</p> required"},{"location":"api/rul_adapt/loss/rul/","title":"rul","text":""},{"location":"api/rul_adapt/loss/utils/","title":"utils","text":""},{"location":"api/rul_adapt/model/","title":"model","text":"<p>This module contains the necessary neural networks to build a RUL estimator.</p> <p>In general, a RUL estimator network consists of two parts: a feature extractor and a regression head. The feature extractor is a network that transforms the input feature windows into a latent feature vector. The regression head maps these latent features to a scalar RUL value. The feature extractors can be found in the cnn and rnn modules. The regression head in the head module.</p> <p>Some domain adaption approaches use a domain discriminator. The networks in the head module can be used to construct them, too.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from rul_adapt import model\n&gt;&gt;&gt; feature_extractor = model.CnnExtractor(14,[32, 16],30,fc_units=8)\n&gt;&gt;&gt; regressor = model.FullyConnectedHead(8, [4, 1])\n&gt;&gt;&gt; latent_features = feature_extractor(torch.randn(10, 14, 30))\n&gt;&gt;&gt; latent_features.shape\ntorch.Size([10, 8])\n&gt;&gt;&gt; rul = regressor(latent_features)\n&gt;&gt;&gt; rul.shape\ntorch.Size([10, 1])\n</code></pre>"},{"location":"api/rul_adapt/model/cnn/","title":"cnn","text":"<p>A module of feature extractors based on convolutional neural networks.</p>"},{"location":"api/rul_adapt/model/cnn/#rul_adapt.model.cnn.CnnExtractor","title":"<code>CnnExtractor</code>","text":"<p>             Bases: <code>Module</code></p> <p>A Convolutional Neural Network (CNN) based network that extracts a feature vector from same-length time windows.</p> <p>This feature extractor consists of multiple CNN layers and an optional fully connected (FC) layer. Each CNN layer can be configured with a number of filters and a kernel size. Additionally, batch normalization, same-padding and dropout can be applied. The fully connected layer can have a separate dropout probability.</p> <p>Both CNN and FC layers use ReLU activation functions by default. Custom activation functions can be set for each layer type.</p> <p>The data flow is as follows: <code>Input --&gt; CNN x n --&gt; [FC] --&gt; Output</code></p> <p>The expected input shape is <code>[batch_size, num_features, window_size]</code>. The output of this network is always flattened to <code>[batch_size, num_extracted_features]</code>.</p> <p>Examples:</p> <pre><code>Without FC\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from rul_adapt.model import CnnExtractor\n&gt;&gt;&gt; cnn = CnnExtractor(14,units=[16, 1],seq_len=30)\n&gt;&gt;&gt; cnn(torch.randn(10, 14, 30)).shape\ntorch.Size([10, 26])\n\nWith FC\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from rul_adapt.model import CnnExtractor\n&gt;&gt;&gt; cnn = CnnExtractor(14,units=[16, 1],seq_len=30,fc_units=16)\n&gt;&gt;&gt; cnn(torch.randn(10, 14, 30)).shape\ntorch.Size([10, 16])\n</code></pre>"},{"location":"api/rul_adapt/model/cnn/#rul_adapt.model.cnn.CnnExtractor.__init__","title":"<code>__init__(input_channels, units, seq_len, kernel_size=3, dilation=1, stride=1, padding=False, fc_units=None, dropout=0.0, fc_dropout=0.0, batch_norm=False, act_func=nn.ReLU, fc_act_func=nn.ReLU)</code>","text":"<p>Create a new CNN-based feature extractor.</p> <p>The <code>units</code> are the number of output filters for each CNN layer. The <code>seq_len</code> is needed to calculate the input units for the FC layer. The kernel size of each CNN layer can be set by passing a list to <code>kernel_size</code>. If an integer is passed, each layer has the same kernel size. If <code>padding</code> is true, same-padding is applied before each CNN layer, which keeps the window_size the same. If <code>batch_norm</code> is set, batch normalization is applied for each CNN layer. If <code>fc_units</code> is set, a fully connected layer is appended.</p> <p>Dropout can be applied to each CNN layer by setting <code>conv_dropout</code> to a number greater than zero. The same is valid for the fully connected layer and <code>fc_dropout</code>. Dropout will never be applied to the input layer.</p> <p>The whole network uses ReLU activation functions. This can be customized by setting either <code>conv_act_func</code> or <code>fc_act_func</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>The number of input channels.</p> required <code>units</code> <code>List[int]</code> <p>The list of output filters for the CNN layers.</p> required <code>seq_len</code> <code>int</code> <p>The window_size of the input data.</p> required <code>kernel_size</code> <code>Union[int, List[int]]</code> <p>The kernel size for the CNN layers. Passing an integer uses          the same kernel size for each layer.</p> <code>3</code> <code>dilation</code> <code>int</code> <p>The dilation for the CNN layers.</p> <code>1</code> <code>stride</code> <code>int</code> <p>The stride for the CNN layers.</p> <code>1</code> <code>padding</code> <code>bool</code> <p>Whether to apply same-padding before each CNN layer.</p> <code>False</code> <code>fc_units</code> <code>Optional[int]</code> <p>Number of output units for the fully connected layer.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>The dropout probability for the CNN layers.</p> <code>0.0</code> <code>fc_dropout</code> <code>float</code> <p>The dropout probability for the fully connected layer.</p> <code>0.0</code> <code>batch_norm</code> <code>bool</code> <p>Whether to use batch normalization on the CNN layers.</p> <code>False</code> <code>act_func</code> <code>Type[Module]</code> <p>The activation function for the CNN layers.</p> <code>ReLU</code> <code>fc_act_func</code> <code>Type[Module]</code> <p>The activation function for the fully connected layer.</p> <code>ReLU</code>"},{"location":"api/rul_adapt/model/head/","title":"head","text":"<p>A module for network working as a regression or classification head.</p>"},{"location":"api/rul_adapt/model/head/#rul_adapt.model.head.FullyConnectedHead","title":"<code>FullyConnectedHead</code>","text":"<p>             Bases: <code>Module</code></p> <p>A fully connected (FC) network that can be used as a RUL regressor or a domain discriminator.</p> <p>This network is a stack of fully connected layers with ReLU activation functions by default. The activation function can be customized through the <code>act_func</code> parameter. If the last layer of the network should not have an activation function, <code>act_func_on_last_layer</code> can be set to <code>False</code>.</p> <p>The data flow is as follows: <code>Inputs --&gt; FC x n --&gt; Outputs</code></p> <p>The expected input shape is <code>[batch_size, num_features]</code>.</p> <p>Examples:</p> <p>Default</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from rul_adapt.model import FullyConnectedHead\n&gt;&gt;&gt; regressor = FullyConnectedHead(32, [16, 1])\n&gt;&gt;&gt; outputs = regressor(torch.randn(10, 32))\n&gt;&gt;&gt; outputs.shape\ntorch.Size([10, 1])\n&gt;&gt;&gt; type(outputs.grad_fn)\n&lt;class 'ReluBackward0'&gt;\n</code></pre> <p>Custom activation function</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from rul_adapt.model import FullyConnectedHead\n&gt;&gt;&gt; regressor = FullyConnectedHead(32, [16, 1], act_func=torch.nn.Sigmoid)\n&gt;&gt;&gt; outputs = regressor(torch.randn(10, 32))\n&gt;&gt;&gt; type(outputs.grad_fn)\n&lt;class 'SigmoidBackward0'&gt;\n</code></pre> <p>Without activation function on last layer</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from rul_adapt.model import FullyConnectedHead\n&gt;&gt;&gt; regressor = FullyConnectedHead(32, [16, 1], act_func_on_last_layer=False)\n&gt;&gt;&gt; outputs = regressor(torch.randn(10, 32))\n&gt;&gt;&gt; type(outputs.grad_fn)\n&lt;class 'AddmmBackward0'&gt;\n</code></pre>"},{"location":"api/rul_adapt/model/head/#rul_adapt.model.head.FullyConnectedHead.__init__","title":"<code>__init__(input_channels, units, dropout=0.0, act_func=nn.ReLU, act_func_on_last_layer=True)</code>","text":"<p>Create a new fully connected head network.</p> <p>The <code>units</code> are the number of output units for each FC layer. The number of output features is <code>units[-1]</code>. If dropout is used, it is applied in each layer, including input.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>The number of input channels.</p> required <code>units</code> <code>List[int]</code> <p>The number of output units for the FC layers.</p> required <code>dropout</code> <code>float</code> <p>The dropout probability before each layer. Set to zero to      deactivate.</p> <code>0.0</code> <code>act_func</code> <code>Type[Module]</code> <p>The activation function for each layer.</p> <code>ReLU</code> <code>act_func_on_last_layer</code> <code>bool</code> <p>Whether to add the activation function to the last                     layer.</p> <code>True</code>"},{"location":"api/rul_adapt/model/rnn/","title":"rnn","text":"<p>A module of feature extractors based on recurrent neural networks.</p>"},{"location":"api/rul_adapt/model/rnn/#rul_adapt.model.rnn.GruExtractor","title":"<code>GruExtractor</code>","text":"<p>             Bases: <code>Module</code></p> <p>A Gated Recurrent Unit (GRU) based network that extracts a feature vector from same-length time windows.</p> <p>This feature extractor consists of multiple fully connected (FC) layers with a ReLU activation functions and a multi-layer GRU. The GRU layers can be configured as bidirectional. Dropout can be applied separately to the GRU layers.</p> <p>The data flow is as follows: <code>Input --&gt; FC x n --&gt; GRU x m --&gt; Output</code></p> <p>The expected input shape is <code>[batch_size, num_features, window_size]</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from rul_adapt.model import GruExtractor\n&gt;&gt;&gt; gru = GruExtractor(input_channels=14, fc_units=[16, 8], gru_units=[8])\n&gt;&gt;&gt; gru(torch.randn(10, 14, 30)).shape\ntorch.Size([10, 8])\n</code></pre>"},{"location":"api/rul_adapt/model/rnn/#rul_adapt.model.rnn.GruExtractor.__init__","title":"<code>__init__(input_channels, fc_units, gru_units, gru_dropout=0.0, bidirectional=False)</code>","text":"<p>Create a new GRU-based feature extractor.</p> <p>The <code>fc_units</code> are the output units for each fully connected layer and <code>gru_units</code> for each LSTM layer. If <code>bidirectional</code> is set to <code>True</code>, a BiGRU is used and the output units are doubled. The number of output features of this network is either <code>gru_units[-1]</code> by default, or <code>2 * gru_units[ -1]</code> if bidirectional is set.</p> <p>Dropout can be applied to each GRU layer by setting <code>lstm_dropout</code> to a number greater than zero.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>The number of input channels.</p> required <code>fc_units</code> <code>List[int]</code> <p>The list of output units for the fully connected layers.</p> required <code>gru_units</code> <code>List[int]</code> <p>The list of output units for the GRU layers</p> required <code>gru_dropout</code> <code>float</code> <p>The dropout probability for the GRU layers.</p> <code>0.0</code> <code>bidirectional</code> <code>bool</code> <p>Whether to use a BiGRU.</p> <code>False</code>"},{"location":"api/rul_adapt/model/rnn/#rul_adapt.model.rnn.LstmExtractor","title":"<code>LstmExtractor</code>","text":"<p>             Bases: <code>Module</code></p> <p>A Long Short Term Memory (LSTM) based network that extracts a feature vector from same-length time windows.</p> <p>This feature extractor consists of a multi-layer LSTM and an optional fully connected (FC) layer with a ReLU activation function. The LSTM layers can be configured as bidirectional. Dropout can be applied separately to LSTM and FC layers.</p> <p>The data flow is as follows: <code>Input --&gt; LSTM x n --&gt; [FC] --&gt; Output</code></p> <p>The expected input shape is <code>[batch_size, num_features, window_size]</code>.</p> <p>Examples:</p> <pre><code>Without FC\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from rul_adapt.model import LstmExtractor\n&gt;&gt;&gt; lstm = LstmExtractor(input_channels=14,units=[16, 16])\n&gt;&gt;&gt; lstm(torch.randn(10, 14, 30)).shape\ntorch.Size([10, 16])\n\nWith FC\n&gt;&gt;&gt; from rul_adapt.model import LstmExtractor\n&gt;&gt;&gt; lstm = LstmExtractor(input_channels=14,units=[16, 16],fc_units=8)\n&gt;&gt;&gt; lstm(torch.randn(10, 14, 30)).shape\ntorch.Size([10, 8])\n</code></pre>"},{"location":"api/rul_adapt/model/rnn/#rul_adapt.model.rnn.LstmExtractor.__init__","title":"<code>__init__(input_channels, units, fc_units=None, dropout=0.0, fc_dropout=0.0, bidirectional=False)</code>","text":"<p>Create a new LSTM-based feature extractor.</p> <p>The <code>units</code> are the output units for each LSTM layer. If <code>bidirectional</code> is set to <code>True</code>, a BiLSTM is used and the output units are doubled. If <code>fc_units</code> is set, a fully connected layer is appended. The number of output features of this network is either <code>units[-1]</code> by default, <code>2 * units[ -1]</code> if bidirectional is set, or <code>fc_units</code> if it is set.</p> <p>Dropout can be applied to each LSTM layer by setting <code>lstm_dropout</code> to a number greater than zero. The same is valid for the fully connected layer and <code>fc_dropout</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>The number of input channels.</p> required <code>units</code> <code>List[int]</code> <p>The list of output units for the LSTM layers.</p> required <code>fc_units</code> <code>Optional[int]</code> <p>The number of output units for the fully connected layer.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>The dropout probability for the LSTM layers.</p> <code>0.0</code> <code>fc_dropout</code> <code>float</code> <p>The dropout probability for the fully connected layer.</p> <code>0.0</code> <code>bidirectional</code> <code>bool</code> <p>Whether to use a BiLSTM.</p> <code>False</code>"},{"location":"api/rul_adapt/model/two_stage/","title":"two_stage","text":""},{"location":"api/rul_adapt/model/two_stage/#rul_adapt.model.two_stage.TwoStageExtractor","title":"<code>TwoStageExtractor</code>","text":"<p>             Bases: <code>Module</code></p> <p>This module combines two feature extractors into a single network.</p> <p>The input data is expected to be of shape <code>[batch_size, upper_seq_len, input_channels, lower_seq_len]</code>. An example would be vibration data recorded in spaced intervals, where lower_seq_len is the length of an interval and upper_seq_len is the window size of a sliding window over the intervals.</p> <p>The lower_stage is applied to each interval individually to extract features. The upper_stage is then applied to the extracted features of the window. The resulting feature vector should represent the window without the need to manually extract features from the raw data of the intervals.</p>"},{"location":"api/rul_adapt/model/two_stage/#rul_adapt.model.two_stage.TwoStageExtractor.__init__","title":"<code>__init__(lower_stage, upper_stage)</code>","text":"<p>Create a new two-stage extractor.</p> <p>The lower stage needs to take a tensor of shape <code>[batch_size, input_channels, seq_len]</code> and return a tensor of shape <code>[batch_size, lower_output_units]</code>. The upper stage needs to take a tensor of shape <code>[batch_size, upper_seq_len, lower_output_units]</code> and return a tensor of shape <code>[batch_size, upper_output_units]</code>. Args: lower_stage: upper_stage:</p>"},{"location":"api/rul_adapt/model/two_stage/#rul_adapt.model.two_stage.TwoStageExtractor.forward","title":"<code>forward(inputs)</code>","text":"<p>Apply the two-stage extractor to the input tensor.</p> <p>The input tensor is expected to be of shape <code>[batch_size, upper_seq_len, input_channels, lower_seq_len]</code>. The output tensor will be of shape <code>[batch_size, upper_output_units]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>the input tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>an output tensor of shape <code>[batch_size, upper_output_units]</code></p>"},{"location":"api/rul_adapt/model/wrapper/","title":"wrapper","text":""},{"location":"examples/","title":"Examples","text":"<ul> <li>ADARUL</li> <li>LSTM-DANN</li> <li>CNN-DANN</li> <li>Conditional Approaches</li> <li>Consistency-DANN</li> <li>LatentAlign</li> <li>Psuedo Labels</li> <li>TBiGRU</li> </ul>"},{"location":"examples/adarul/","title":"ADARUL","text":"In\u00a0[1]: Copied! <pre>import rul_datasets\nimport rul_adapt\nimport pytorch_lightning as pl\nimport omegaconf\n</pre> import rul_datasets import rul_adapt import pytorch_lightning as pl import omegaconf In\u00a0[2]: Copied! <pre>pl.seed_everything(42, workers=True)  # makes is reproducible\npre_training, main_training = rul_adapt.construct.get_adarul(\n    3, 1, {\"max_epochs\": 1}, {\"max_epochs\": 1}\n)\n</pre> pl.seed_everything(42, workers=True)  # makes is reproducible pre_training, main_training = rul_adapt.construct.get_adarul(     3, 1, {\"max_epochs\": 1}, {\"max_epochs\": 1} ) <pre>Global seed set to 42\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</pre> <p>The function returns two tuples. The first contains everything needed for pre-training, the second everything needed for the main training.</p> In\u00a0[3]: Copied! <pre>pre_dm, pre_approach, pre_trainer = pre_training\npre_trainer.fit(pre_approach, pre_dm)\n</pre> pre_dm, pre_approach, pre_trainer = pre_training pre_trainer.fit(pre_approach, pre_dm) <pre>\n  | Name               | Type                     | Params\n----------------------------------------------------------------\n0 | train_loss         | MeanSquaredError         | 0     \n1 | val_loss           | MeanSquaredError         | 0     \n2 | test_loss          | MeanSquaredError         | 0     \n3 | evaluator          | AdaptionEvaluator        | 0     \n4 | _feature_extractor | ActivationDropoutWrapper | 62.5 K\n5 | _regressor         | FullyConnectedHead       | 6.3 K \n----------------------------------------------------------------\n68.7 K    Trainable params\n0         Non-trainable params\n68.7 K    Total params\n0.275     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <p>After pre-training, we can use the pre-trained networks to initialize the main training. The networks of the pre-training approach, i.e. <code>feature_extractor</code> and <code>regressor</code>, can be accessed as properties.</p> In\u00a0[4]: Copied! <pre>dm, approach, domain_disc, trainer = main_training\napproach.set_model(pre_approach.feature_extractor, pre_approach.regressor, domain_disc)\ntrainer.fit(approach, dm)\ntrainer.test(approach, dm)\n</pre> dm, approach, domain_disc, trainer = main_training approach.set_model(pre_approach.feature_extractor, pre_approach.regressor, domain_disc) trainer.fit(approach, dm) trainer.test(approach, dm) <pre>\n  | Name                     | Type                     | Params\n----------------------------------------------------------------------\n0 | gan_loss                 | BCEWithLogitsLoss        | 0     \n1 | evaluator                | AdaptionEvaluator        | 0     \n2 | _feature_extractor       | ActivationDropoutWrapper | 62.5 K\n3 | _regressor               | FullyConnectedHead       | 6.3 K \n4 | _domain_disc             | FullyConnectedHead       | 6.3 K \n5 | frozen_feature_extractor | ActivationDropoutWrapper | 62.5 K\n----------------------------------------------------------------------\n75.0 K    Trainable params\n62.5 K    Non-trainable params\n137 K     Total params\n0.550     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse        24.635229110717773\n    test/source/score       1310.8724365234375\n    test/target/rmse                                 31.754472732543945\n    test/target/score                                 2988.716064453125\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[4]: <pre>[{'test/source/rmse/dataloader_idx_0': 24.635229110717773,\n  'test/source/score/dataloader_idx_0': 1310.8724365234375},\n {'test/target/rmse/dataloader_idx_1': 31.754472732543945,\n  'test/target/score/dataloader_idx_1': 2988.716064453125}]</pre> <p>If you only want to see the hyperparameters, you can use the <code>get_adarul_config</code> function. This returns an <code>omegaconf.DictConfig</code> which you can modify. Afterwards, you can pass the config to <code>adarul_from_config</code> to receive the training-ready approach.</p> In\u00a0[5]: Copied! <pre>three2one_config = rul_adapt.construct.get_adarul_config(3, 1)\nprint(omegaconf.OmegaConf.to_yaml(three2one_config, resolve=True))\n</pre> three2one_config = rul_adapt.construct.get_adarul_config(3, 1) print(omegaconf.OmegaConf.to_yaml(three2one_config, resolve=True)) <pre>dm:\n  source:\n    _target_: rul_datasets.CmapssReader\n    fd: 3\n    window_size: 30\n    max_rul: 130\n    operation_condition_aware_scaling: true\n  target:\n    fd: 1\n    percent_broken: 1.0\n  batch_size: 10\nfeature_extractor:\n  _convert_: all\n  _target_: rul_adapt.model.ActivationDropoutWrapper\n  wrapped:\n    _target_: rul_adapt.model.LstmExtractor\n    input_channels: 14\n    units:\n    - 32\n    - 32\n    - 32\n    bidirectional: true\n  dropout: 0.5\nregressor:\n  _convert_: all\n  _target_: rul_adapt.model.FullyConnectedHead\n  input_channels: 64\n  act_func_on_last_layer: false\n  units:\n  - 64\n  - 32\n  - 1\n  dropout: 0.5\ndomain_disc:\n  _convert_: all\n  _target_: rul_adapt.model.FullyConnectedHead\n  input_channels: 64\n  act_func_on_last_layer: false\n  units:\n  - 64\n  - 32\n  - 1\nadarul_pre:\n  _target_: rul_adapt.approach.SupervisedApproach\n  lr: 0.0001\n  loss_type: mse\n  optim_type: adam\n  rul_scale: 130\nadarul:\n  _target_: rul_adapt.approach.AdaRulApproach\n  lr: 0.0001\n  max_rul: 130\n  num_disc_updates: 35\n  num_gen_updates: 1\ntrainer_pre:\n  _target_: pytorch_lightning.Trainer\n  max_epochs: 5\n  gradient_clip_val: 1.0\ntrainer:\n  _target_: pytorch_lightning.Trainer\n  max_epochs: 20\n  limit_train_batches: 36\n</pre> In\u00a0[7]: Copied! <pre>source = rul_datasets.CmapssReader(3)\ntarget = source.get_compatible(1, percent_broken=0.8)\n\npre_dm = rul_datasets.RulDataModule(source, batch_size=32)\ndm = rul_datasets.DomainAdaptionDataModule(\n    pre_dm, rul_datasets.RulDataModule(target, batch_size=32),\n)\n\nfeature_extractor = rul_adapt.model.CnnExtractor(\n    input_channels=14,\n    units=[16, 16, 16],\n    seq_len=30,\n    fc_units=8,\n)\nregressor = rul_adapt.model.FullyConnectedHead(\n    input_channels=8,\n    units=[8, 1],\n    act_func_on_last_layer=False,\n)\ndomain_disc = rul_adapt.model.FullyConnectedHead(\n    input_channels=8,\n    units=[8, 1],\n    act_func_on_last_layer=False,\n)\n\npre_approach = rul_adapt.approach.SupervisedApproach(\n    lr=0.001, loss_type=\"mse\", optim_type=\"adam\", rul_scale=source.max_rul\n)\npre_approach.set_model(feature_extractor, regressor)\npre_trainer = pl.Trainer(max_epochs=1)\ntrainer.fit(pre_approach, pre_dm)\n\napproach = rul_adapt.approach.AdaRulApproach(\n    lr=0.001,\n    max_rul=source.max_rul,\n    num_disc_updates=35,\n    num_gen_updates=1,\n)\napproach.set_model(\n    pre_approach.feature_extractor, pre_approach.regressor, domain_disc\n)\ntrainer = pl.Trainer(max_epochs=1)\ntrainer.fit(approach, dm)\ntrainer.test(approach, dm)\n</pre> source = rul_datasets.CmapssReader(3) target = source.get_compatible(1, percent_broken=0.8)  pre_dm = rul_datasets.RulDataModule(source, batch_size=32) dm = rul_datasets.DomainAdaptionDataModule(     pre_dm, rul_datasets.RulDataModule(target, batch_size=32), )  feature_extractor = rul_adapt.model.CnnExtractor(     input_channels=14,     units=[16, 16, 16],     seq_len=30,     fc_units=8, ) regressor = rul_adapt.model.FullyConnectedHead(     input_channels=8,     units=[8, 1],     act_func_on_last_layer=False, ) domain_disc = rul_adapt.model.FullyConnectedHead(     input_channels=8,     units=[8, 1],     act_func_on_last_layer=False, )  pre_approach = rul_adapt.approach.SupervisedApproach(     lr=0.001, loss_type=\"mse\", optim_type=\"adam\", rul_scale=source.max_rul ) pre_approach.set_model(feature_extractor, regressor) pre_trainer = pl.Trainer(max_epochs=1) trainer.fit(pre_approach, pre_dm)  approach = rul_adapt.approach.AdaRulApproach(     lr=0.001,     max_rul=source.max_rul,     num_disc_updates=35,     num_gen_updates=1, ) approach.set_model(     pre_approach.feature_extractor, pre_approach.regressor, domain_disc ) trainer = pl.Trainer(max_epochs=1) trainer.fit(approach, dm) trainer.test(approach, dm) <pre>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/tilman/Programming/rul-adapt/docs/examples/lightning_logs/version_24/checkpoints exists and is not empty.\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n\n  | Name               | Type               | Params\n----------------------------------------------------------\n0 | train_loss         | MeanSquaredError   | 0     \n1 | val_loss           | MeanSquaredError   | 0     \n2 | test_loss          | MeanSquaredError   | 0     \n3 | evaluator          | AdaptionEvaluator  | 0     \n4 | _feature_extractor | CnnExtractor       | 5.3 K \n5 | _regressor         | FullyConnectedHead | 81    \n----------------------------------------------------------\n5.4 K     Trainable params\n0         Non-trainable params\n5.4 K     Total params\n0.022     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name                     | Type               | Params\n----------------------------------------------------------------\n0 | gan_loss                 | BCEWithLogitsLoss  | 0     \n1 | evaluator                | AdaptionEvaluator  | 0     \n2 | _feature_extractor       | CnnExtractor       | 5.3 K \n3 | _regressor               | FullyConnectedHead | 81    \n4 | _domain_disc             | FullyConnectedHead | 81    \n5 | frozen_feature_extractor | CnnExtractor       | 5.3 K \n----------------------------------------------------------------\n5.5 K     Trainable params\n5.3 K     Non-trainable params\n10.8 K    Total params\n0.043     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse         65.23387908935547\n    test/source/score          70147.4921875\n    test/target/rmse                                  66.29417419433594\n    test/target/score                                   64386.578125\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[7]: <pre>[{'test/source/rmse/dataloader_idx_0': 65.23387908935547,\n  'test/source/score/dataloader_idx_0': 70147.4921875},\n {'test/target/rmse/dataloader_idx_1': 66.29417419433594,\n  'test/target/score/dataloader_idx_1': 64386.578125}]</pre>"},{"location":"examples/adarul/#adarul","title":"ADARUL\u00b6","text":""},{"location":"examples/adarul/#reproduce-original-configurations","title":"Reproduce original configurations\u00b6","text":"<p>You can reproduce the original experiments by Ragab et al. by using the <code>get_adarul</code> constructor function.</p> <p>Additional kwargs for the trainer, e.g. accelerator=\"gpu\" for training on a GPU, can be passed to the function as a dictionary. The first dictionary is used for the pre-training trainer and the second one for the main trainer.</p>"},{"location":"examples/adarul/#run-your-own-experiments","title":"Run your own experiments\u00b6","text":"<p>You can use the ADARUL implementation to run your own experiments with different hyperparameters or on different datasets. Here we build an approach with an CNN feature extractor.</p>"},{"location":"examples/cnn_dann/","title":"CNN DANN","text":"In\u00a0[15]: Copied! <pre>import rul_adapt\nimport rul_datasets\nimport pytorch_lightning as pl\nimport omegaconf\n</pre> import rul_adapt import rul_datasets import pytorch_lightning as pl import omegaconf In\u00a0[16]: Copied! <pre>pl.seed_everything(42, workers=True)  # make reproducible\ndm, dann, trainer = rul_adapt.construct.get_cnn_dann(3, 1, max_epochs=1)\n</pre> pl.seed_everything(42, workers=True)  # make reproducible dm, dann, trainer = rul_adapt.construct.get_cnn_dann(3, 1, max_epochs=1) <pre>Global seed set to 42\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</pre> <p>The networks, <code>feature_extractor</code>, <code>regressor</code>, <code>domain_disc</code>, can be accessed as properties of the <code>dann</code> object.</p> In\u00a0[17]: Copied! <pre>dann.feature_extractor\n</pre> dann.feature_extractor Out[17]: <pre>CnnExtractor(\n  (_layers): Sequential(\n    (conv_0): Sequential(\n      (0): Conv1d(14, 10, kernel_size=(10,), stride=(1,), padding=same)\n      (1): Tanh()\n    )\n    (conv_1): Sequential(\n      (0): Conv1d(10, 10, kernel_size=(10,), stride=(1,), padding=same)\n      (1): Tanh()\n    )\n    (conv_2): Sequential(\n      (0): Conv1d(10, 10, kernel_size=(10,), stride=(1,), padding=same)\n      (1): Tanh()\n    )\n    (conv_3): Sequential(\n      (0): Conv1d(10, 10, kernel_size=(10,), stride=(1,), padding=same)\n      (1): Tanh()\n    )\n    (conv_4): Sequential(\n      (0): Conv1d(10, 1, kernel_size=(10,), stride=(1,), padding=same)\n      (1): Tanh()\n    )\n    (5): Flatten(start_dim=1, end_dim=-1)\n  )\n)</pre> <p>Training is done in the PyTorch Lightning fashion. We used the <code>trainer_kwargs</code> to train only one epoch for demonstration purposes.</p> In\u00a0[19]: Copied! <pre>trainer.fit(dann, dm)\ntrainer.test(ckpt_path=\"best\", datamodule=dm)  # loads the best model checkpoint\n</pre> trainer.fit(dann, dm) trainer.test(ckpt_path=\"best\", datamodule=dm)  # loads the best model checkpoint <pre>\n  | Name               | Type                  | Params\n-------------------------------------------------------------\n0 | train_source_loss  | MeanSquaredError      | 0     \n1 | evaluator          | AdaptionEvaluator     | 0     \n2 | _feature_extractor | CnnExtractor          | 4.5 K \n3 | _regressor         | DropoutPrefix         | 3.2 K \n4 | dann_loss          | DomainAdversarialLoss | 1.0 K \n-------------------------------------------------------------\n8.8 K     Trainable params\n0         Non-trainable params\n8.8 K     Total params\n0.035     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (35) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\nRestoring states from the checkpoint path at /home/tilman/Programming/rul-adapt/docs/examples/lightning_logs/version_21/checkpoints/epoch=0-step=35.ckpt\nLoaded model weights from checkpoint at /home/tilman/Programming/rul-adapt/docs/examples/lightning_logs/version_21/checkpoints/epoch=0-step=35.ckpt\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse         73.9958724975586\n    test/source/score          158354.40625\n    test/target/rmse                                  75.42831420898438\n    test/target/score                                   151000.71875\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[19]: <pre>[{'test/source/rmse/dataloader_idx_0': 73.9958724975586,\n  'test/source/score/dataloader_idx_0': 158354.40625},\n {'test/target/rmse/dataloader_idx_1': 75.42831420898438,\n  'test/target/score/dataloader_idx_1': 151000.71875}]</pre> <p>If you only want to see the hyperparameters, you can use the <code>get_lstm_dann_config</code> function. This returns an <code>omegeconf.DictConfig</code> which you can modify.</p> In\u00a0[20]: Copied! <pre>three2one_config = rul_adapt.construct.get_cnn_dann_config(3, 1)\nprint(omegaconf.OmegaConf.to_yaml(three2one_config, resolve=True))\n</pre> three2one_config = rul_adapt.construct.get_cnn_dann_config(3, 1) print(omegaconf.OmegaConf.to_yaml(three2one_config, resolve=True)) <pre>dm:\n  source:\n    _target_: rul_datasets.CmapssReader\n    window_size: 30\n    fd: 3\n  target:\n    fd: 1\n    percent_broken: 1.0\n  batch_size: 512\nfeature_extractor:\n  _convert_: all\n  _target_: rul_adapt.model.CnnExtractor\n  input_channels: 14\n  units:\n  - 10\n  - 10\n  - 10\n  - 10\n  - 1\n  seq_len: 30\n  kernel_size: 10\n  padding: true\n  act_func: torch.nn.Tanh\nregressor:\n  _target_: rul_adapt.model.wrapper.DropoutPrefix\n  wrapped:\n    _convert_: all\n    _target_: rul_adapt.model.FullyConnectedHead\n    input_channels: 30\n    act_func_on_last_layer: false\n    act_func: torch.nn.Tanh\n    units:\n    - 100\n    - 1\n  dropout: 0.5\ndomain_disc:\n  _convert_: all\n  _target_: rul_adapt.model.FullyConnectedHead\n  input_channels: 30\n  act_func_on_last_layer: false\n  units:\n  - 32\n  - 1\n  act_func: torch.nn.Tanh\ndann:\n  _convert_: all\n  _target_: rul_adapt.approach.DannApproach\n  dann_factor: 3.0\n  lr: 0.001\n  loss_type: rmse\n  optim_type: adam\n  optim_betas:\n  - 0.5\n  - 0.999\ntrainer:\n  _target_: pytorch_lightning.Trainer\n  max_epochs: 125\n  callbacks:\n  - _target_: pytorch_lightning.callbacks.ModelCheckpoint\n    save_top_k: 1\n    monitor: val/target/rmse/dataloader_idx_1\n    mode: min\n</pre> In\u00a0[21]: Copied! <pre>source = rul_datasets.CmapssReader(3)\ntarget = source.get_compatible(1, percent_broken=0.8)\ndm = rul_datasets.DomainAdaptionDataModule(\n    rul_datasets.RulDataModule(source, batch_size=32),\n    rul_datasets.RulDataModule(target, batch_size=32),\n)\n\nfeature_extractor = rul_adapt.model.LstmExtractor(\n    input_channels=14,\n    units=[16],\n    fc_units=8,\n)\nregressor = rul_adapt.model.FullyConnectedHead(\n    input_channels=8,\n    units=[8, 1],\n    act_func_on_last_layer=False,\n)\ndomain_disc = rul_adapt.model.FullyConnectedHead(\n    input_channels=8,\n    units=[8, 1],\n    act_func_on_last_layer=False,\n)\n\ndann = rul_adapt.approach.DannApproach(\n    dann_factor=1.0, lr=0.001, optim_type=\"adam\"\n)\ndann.set_model(feature_extractor, regressor, domain_disc)\n\ntrainer = pl.Trainer(max_epochs=1)\n\ntrainer.fit(dann, dm)\ntrainer.test(dann, dm)\n</pre> source = rul_datasets.CmapssReader(3) target = source.get_compatible(1, percent_broken=0.8) dm = rul_datasets.DomainAdaptionDataModule(     rul_datasets.RulDataModule(source, batch_size=32),     rul_datasets.RulDataModule(target, batch_size=32), )  feature_extractor = rul_adapt.model.LstmExtractor(     input_channels=14,     units=[16],     fc_units=8, ) regressor = rul_adapt.model.FullyConnectedHead(     input_channels=8,     units=[8, 1],     act_func_on_last_layer=False, ) domain_disc = rul_adapt.model.FullyConnectedHead(     input_channels=8,     units=[8, 1],     act_func_on_last_layer=False, )  dann = rul_adapt.approach.DannApproach(     dann_factor=1.0, lr=0.001, optim_type=\"adam\" ) dann.set_model(feature_extractor, regressor, domain_disc)  trainer = pl.Trainer(max_epochs=1)  trainer.fit(dann, dm) trainer.test(dann, dm) <pre>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name               | Type                  | Params\n-------------------------------------------------------------\n0 | train_source_loss  | MeanAbsoluteError     | 0     \n1 | evaluator          | AdaptionEvaluator     | 0     \n2 | _feature_extractor | LstmExtractor         | 2.2 K \n3 | _regressor         | FullyConnectedHead    | 81    \n4 | dann_loss          | DomainAdversarialLoss | 81    \n-------------------------------------------------------------\n2.3 K     Trainable params\n0         Non-trainable params\n2.3 K     Total params\n0.009     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse        20.788148880004883\n    test/source/score         3068.064453125\n    test/target/rmse                                  18.67778778076172\n    test/target/score                                1114.4984130859375\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[21]: <pre>[{'test/source/rmse/dataloader_idx_0': 20.788148880004883,\n  'test/source/score/dataloader_idx_0': 3068.064453125},\n {'test/target/rmse/dataloader_idx_1': 18.67778778076172,\n  'test/target/score/dataloader_idx_1': 1114.4984130859375}]</pre>"},{"location":"examples/cnn_dann/#cnn-dann","title":"CNN DANN\u00b6","text":""},{"location":"examples/cnn_dann/#reproduce-original-configurations","title":"Reproduce original configurations\u00b6","text":"<p>You can reproduce the original experiments of Krokotsch et al. by using the <code>get_cnn_dann</code> constructor function. Known differences to the original are:</p> <ul> <li>the model with the best validation RMSE is saved instead of the model with the best test RMSE.</li> </ul> <p>In this example, we re-create configuration for adaption CMAPSS FD003 to FD001. Additional <code>kwargs</code> for the trainer, e.g. <code>accelerator=\"gpu\"</code> for training on a GPU, can be passed to this function, too.</p>"},{"location":"examples/cnn_dann/#run-your-own-experiments","title":"Run your own experiments\u00b6","text":"<p>You can use the CNN DANN implementation to run your own experiments with different hyperparameters or on different datasets. Here we build a small LSTM DANN version for CMAPSS.</p>"},{"location":"examples/conditional/","title":"Conditional Adaption Approach","text":"In\u00a0[8]: Copied! <pre>import numpy as np\nimport pytorch_lightning as pl\nimport rul_datasets\n\nimport rul_adapt\n</pre> import numpy as np import pytorch_lightning as pl import rul_datasets  import rul_adapt <p>The conditional adaption approaches are variants of the MMD and DANN approaches. In addition to applying their adaption loss to the whole data, they also apply it separately to pre-defined subsets. Each subset represents a different condition and is defined by a fuzzy set with a rectangular membership function.</p> <p>In the original paper, three subsets are defined based on the RUL values of the source domain. The first fuzzy set contains healthy data which has a RUL value smaller than the median. The second fuzzy set contains degrading data with a RUL value between the 25th and 75th percentile. The last fuzzy set contains faulty data with a RUL value larger than the median. These sets are overlapping by design to take the uncertainty of sample membership into account.</p> <p>We will extract the fuzzy sets' boundaries from the training split of CMAPSS FD003.</p> In\u00a0[\u00a0]: Copied! <pre>fd3 = rul_datasets.CmapssReader(3)\n_, targets = fd3.load_split(\"dev\")\ntargets = np.concatenate(targets) # combine all runs\nmedian = np.median(targets)\nlower_quart = np.quantile(targets, 0.25)\nupper_quart = np.quantile(targets, 0.75)\n\nfuzzy_sets = [(median, fd3.max_rul), (lower_quart, upper_quart), (0.0, median)]\nprint(fuzzy_sets)\n</pre> fd3 = rul_datasets.CmapssReader(3) _, targets = fd3.load_split(\"dev\") targets = np.concatenate(targets) # combine all runs median = np.median(targets) lower_quart = np.quantile(targets, 0.25) upper_quart = np.quantile(targets, 0.75)  fuzzy_sets = [(median, fd3.max_rul), (lower_quart, upper_quart), (0.0, median)] print(fuzzy_sets) <pre>[(110.0, 125), (55.0, 125.0), (0.0, 110.0)]\n</pre> <p>Now, we can use these fuzzy sets to adapt FD003 to FD001 with conditional MMD. First, we define the adaption data module.</p> In\u00a0[\u00a0]: Copied! <pre>fd1 = rul_datasets.CmapssReader(1, percent_broken=0.8)\ndm = rul_datasets.DomainAdaptionDataModule(\n    rul_datasets.RulDataModule(fd3, batch_size=128),\n    rul_datasets.RulDataModule(fd1, batch_size=128)\n)\n</pre> fd1 = rul_datasets.CmapssReader(1, percent_broken=0.8) dm = rul_datasets.DomainAdaptionDataModule(     rul_datasets.RulDataModule(fd3, batch_size=128),     rul_datasets.RulDataModule(fd1, batch_size=128) ) <p>Next, we instantiate a conditional MMD approach with a simple CNN feature extractor.</p> In\u00a0[\u00a0]: Copied! <pre>feature_extractor = rul_adapt.model.CnnExtractor(14, [16, 32], 30, fc_units=64)\nregressor = rul_adapt.model.FullyConnectedHead(\n    64, [1], act_func_on_last_layer=False\n)\n\napproach = rul_adapt.approach.ConditionalMmdApproach(\n    lr=0.001,\n    mmd_factor=1.0,\n    num_mmd_kernels=5,\n    dynamic_adaptive_factor=0.5,\n    fuzzy_sets=fuzzy_sets\n)\napproach.set_model(feature_extractor, regressor)\n</pre> feature_extractor = rul_adapt.model.CnnExtractor(14, [16, 32], 30, fc_units=64) regressor = rul_adapt.model.FullyConnectedHead(     64, [1], act_func_on_last_layer=False )  approach = rul_adapt.approach.ConditionalMmdApproach(     lr=0.001,     mmd_factor=1.0,     num_mmd_kernels=5,     dynamic_adaptive_factor=0.5,     fuzzy_sets=fuzzy_sets ) approach.set_model(feature_extractor, regressor) <p>We train the approach for 10 epochs and evaluate it on the test split of FD001.</p> In\u00a0[\u00a0]: Copied! <pre>trainer = pl.Trainer(max_epochs=10)\ntrainer.fit(approach, dm)\ntrainer.test(approach, dm)\n</pre> trainer = pl.Trainer(max_epochs=10) trainer.fit(approach, dm) trainer.test(approach, dm) <p>We can also use the conditional DANN approach. This will use the same feature extractor and regressor as before and an additional domain discriminator. The discriminator network will be copied for each fuzzy set.</p> In\u00a0[11]: Copied! <pre>feature_extractor = rul_adapt.model.CnnExtractor(14, [16, 32], 30, fc_units=64)\nregressor = rul_adapt.model.FullyConnectedHead(\n    64, [1], act_func_on_last_layer=False\n)\ndomain_disc = rul_adapt.model.FullyConnectedHead(\n    64, [1], act_func_on_last_layer=False\n)\n\napproach = rul_adapt.approach.ConditionalDannApproach(\n    lr=0.001,\n    dann_factor=1.0,\n    dynamic_adaptive_factor=0.5,\n    fuzzy_sets=fuzzy_sets\n)\napproach.set_model(feature_extractor, regressor, domain_disc)\n</pre> feature_extractor = rul_adapt.model.CnnExtractor(14, [16, 32], 30, fc_units=64) regressor = rul_adapt.model.FullyConnectedHead(     64, [1], act_func_on_last_layer=False ) domain_disc = rul_adapt.model.FullyConnectedHead(     64, [1], act_func_on_last_layer=False )  approach = rul_adapt.approach.ConditionalDannApproach(     lr=0.001,     dann_factor=1.0,     dynamic_adaptive_factor=0.5,     fuzzy_sets=fuzzy_sets ) approach.set_model(feature_extractor, regressor, domain_disc) <p>Again, we will train for 10 epochs and evaluate on the test split of FD001.</p> In\u00a0[13]: Copied! <pre>trainer = pl.Trainer(max_epochs=10)\ntrainer.fit(approach, dm)\ntrainer.test(approach, dm)\n</pre> trainer = pl.Trainer(max_epochs=10) trainer.fit(approach, dm) trainer.test(approach, dm) <pre>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name                 | Type                       | Params\n--------------------------------------------------------------------\n0 | train_source_loss    | MeanAbsoluteError          | 0     \n1 | mmd_loss             | MaximumMeanDiscrepancyLoss | 0     \n2 | conditional_mmd_loss | ConditionalAdaptionLoss    | 0     \n3 | evaluator            | AdaptionEvaluator          | 0     \n4 | _feature_extractor   | CnnExtractor               | 55.6 K\n5 | _regressor           | FullyConnectedHead         | 65    \n--------------------------------------------------------------------\n55.6 K    Trainable params\n0         Non-trainable params\n55.6 K    Total params\n0.223     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=10` reached.\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse         15.53573226928711\n    test/source/score        615.2120361328125\n    test/target/rmse                                 23.868637084960938\n    test/target/score                                 2933.725830078125\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[13]: <pre>[{'test/source/rmse/dataloader_idx_0': 15.53573226928711,\n  'test/source/score/dataloader_idx_0': 615.2120361328125},\n {'test/target/rmse/dataloader_idx_1': 23.868637084960938,\n  'test/target/score/dataloader_idx_1': 2933.725830078125}]</pre>"},{"location":"examples/conditional/#conditional-adaption-approach","title":"Conditional Adaption Approach\u00b6","text":""},{"location":"examples/consistency_dann/","title":"Consistency DANN","text":"In\u00a0[1]: Copied! <pre>import rul_datasets\nimport rul_adapt\nimport pytorch_lightning as pl\nimport omegaconf\n</pre> import rul_datasets import rul_adapt import pytorch_lightning as pl import omegaconf In\u00a0[2]: Copied! <pre>pl.seed_everything(42, workers=True)  # makes it reproducible\npre_training, main_training = rul_adapt.construct.get_consistency_dann(\n    \"cmapss\", 3, 1, {\"max_epochs\": 1}, {\"max_epochs\": 1}\n)\n</pre> pl.seed_everything(42, workers=True)  # makes it reproducible pre_training, main_training = rul_adapt.construct.get_consistency_dann(     \"cmapss\", 3, 1, {\"max_epochs\": 1}, {\"max_epochs\": 1} ) <pre>Global seed set to 42\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</pre> <p>The function returns two tuples. The first contains everything needed for pre-training, the second everything needed for the main training.</p> In\u00a0[3]: Copied! <pre>pre_dm, pre_approach, pre_trainer = pre_training\npre_trainer.fit(pre_approach, pre_dm)\n</pre> pre_dm, pre_approach, pre_trainer = pre_training pre_trainer.fit(pre_approach, pre_dm) <pre>\n  | Name               | Type               | Params\n----------------------------------------------------------\n0 | train_loss         | MeanSquaredError   | 0     \n1 | val_loss           | MeanSquaredError   | 0     \n2 | test_loss          | MeanSquaredError   | 0     \n3 | evaluator          | AdaptionEvaluator  | 0     \n4 | _feature_extractor | CnnExtractor       | 3.3 K \n5 | _regressor         | FullyConnectedHead | 221   \n----------------------------------------------------------\n3.5 K     Trainable params\n0         Non-trainable params\n3.5 K     Total params\n0.014     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <p>After pre-training, we can use the pre-trained networks to initialize the main training. The networks of the pre-training approach, i.e. <code>feature_extractor</code> and <code>regressor</code>, can be accessed as properties.</p> In\u00a0[4]: Copied! <pre>dm, approach, domain_disc, trainer = main_training\napproach.set_model(pre_approach.feature_extractor, pre_approach.regressor, domain_disc)\ntrainer.fit(approach, dm)\ntrainer.test(approach, dm)\n</pre> dm, approach, domain_disc, trainer = main_training approach.set_model(pre_approach.feature_extractor, pre_approach.regressor, domain_disc) trainer.fit(approach, dm) trainer.test(approach, dm) <pre>\n  | Name                     | Type                  | Params\n-------------------------------------------------------------------\n0 | train_source_loss        | MeanSquaredError      | 0     \n1 | consistency_loss         | ConsistencyLoss       | 0     \n2 | evaluator                | AdaptionEvaluator     | 0     \n3 | _feature_extractor       | CnnExtractor          | 3.3 K \n4 | _regressor               | FullyConnectedHead    | 221   \n5 | dann_loss                | DomainAdversarialLoss | 21    \n6 | frozen_feature_extractor | CnnExtractor          | 3.3 K \n-------------------------------------------------------------------\n3.5 K     Trainable params\n3.3 K     Non-trainable params\n6.8 K     Total params\n0.027     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse         83.57210540771484\n    test/source/score          371325.28125\n    test/target/rmse                                  84.60678100585938\n    test/target/score                                   342092.78125\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[4]: <pre>[{'test/source/rmse/dataloader_idx_0': 83.57210540771484,\n  'test/source/score/dataloader_idx_0': 371325.28125},\n {'test/target/rmse/dataloader_idx_1': 84.60678100585938,\n  'test/target/score/dataloader_idx_1': 342092.78125}]</pre> <p>If you only want to see the hyperparameters, you can use the <code>get_consistency_dann_config</code> function. This returns an <code>omegaconf.DictConfig</code> which you can modify. Afterwards, you can pass the config to <code>consistency_dann_from_config</code> to receive the training-ready approach.</p> In\u00a0[5]: Copied! <pre>cmapss_three2one_config = rul_adapt.construct.get_consistency_dann_config(\"cmapss\", 3, 1)\nprint(omegaconf.OmegaConf.to_yaml(cmapss_three2one_config, resolve=True))\n</pre> cmapss_three2one_config = rul_adapt.construct.get_consistency_dann_config(\"cmapss\", 3, 1) print(omegaconf.OmegaConf.to_yaml(cmapss_three2one_config, resolve=True)) <pre>dm:\n  source:\n    _target_: rul_datasets.CmapssReader\n    fd: 3\n    window_size: 20\n  target:\n    fd: 1\n    percent_broken: 1.0\n  kwargs:\n    batch_size: 128\nfeature_extractor:\n  _convert_: all\n  _target_: rul_adapt.model.CnnExtractor\n  input_channels: 14\n  units:\n  - 32\n  - 16\n  - 1\n  seq_len: 20\n  fc_units: 20\n  dropout: 0.5\n  fc_dropout: 0.5\nregressor:\n  _convert_: all\n  _target_: rul_adapt.model.FullyConnectedHead\n  input_channels: 20\n  act_func_on_last_layer: false\n  units:\n  - 10\n  - 1\ndomain_disc:\n  _convert_: all\n  _target_: rul_adapt.model.FullyConnectedHead\n  input_channels: 20\n  act_func_on_last_layer: false\n  units:\n  - 1\nconsistency_pre:\n  _target_: rul_adapt.approach.SupervisedApproach\n  lr: 0.0001\n  loss_type: rmse\n  optim_type: sgd\nconsistency:\n  _target_: rul_adapt.approach.ConsistencyApproach\n  consistency_factor: 1.0\n  max_epochs: 3000\n  lr: 1.0e-05\n  optim_type: sgd\ntrainer_pre:\n  _target_: pytorch_lightning.Trainer\n  max_epochs: 1000\ntrainer:\n  _target_: pytorch_lightning.Trainer\n  max_epochs: 3000\n</pre> In\u00a0[7]: Copied! <pre>source = rul_datasets.CmapssReader(3)\ntarget = source.get_compatible(1, percent_broken=0.8)\n\npre_dm = rul_datasets.RulDataModule(source, batch_size=32)\ndm = rul_datasets.DomainAdaptionDataModule(\n    pre_dm, rul_datasets.RulDataModule(target, batch_size=32),\n)\n\nfeature_extractor = rul_adapt.model.LstmExtractor(\n    input_channels=14,\n    units=[16],\n    fc_units=8,\n)\nregressor = rul_adapt.model.FullyConnectedHead(\n    input_channels=8,\n    units=[8, 1],\n    act_func_on_last_layer=False,\n)\ndomain_disc = rul_adapt.model.FullyConnectedHead(\n    input_channels=8,\n    units=[8, 1],\n    act_func_on_last_layer=False,\n)\n\npre_approach = rul_adapt.approach.SupervisedApproach(\n    lr=0.001, loss_type=\"rmse\", optim_type=\"sgd\"\n)\npre_approach.set_model(feature_extractor, regressor)\npre_trainer = pl.Trainer(max_epochs=1)\ntrainer.fit(pre_approach, pre_dm)\n\napproach = rul_adapt.approach.ConsistencyApproach(\n    consistency_factor=1.0, lr=0.001, max_epochs=1\n)\napproach.set_model(\n    pre_approach.feature_extractor, pre_approach.regressor, domain_disc\n)\ntrainer = pl.Trainer(max_epochs=1)\ntrainer.fit(approach, dm)\ntrainer.test(approach, dm)\n</pre> source = rul_datasets.CmapssReader(3) target = source.get_compatible(1, percent_broken=0.8)  pre_dm = rul_datasets.RulDataModule(source, batch_size=32) dm = rul_datasets.DomainAdaptionDataModule(     pre_dm, rul_datasets.RulDataModule(target, batch_size=32), )  feature_extractor = rul_adapt.model.LstmExtractor(     input_channels=14,     units=[16],     fc_units=8, ) regressor = rul_adapt.model.FullyConnectedHead(     input_channels=8,     units=[8, 1],     act_func_on_last_layer=False, ) domain_disc = rul_adapt.model.FullyConnectedHead(     input_channels=8,     units=[8, 1],     act_func_on_last_layer=False, )  pre_approach = rul_adapt.approach.SupervisedApproach(     lr=0.001, loss_type=\"rmse\", optim_type=\"sgd\" ) pre_approach.set_model(feature_extractor, regressor) pre_trainer = pl.Trainer(max_epochs=1) trainer.fit(pre_approach, pre_dm)  approach = rul_adapt.approach.ConsistencyApproach(     consistency_factor=1.0, lr=0.001, max_epochs=1 ) approach.set_model(     pre_approach.feature_extractor, pre_approach.regressor, domain_disc ) trainer = pl.Trainer(max_epochs=1) trainer.fit(approach, dm) trainer.test(approach, dm) <pre>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/tilman/Programming/rul-adapt/docs/examples/lightning_logs/version_27/checkpoints exists and is not empty.\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n\n  | Name               | Type               | Params\n----------------------------------------------------------\n0 | train_loss         | MeanSquaredError   | 0     \n1 | val_loss           | MeanSquaredError   | 0     \n2 | test_loss          | MeanSquaredError   | 0     \n3 | evaluator          | AdaptionEvaluator  | 0     \n4 | _feature_extractor | LstmExtractor      | 2.2 K \n5 | _regressor         | FullyConnectedHead | 81    \n----------------------------------------------------------\n2.3 K     Trainable params\n0         Non-trainable params\n2.3 K     Total params\n0.009     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name                     | Type                  | Params\n-------------------------------------------------------------------\n0 | train_source_loss        | MeanSquaredError      | 0     \n1 | consistency_loss         | ConsistencyLoss       | 0     \n2 | evaluator                | AdaptionEvaluator     | 0     \n3 | _feature_extractor       | LstmExtractor         | 2.2 K \n4 | _regressor               | FullyConnectedHead    | 81    \n5 | dann_loss                | DomainAdversarialLoss | 81    \n6 | frozen_feature_extractor | LstmExtractor         | 2.2 K \n-------------------------------------------------------------------\n2.3 K     Trainable params\n2.2 K     Non-trainable params\n4.5 K     Total params\n0.018     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse         18.09880828857422\n    test/source/score       1549.2022705078125\n    test/target/rmse                                 22.494943618774414\n    test/target/score                                 814.8432006835938\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[7]: <pre>[{'test/source/rmse/dataloader_idx_0': 18.09880828857422,\n  'test/source/score/dataloader_idx_0': 1549.2022705078125},\n {'test/target/rmse/dataloader_idx_1': 22.494943618774414,\n  'test/target/score/dataloader_idx_1': 814.8432006835938}]</pre>"},{"location":"examples/consistency_dann/#consistency-dann","title":"Consistency DANN\u00b6","text":""},{"location":"examples/consistency_dann/#reproduce-original-configurations","title":"Reproduce original configurations\u00b6","text":"<p>You can reproduce the original experiments by Siahpour et al. by using the <code>get_consistency_dann</code> constructor function. Known differences to the original paper are:</p> <ul> <li>the <code>consistency_factor</code> is set to 1.0 because the real value is not mentioned in the paper</li> <li>the raw vibration data of XJTU-SY is preprocessed by extracting the standard deviation from each window because the given architecture could not handle the raw data</li> </ul> <p>Additional <code>kwargs</code> for the trainer, e.g. <code>accelerator=\"gpu\"</code> for training on a GPU, can be passed to the function as a dictionary. The first dictionary is used for the pre-training trainer and the second one for the main trainer.</p>"},{"location":"examples/consistency_dann/#run-your-own-experiments","title":"Run your own experiments\u00b6","text":"<p>You can use the Consistency DANN implementation to run your own experiments with different hyperparameters or on different datasets. Here we build an approach with an LSTM feature extractor.</p>"},{"location":"examples/latent_align/","title":"Latent Alignment Approach","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport omegaconf\nimport pytorch_lightning as pl\nimport rul_datasets\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport rul_adapt\n</pre> import matplotlib.pyplot as plt import omegaconf import pytorch_lightning as pl import rul_datasets import torch from torch.utils.data import DataLoader  import rul_adapt In\u00a0[4]: Copied! <pre>reader = rul_datasets.XjtuSyReader(\n    1, run_split_dist={\"dev\": [1, 2, 3, 4, 5], \"val\": [], \"test\": []}\n)\nreader.prepare_data()\nchunk_extractor = rul_adapt.approach.latent_align.ChunkWindowExtractor(5, 256)\nfeatures, targets = reader.load_split(\"dev\")\nfeatures, targets = zip(\n    *(\n        chunk_extractor(feature, target)\n        for feature, target in zip(features, targets)\n    )\n)\nhealthy, _ = rul_datasets.adaption.split_healthy(\n    features, targets, by_steps=15 * 128\n)\nhealthy_dl = DataLoader(healthy, batch_size=128, shuffle=True)\n\nfeature_extractor = rul_adapt.model.CnnExtractor(\n    2,\n    [64, 32, 16],\n    features[0].shape[1],\n    fc_units=512,\n    act_func=torch.nn.LeakyReLU,\n    fc_act_func=torch.nn.LeakyReLU,\n)\nregressor = rul_adapt.model.FullyConnectedHead(\n    512, [1], act_func_on_last_layer=False\n)\ngenerator = rul_adapt.model.CnnExtractor(\n    1,\n    [64, 32, 16, 512, 2],\n    features[0].shape[1],\n    [10, 10, 10, 1, 1],\n    padding=True,\n    act_func=torch.nn.LeakyReLU,\n)\n\napproach = rul_adapt.approach.LatentAlignFttpApproach(\n    features[0].shape[1], lr=1e-5\n)\napproach.set_model(feature_extractor, regressor, generator)\n\ntrainer = pl.Trainer(max_epochs=1)\n\ntrainer.fit(approach, healthy_dl)\n</pre> reader = rul_datasets.XjtuSyReader(     1, run_split_dist={\"dev\": [1, 2, 3, 4, 5], \"val\": [], \"test\": []} ) reader.prepare_data() chunk_extractor = rul_adapt.approach.latent_align.ChunkWindowExtractor(5, 256) features, targets = reader.load_split(\"dev\") features, targets = zip(     *(         chunk_extractor(feature, target)         for feature, target in zip(features, targets)     ) ) healthy, _ = rul_datasets.adaption.split_healthy(     features, targets, by_steps=15 * 128 ) healthy_dl = DataLoader(healthy, batch_size=128, shuffle=True)  feature_extractor = rul_adapt.model.CnnExtractor(     2,     [64, 32, 16],     features[0].shape[1],     fc_units=512,     act_func=torch.nn.LeakyReLU,     fc_act_func=torch.nn.LeakyReLU, ) regressor = rul_adapt.model.FullyConnectedHead(     512, [1], act_func_on_last_layer=False ) generator = rul_adapt.model.CnnExtractor(     1,     [64, 32, 16, 512, 2],     features[0].shape[1],     [10, 10, 10, 1, 1],     padding=True,     act_func=torch.nn.LeakyReLU, )  approach = rul_adapt.approach.LatentAlignFttpApproach(     features[0].shape[1], lr=1e-5 ) approach.set_model(feature_extractor, regressor, generator)  trainer = pl.Trainer(max_epochs=1)  trainer.fit(approach, healthy_dl) <pre>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name               | Type                  | Params\n-------------------------------------------------------------\n0 | gan_loss           | BCEWithLogitsLoss     | 0     \n1 | grl                | GradientReversalLayer | 0     \n2 | _feature_extractor | CnnExtractor          | 10.4 M\n3 | _regressor         | FullyConnectedHead    | 513   \n4 | _generator         | CnnExtractor          | 36.1 K\n-------------------------------------------------------------\n10.5 M    Trainable params\n0         Non-trainable params\n10.5 M    Total params\n41.928    Total estimated model params size (MB)\n</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:309: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n  return F.conv1d(input, weight, bias, self.stride,\n`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <p>Afterward, the GAN discriminator is used to calculate the health index. We chose 2.5 times the average health index of the training data as the threshold because none was given in the paper.</p> In\u00a0[5]: Copied! <pre>features, _ = reader.load_split(\"dev\")\nhealth_indicator = rul_adapt.approach.latent_align.get_health_indicator(\n    approach, features[0], 5, 256\n)\nthreshold = 2.5 * health_indicator[:10].mean()\nplt.plot(health_indicator)\nplt.axhline(threshold, color=\"red\", linestyle=\"--\")\nplt.show()\n</pre> features, _ = reader.load_split(\"dev\") health_indicator = rul_adapt.approach.latent_align.get_health_indicator(     approach, features[0], 5, 256 ) threshold = 2.5 * health_indicator[:10].mean() plt.plot(health_indicator) plt.axhline(threshold, color=\"red\", linestyle=\"--\") plt.show() <p>The following function can be used for conveniently calculating the FTTP for a given bearing.</p> In\u00a0[6]: Copied! <pre>fttp = rul_adapt.approach.latent_align.get_first_time_to_predict(\n    approach,\n    features[0],\n    window_size=5,\n    chunk_size=256,\n    healthy_index=15,\n    threshold_coefficient=2.5\n)\nprint(f\"FTTP of Bearing 1-1: {fttp}\")\n</pre> fttp = rul_adapt.approach.latent_align.get_first_time_to_predict(     approach,     features[0],     window_size=5,     chunk_size=256,     healthy_index=15,     threshold_coefficient=2.5 ) print(f\"FTTP of Bearing 1-1: {fttp}\") <pre>FTTP of Bearing 1-1: 83\n</pre> In\u00a0[7]: Copied! <pre>pl.seed_everything(42)  # make reproducible\ndm, latent, trainer = rul_adapt.construct.get_latent_align(\"cmapss\", 3, 1, max_epochs=1)\n</pre> pl.seed_everything(42)  # make reproducible dm, latent, trainer = rul_adapt.construct.get_latent_align(\"cmapss\", 3, 1, max_epochs=1) <pre>Global seed set to 42\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</pre> <p>The networks, <code>feature_extractor</code> and <code>regressor</code>, can be accessed as properties of the <code>latent</code> object.</p> In\u00a0[8]: Copied! <pre>latent.feature_extractor\n</pre> latent.feature_extractor Out[8]: <pre>CnnExtractor(\n  (_layers): Sequential(\n    (conv_0): Sequential(\n      (0): Conv1d(14, 32, kernel_size=(3,), stride=(1,), padding=valid)\n      (1): LeakyReLU(negative_slope=0.01)\n    )\n    (conv_1): Sequential(\n      (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=valid)\n      (1): LeakyReLU(negative_slope=0.01)\n    )\n    (conv_2): Sequential(\n      (0): Conv1d(16, 1, kernel_size=(3,), stride=(1,), padding=valid)\n      (1): LeakyReLU(negative_slope=0.01)\n    )\n    (3): Flatten(start_dim=1, end_dim=-1)\n    (fc): Sequential(\n      (0): Dropout(p=0.5, inplace=False)\n      (1): Linear(in_features=24, out_features=256, bias=True)\n      (2): LeakyReLU(negative_slope=0.01)\n    )\n  )\n)</pre> <p>Training is done in the PyTorch Lightning fashion. We used the <code>trainer_kwargs</code> to train only one epoch for demonstration purposes.</p> In\u00a0[9]: Copied! <pre>trainer.fit(latent, dm)\ntrainer.test(latent, dm)\n</pre> trainer.fit(latent, dm) trainer.test(latent, dm) <pre>\n  | Name               | Type                               | Params\n--------------------------------------------------------------------------\n0 | train_mse          | MeanSquaredError                   | 0     \n1 | healthy_align      | HealthyStateAlignmentLoss          | 0     \n2 | direction_align    | DegradationDirectionAlignmentLoss  | 0     \n3 | level_align        | DegradationLevelRegularizationLoss | 0     \n4 | fusion_align       | MaximumMeanDiscrepancyLoss         | 0     \n5 | evaluator          | AdaptionEvaluator                  | 0     \n6 | _feature_extractor | CnnExtractor                       | 9.4 K \n7 | _regressor         | FullyConnectedHead                 | 257   \n--------------------------------------------------------------------------\n9.6 K     Trainable params\n0         Non-trainable params\n9.6 K     Total params\n0.039     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse         83.4994888305664\n    test/source/score           368967.1875\n    test/target/rmse                                  84.54061889648438\n    test/target/score                                     340130.5\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[9]: <pre>[{'test/source/rmse/dataloader_idx_0': 83.4994888305664,\n  'test/source/score/dataloader_idx_0': 368967.1875},\n {'test/target/rmse/dataloader_idx_1': 84.54061889648438,\n  'test/target/score/dataloader_idx_1': 340130.5}]</pre> <p>If you only want to see the hyperparameters, you can use the <code>get_latent_align_config</code> function. This returns an <code>omegaconf.DictConfig</code> which you can modify.</p> In\u00a0[10]: Copied! <pre>three2one_config = rul_adapt.construct.get_latent_align_config(\"cmapss\", 3, 1)\nprint(omegaconf.OmegaConf.to_yaml(three2one_config, resolve=True))\n</pre> three2one_config = rul_adapt.construct.get_latent_align_config(\"cmapss\", 3, 1) print(omegaconf.OmegaConf.to_yaml(three2one_config, resolve=True)) <pre>dm:\n  source:\n    _target_: rul_datasets.CmapssReader\n    fd: 3\n    window_size: 30\n  target:\n    _target_: rul_datasets.CmapssReader\n    fd: 1\n    window_size: 30\n    percent_broken: 1.0\n  kwargs:\n    batch_size: 128\n  adaption_kwargs:\n    inductive: true\n    split_by_steps: 80\nfeature_extractor:\n  _convert_: all\n  _target_: rul_adapt.model.CnnExtractor\n  input_channels: 14\n  units:\n  - 32\n  - 16\n  - 1\n  seq_len: 30\n  fc_units: 256\n  fc_dropout: 0.5\n  act_func: torch.nn.LeakyReLU\n  fc_act_func: torch.nn.LeakyReLU\nregressor:\n  _convert_: all\n  _target_: rul_adapt.model.FullyConnectedHead\n  input_channels: 256\n  act_func_on_last_layer: false\n  units:\n  - 1\nlatent_align:\n  _target_: rul_adapt.approach.LatentAlignApproach\n  alpha_healthy: 1.0\n  alpha_direction: 1.0\n  alpha_level: 1.0\n  alpha_fusion: 1.0\n  labels_as_percentage: true\n  lr: 0.0005\ntrainer:\n  _target_: pytorch_lightning.Trainer\n  max_epochs: 2000\n</pre> In\u00a0[2]: Copied! <pre>source = rul_datasets.XjtuSyReader(\n    1,\n    first_time_to_predict=[100] * 5,  # dummy values\n    norm_rul=True,\n    run_split_dist={\"dev\": [1, 2, 3, 4, 5], \"val\": [], \"test\": []}\n)\ntarget = rul_datasets.XjtuSyReader(\n    3,\n    percent_broken=0.5,\n    truncate_degraded_only=True,\n    first_time_to_predict=[100] * 5,  # dummy values\n    norm_rul=True,\n    run_split_dist={\"dev\": [3], \"val\": [], \"test\": [3]},\n)\nextractor = rul_adapt.approach.latent_align.ChunkWindowExtractor(5, 256)\ndm = rul_datasets.LatentAlignDataModule(\n    rul_datasets.RulDataModule(source, 128, extractor),\n    rul_datasets.RulDataModule(target, 128, extractor),\n    split_by_max_rul=True,\n    inductive=True,\n)\n\nfeature_extractor = rul_adapt.model.CnnExtractor(\n    2,\n    [32, 16, 1],\n    1280,\n    fc_units=256,\n    fc_dropout=0.5,\n    act_func=torch.nn.LeakyReLU,\n    fc_act_func=torch.nn.LeakyReLU,\n)\n\nregressor = rul_adapt.model.FullyConnectedHead(256, [1], act_func_on_last_layer=False)\n\nlatent = rul_adapt.approach.LatentAlignApproach(1.0, 1.0, 1.0, 1.0, lr=5e-4)\nlatent.set_model(feature_extractor, regressor)\n\ntrainer = pl.Trainer(max_epochs=1)\n\ntrainer.fit(latent, dm)\ntrainer.test(latent, dm)\n</pre> source = rul_datasets.XjtuSyReader(     1,     first_time_to_predict=[100] * 5,  # dummy values     norm_rul=True,     run_split_dist={\"dev\": [1, 2, 3, 4, 5], \"val\": [], \"test\": []} ) target = rul_datasets.XjtuSyReader(     3,     percent_broken=0.5,     truncate_degraded_only=True,     first_time_to_predict=[100] * 5,  # dummy values     norm_rul=True,     run_split_dist={\"dev\": [3], \"val\": [], \"test\": [3]}, ) extractor = rul_adapt.approach.latent_align.ChunkWindowExtractor(5, 256) dm = rul_datasets.LatentAlignDataModule(     rul_datasets.RulDataModule(source, 128, extractor),     rul_datasets.RulDataModule(target, 128, extractor),     split_by_max_rul=True,     inductive=True, )  feature_extractor = rul_adapt.model.CnnExtractor(     2,     [32, 16, 1],     1280,     fc_units=256,     fc_dropout=0.5,     act_func=torch.nn.LeakyReLU,     fc_act_func=torch.nn.LeakyReLU, )  regressor = rul_adapt.model.FullyConnectedHead(256, [1], act_func_on_last_layer=False)  latent = rul_adapt.approach.LatentAlignApproach(1.0, 1.0, 1.0, 1.0, lr=5e-4) latent.set_model(feature_extractor, regressor)  trainer = pl.Trainer(max_epochs=1)  trainer.fit(latent, dm) trainer.test(latent, dm) <pre>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name               | Type                               | Params\n--------------------------------------------------------------------------\n0 | train_mse          | MeanSquaredError                   | 0     \n1 | healthy_align      | HealthyStateAlignmentLoss          | 0     \n2 | direction_align    | DegradationDirectionAlignmentLoss  | 0     \n3 | level_align        | DegradationLevelRegularizationLoss | 0     \n4 | fusion_align       | MaximumMeanDiscrepancyLoss         | 0     \n5 | evaluator          | AdaptionEvaluator                  | 0     \n6 | _feature_extractor | CnnExtractor                       | 328 K \n7 | _regressor         | FullyConnectedHead                 | 257   \n--------------------------------------------------------------------------\n328 K     Trainable params\n0         Non-trainable params\n328 K     Total params\n1.314     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:109: UserWarning: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n  rank_zero_warn(\n</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:109: UserWarning: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n  rank_zero_warn(\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/lightning_fabric/utilities/data.py:55: UserWarning: `DataLoader` returned 0 length. Please make sure this was your intention.\n  rank_zero_warn(\n</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/target/rmse                                 0.3762059807777405\n    test/target/score                                 1309.618896484375\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[2]: <pre>[{},\n {'test/target/rmse/dataloader_idx_1': 0.3762059807777405,\n  'test/target/score/dataloader_idx_1': 1309.618896484375}]</pre>"},{"location":"examples/latent_align/#latent-alignment-approach","title":"Latent Alignment Approach\u00b6","text":""},{"location":"examples/latent_align/#training-preparations","title":"Training Preparations\u00b6","text":"<p>The Latent Alignment uses a first-time-to-predict (FTTP) estimation approach to appropriately label the XJTU-SY dataset. The FTTP approach uses a GAN trained only on the first few windows of each bearing which are considered healthy. The discriminator of the GAN is used to calculate a healthy index for each window. The FTTP is the index of the first window that exceeds a threshold defined as a multiple of the average health index of the training data.</p> <p>The RUL Adapt library implements the FTTP approach but includes the results in the configuration for an easier replication process. The GAN was trained as follows:</p>"},{"location":"examples/latent_align/#reproduce-original-configurations","title":"Reproduce original configurations\u00b6","text":"<p>You can reproduce the original experiments of Zhang et al. by using the <code>get_latent_align</code> constructor function. Known differences to the original paper:</p> <ul> <li>The healthy state cutoff for FTTP estimation was set to 15 because none was given in the paper.</li> <li>The threshold coefficient for FTTP estimation was set to 2.5 because none was given in the paper.</li> <li>The generator architecture for FTTP estimation was altered as we found no way to make the one from the paper work.</li> </ul> <p>In this example we re-create the configuration for adapting CMAPSS FD003 to FD001. Additional kwargs for the trainer, e.g. <code>accelerator=\"gpu\"</code> for training on a GPU, can be passed to the function, too.</p>"},{"location":"examples/latent_align/#run-your-own-experiments","title":"Run your own experiments\u00b6","text":"<p>You can use the Latent Alignment implementation to run your own experiments with different hyperparameters or on different datasets. Here we adapt from FD001 to FD003 of XJTU-SY which is missing from the original paper.</p>"},{"location":"examples/lstm_dann/","title":"LSTM DANN","text":"In\u00a0[1]: Copied! <pre>import rul_adapt\nimport rul_datasets\nimport pytorch_lightning as pl\nimport omegaconf\n</pre> import rul_adapt import rul_datasets import pytorch_lightning as pl import omegaconf In\u00a0[2]: Copied! <pre>pl.seed_everything(42, workers=True)  # make reproducible\ndm, dann, trainer = rul_adapt.construct.get_lstm_dann(3, 1, max_epochs=1)\n</pre> pl.seed_everything(42, workers=True)  # make reproducible dm, dann, trainer = rul_adapt.construct.get_lstm_dann(3, 1, max_epochs=1) <pre>Global seed set to 42\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</pre> <p>The networks, <code>feature_extractor</code>, <code>regressor</code>, <code>domain_disc</code>, can be accessed as properties of the <code>dann</code> object.</p> In\u00a0[3]: Copied! <pre>dann.feature_extractor\n</pre> dann.feature_extractor Out[3]: <pre>LstmExtractor(\n  (_lstm_layers): _Rnn(\n    (_layers): ModuleList(\n      (0): LSTM(24, 64)\n      (1): LSTM(64, 32)\n    )\n  )\n  (_fc_layer): Sequential(\n    (0): Dropout(p=0.3, inplace=False)\n    (1): Linear(in_features=32, out_features=128, bias=True)\n    (2): ReLU()\n  )\n)</pre> <p>Training is done in the PyTorch Lightning fashion. We used the <code>trainer_kwargs</code> to train only one epoch for demonstration purposes.</p> In\u00a0[4]: Copied! <pre>trainer.fit(dann, dm)\ntrainer.test(ckpt_path=\"best\", datamodule=dm) # loads the best model checkpoint\n</pre> trainer.fit(dann, dm) trainer.test(ckpt_path=\"best\", datamodule=dm) # loads the best model checkpoint <pre>\n  | Name               | Type                  | Params\n-------------------------------------------------------------\n0 | train_source_loss  | MeanAbsoluteError     | 0     \n1 | evaluator          | AdaptionEvaluator     | 0     \n2 | _feature_extractor | LstmExtractor         | 39.8 K\n3 | _regressor         | FullyConnectedHead    | 5.2 K \n4 | dann_loss          | DomainAdversarialLoss | 5.2 K \n-------------------------------------------------------------\n50.2 K    Trainable params\n0         Non-trainable params\n50.2 K    Total params\n0.201     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\nRestoring states from the checkpoint path at /home/tilman/Programming/rul-adapt/docs/examples/lightning_logs/version_32/checkpoints/epoch=0-step=69.ckpt\nLoaded model weights from checkpoint at /home/tilman/Programming/rul-adapt/docs/examples/lightning_logs/version_32/checkpoints/epoch=0-step=69.ckpt\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse        20.155813217163086\n    test/source/score        1689.973876953125\n    test/target/rmse                                  32.33406448364258\n    test/target/score                                 12900.6259765625\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[4]: <pre>[{'test/source/rmse/dataloader_idx_0': 20.155813217163086,\n  'test/source/score/dataloader_idx_0': 1689.973876953125},\n {'test/target/rmse/dataloader_idx_1': 32.33406448364258,\n  'test/target/score/dataloader_idx_1': 12900.6259765625}]</pre> <p>If you only want to see the hyperparameters, you can use the <code>get_lstm_dann_config</code> function. This returns an <code>omegeconf.DictConfig</code> which you can modify.</p> In\u00a0[5]: Copied! <pre>three2one_config = rul_adapt.construct.get_lstm_dann_config(3, 1)\nprint(omegaconf.OmegaConf.to_yaml(three2one_config, resolve=True))\n</pre> three2one_config = rul_adapt.construct.get_lstm_dann_config(3, 1) print(omegaconf.OmegaConf.to_yaml(three2one_config, resolve=True)) <pre>dm:\n  source:\n    _target_: rul_datasets.CmapssReader\n    fd: 3\n    feature_select:\n    - 0\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    - 9\n    - 10\n    - 11\n    - 12\n    - 13\n    - 14\n    - 15\n    - 16\n    - 17\n    - 18\n    - 19\n    - 20\n    - 21\n    - 22\n    - 23\n  target:\n    fd: 1\n    percent_broken: 1.0\n  batch_size: 256\nfeature_extractor:\n  _convert_: all\n  _target_: rul_adapt.model.LstmExtractor\n  input_channels: 24\n  units:\n  - 64\n  - 32\n  fc_units: 128\n  dropout: 0.3\n  fc_dropout: 0.3\nregressor:\n  _convert_: all\n  _target_: rul_adapt.model.FullyConnectedHead\n  input_channels: 128\n  act_func_on_last_layer: false\n  units:\n  - 32\n  - 32\n  - 1\n  dropout: 0.1\ndomain_disc:\n  _convert_: all\n  _target_: rul_adapt.model.FullyConnectedHead\n  input_channels: 128\n  act_func_on_last_layer: false\n  units:\n  - 32\n  - 32\n  - 1\n  dropout: 0.1\ndann:\n  _target_: rul_adapt.approach.DannApproach\n  scheduler_type: step\n  scheduler_gamma: 0.1\n  scheduler_step_size: 100\n  dann_factor: 2.0\n  lr: 0.01\n  optim_weight_decay: 0.01\ntrainer:\n  _target_: pytorch_lightning.Trainer\n  max_epochs: 200\n  gradient_clip_val: 1.0\n  callbacks:\n  - _target_: pytorch_lightning.callbacks.EarlyStopping\n    monitor: val/target/rmse/dataloader_idx_1\n    patience: 20\n  - _target_: pytorch_lightning.callbacks.ModelCheckpoint\n    save_top_k: 1\n    monitor: val/target/rmse/dataloader_idx_1\n    mode: min\n</pre> In\u00a0[7]: Copied! <pre>source = rul_datasets.CmapssReader(3)\ntarget = source.get_compatible(1, percent_broken=0.8)\ndm = rul_datasets.DomainAdaptionDataModule(\n    rul_datasets.RulDataModule(source, batch_size=32),\n    rul_datasets.RulDataModule(target, batch_size=32),\n)\n\nfeature_extractor = rul_adapt.model.LstmExtractor(\n    input_channels=14,\n    units=[16],\n    fc_units=8,\n)\nregressor = rul_adapt.model.FullyConnectedHead(\n    input_channels=8,\n    units=[8, 1],\n    act_func_on_last_layer=False,\n)\ndomain_disc = rul_adapt.model.FullyConnectedHead(\n    input_channels=8,\n    units=[8, 1],\n    act_func_on_last_layer=False,\n)\n\ndann = rul_adapt.approach.DannApproach(dann_factor=1.0, lr=0.001)\ndann.set_model(feature_extractor, regressor, domain_disc)\n\ntrainer = pl.Trainer(max_epochs=1)\n\ntrainer.fit(dann, dm)\ntrainer.test(dann, dm)\n</pre> source = rul_datasets.CmapssReader(3) target = source.get_compatible(1, percent_broken=0.8) dm = rul_datasets.DomainAdaptionDataModule(     rul_datasets.RulDataModule(source, batch_size=32),     rul_datasets.RulDataModule(target, batch_size=32), )  feature_extractor = rul_adapt.model.LstmExtractor(     input_channels=14,     units=[16],     fc_units=8, ) regressor = rul_adapt.model.FullyConnectedHead(     input_channels=8,     units=[8, 1],     act_func_on_last_layer=False, ) domain_disc = rul_adapt.model.FullyConnectedHead(     input_channels=8,     units=[8, 1],     act_func_on_last_layer=False, )  dann = rul_adapt.approach.DannApproach(dann_factor=1.0, lr=0.001) dann.set_model(feature_extractor, regressor, domain_disc)  trainer = pl.Trainer(max_epochs=1)  trainer.fit(dann, dm) trainer.test(dann, dm) <pre>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name               | Type                  | Params\n-------------------------------------------------------------\n0 | train_source_loss  | MeanAbsoluteError     | 0     \n1 | evaluator          | AdaptionEvaluator     | 0     \n2 | _feature_extractor | LstmExtractor         | 2.2 K \n3 | _regressor         | FullyConnectedHead    | 81    \n4 | dann_loss          | DomainAdversarialLoss | 81    \n-------------------------------------------------------------\n2.3 K     Trainable params\n0         Non-trainable params\n2.3 K     Total params\n0.009     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse        20.648313522338867\n    test/source/score          876.435546875\n    test/target/rmse                                 21.399911880493164\n    test/target/score                                1010.3373413085938\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[7]: <pre>[{'test/source/rmse/dataloader_idx_0': 20.648313522338867,\n  'test/source/score/dataloader_idx_0': 876.435546875},\n {'test/target/rmse/dataloader_idx_1': 21.399911880493164,\n  'test/target/score/dataloader_idx_1': 1010.3373413085938}]</pre>"},{"location":"examples/lstm_dann/#lstm-dann","title":"LSTM DANN\u00b6","text":""},{"location":"examples/lstm_dann/#reproduce-original-configurations","title":"Reproduce original configurations\u00b6","text":"<p>You can reproduce the original experiments of daCosta et al. by using the <code>get_lstm_dann</code> constructor function. Known differences to the original are:</p> <ul> <li>a bigger validation split (20% instead of 10% of training data).</li> </ul> <p>In this example, we re-create configuration for adaption CMAPSS FD003 to FD001. Additional <code>kwargs</code> for the trainer, e.g. <code>accelerator=\"gpu\"</code> for training on a GPU, can be passed to this function, too.</p>"},{"location":"examples/lstm_dann/#run-your-own-experiments","title":"Run your own experiments\u00b6","text":"<p>You can use the LSTM DANN implementation to run your own experiments with different hyperparameters or on different datasets. Here we build a smaller LSTM DANN version for CMAPSS.</p>"},{"location":"examples/pseudo_labels/","title":"Pseudo Label Approach","text":"In\u00a0[1]: Copied! <pre>from itertools import chain\n\nimport rul_adapt\nimport rul_datasets\nimport pytorch_lightning as pl\nimport torch\n</pre> from itertools import chain  import rul_adapt import rul_datasets import pytorch_lightning as pl import torch <p>The pseudo label approach works by training a supervised model on the source domain and then using the model to predict labels for the target domain. The target domain is then combined with the source domain and the model is retrained on the combined dataset. This process is repeated until the model converges.</p> <p>Here, we will train a model of FD003 of the CMAPSS dataset and pseudo label the FD001 dataset.</p> In\u00a0[2]: Copied! <pre>feature_extractor = rul_adapt.model.CnnExtractor(\n    14, [32, 16, 8], 30, fc_units=64\n)\nregressor = rul_adapt.model.FullyConnectedHead(\n    64, [1], act_func_on_last_layer=False\n)\n</pre> feature_extractor = rul_adapt.model.CnnExtractor(     14, [32, 16, 8], 30, fc_units=64 ) regressor = rul_adapt.model.FullyConnectedHead(     64, [1], act_func_on_last_layer=False ) In\u00a0[3]: Copied! <pre>fd3 = rul_datasets.CmapssReader(fd=3)\ndm_labeled = rul_datasets.RulDataModule(fd3, batch_size=128)\n</pre> fd3 = rul_datasets.CmapssReader(fd=3) dm_labeled = rul_datasets.RulDataModule(fd3, batch_size=128) <p>Then we set up a supervised approach and train it for 10 epochs. In practice, it should be trained until the validation loss stops decreasing.</p> In\u00a0[4]: Copied! <pre>approach = rul_adapt.approach.SupervisedApproach(\n    lr=0.001, loss_type=\"rmse\", optim_type=\"adam\"\n)\napproach.set_model(feature_extractor, regressor)\n\ntrainer = pl.Trainer(max_epochs=10)\ntrainer.fit(approach, dm_labeled)\ntrainer.validate(approach, dm_labeled)\n</pre> approach = rul_adapt.approach.SupervisedApproach(     lr=0.001, loss_type=\"rmse\", optim_type=\"adam\" ) approach.set_model(feature_extractor, regressor)  trainer = pl.Trainer(max_epochs=10) trainer.fit(approach, dm_labeled) trainer.validate(approach, dm_labeled) <pre>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name               | Type               | Params\n----------------------------------------------------------\n0 | train_loss         | MeanSquaredError   | 0     \n1 | val_loss           | MeanSquaredError   | 0     \n2 | test_loss          | MeanSquaredError   | 0     \n3 | evaluator          | AdaptionEvaluator  | 0     \n4 | _feature_extractor | CnnExtractor       | 15.7 K\n5 | _regressor         | FullyConnectedHead | 65    \n----------------------------------------------------------\n15.7 K    Trainable params\n0         Non-trainable params\n15.7 K    Total params\n0.063     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=10` reached.\n</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n     Validate metric           DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        val/loss            14.083422660827637\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[4]: <pre>[{'val/loss': 14.083422660827637}]</pre> In\u00a0[5]: Copied! <pre>fd1 = rul_datasets.CmapssReader(fd=1, percent_broken=0.8)\ndm_unlabeled = rul_datasets.RulDataModule(fd1, batch_size=128)\n</pre> fd1 = rul_datasets.CmapssReader(fd=1, percent_broken=0.8) dm_unlabeled = rul_datasets.RulDataModule(fd1, batch_size=128) <p>The pseudo label is generated for the last time step of each sequence. They may be implausible, e.g. less than zero, in the early iterations and need to be clipped. When patching the data module with the pseudo labels, a suitable RUL values for each sequence are created.</p> In\u00a0[6]: Copied! <pre>pseudo_labels = rul_adapt.approach.generate_pseudo_labels(dm_unlabeled, approach)\npseudo_labels = [max(0, pl) for pl in pseudo_labels]\nrul_adapt.approach.patch_pseudo_labels(dm_unlabeled, pseudo_labels)\n</pre> pseudo_labels = rul_adapt.approach.generate_pseudo_labels(dm_unlabeled, approach) pseudo_labels = [max(0, pl) for pl in pseudo_labels] rul_adapt.approach.patch_pseudo_labels(dm_unlabeled, pseudo_labels) <pre>/home/tilman/Programming/rul-adapt/rul_adapt/approach/pseudo_labels.py:88: UserWarning: At least one of the generated pseudo labels is negative. Please consider clipping them to zero.\n  warnings.warn(\n</pre> <p>We create a new trainer and validate our pre-trained approach on FD001 to get a baseline.</p> In\u00a0[7]: Copied! <pre>trainer = pl.Trainer(max_epochs=10)\ntrainer.validate(approach, dm_unlabeled)\n</pre> trainer = pl.Trainer(max_epochs=10) trainer.validate(approach, dm_unlabeled) <pre>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n     Validate metric           DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        val/loss            36.179779052734375\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[7]: <pre>[{'val/loss': 36.179779052734375}]</pre> <p>Afterward, we combine FD003 and the pseudo labeled FD001 and train the approach for another 10 epochs. We can observe that the validation loss decreased significantly. The pseudo labeling and training can now be repeated with the new model until the validation loss converges.</p> In\u00a0[8]: Copied! <pre>combined_train_data = torch.utils.data.ConcatDataset(\n    [dm_labeled.to_dataset(\"dev\"), dm_unlabeled.to_dataset(\"dev\")]\n)\ncombined_train_dl = torch.utils.data.DataLoader(\n    combined_train_data, batch_size=128, shuffle=True\n)\ntrainer.fit(approach, train_dataloaders=combined_train_dl)\ntrainer.validate(approach, dm_unlabeled)\n</pre> combined_train_data = torch.utils.data.ConcatDataset(     [dm_labeled.to_dataset(\"dev\"), dm_unlabeled.to_dataset(\"dev\")] ) combined_train_dl = torch.utils.data.DataLoader(     combined_train_data, batch_size=128, shuffle=True ) trainer.fit(approach, train_dataloaders=combined_train_dl) trainer.validate(approach, dm_unlabeled) <pre>/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n  rank_zero_warn(\n\n  | Name               | Type               | Params\n----------------------------------------------------------\n0 | train_loss         | MeanSquaredError   | 0     \n1 | val_loss           | MeanSquaredError   | 0     \n2 | test_loss          | MeanSquaredError   | 0     \n3 | evaluator          | AdaptionEvaluator  | 0     \n4 | _feature_extractor | CnnExtractor       | 15.7 K\n5 | _regressor         | FullyConnectedHead | 65    \n----------------------------------------------------------\n15.7 K    Trainable params\n0         Non-trainable params\n15.7 K    Total params\n0.063     Total estimated model params size (MB)\n</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=10` reached.\n</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n     Validate metric           DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        val/loss             29.42894172668457\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[8]: <pre>[{'val/loss': 29.42894172668457}]</pre>"},{"location":"examples/pseudo_labels/#pseudo-label-approach","title":"Pseudo Label Approach\u00b6","text":""},{"location":"examples/pseudo_labels/#supervised-training","title":"Supervised Training\u00b6","text":"<p>First we set up a data module for FD003.</p>"},{"location":"examples/pseudo_labels/#pseudo-labeling","title":"Pseudo Labeling\u00b6","text":"<p>Now we can use the trained model to generate labels for FD001. We truncate FD001 to 80% to simulate a target domain without failure data.</p>"},{"location":"examples/tbigru/","title":"TBiGRU","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport omegaconf\nimport pytorch_lightning as pl\nimport pywt\nimport rul_datasets\n\nimport rul_adapt\n</pre> import matplotlib.pyplot as plt import numpy as np import omegaconf import pytorch_lightning as pl import pywt import rul_datasets  import rul_adapt In\u00a0[2]: Copied! <pre>source = rul_datasets.FemtoReader(fd=1, run_split_dist={\"dev\": [1, 7]})\ntarget = rul_datasets.FemtoReader(fd=2, percent_broken=0.8,\n                                  run_split_dist={\"dev\": [1, 2]})\n\nfeature_idx = rul_adapt.approach.select_features(source, target, num_features=30)\nprint(sorted(feature_idx))\n</pre> source = rul_datasets.FemtoReader(fd=1, run_split_dist={\"dev\": [1, 7]}) target = rul_datasets.FemtoReader(fd=2, percent_broken=0.8,                                   run_split_dist={\"dev\": [1, 2]})  feature_idx = rul_adapt.approach.select_features(source, target, num_features=30) print(sorted(feature_idx)) <pre>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 20, 21, 22, 23, 32, 44, 48, 50, 52, 54, 56, 57, 58, 59]\n</pre> <p>These feature indices can be used for constructing a domain adaption data module with transferable features.</p> In\u00a0[3]: Copied! <pre>extractor = rul_adapt.approach.VibrationFeatureExtractor(num_input_features=2,\n                                                         feature_idx=feature_idx)\nextractor.fit(\n    source.load_split(\"dev\")[0] + target.load_split(\"dev\")[0])  # fit internal scaler\n\ndm = rul_datasets.DomainAdaptionDataModule(\n    rul_datasets.RulDataModule(source, 32, extractor, 20),\n    rul_datasets.RulDataModule(target, 32, extractor, 20),\n)\n</pre> extractor = rul_adapt.approach.VibrationFeatureExtractor(num_input_features=2,                                                          feature_idx=feature_idx) extractor.fit(     source.load_split(\"dev\")[0] + target.load_split(\"dev\")[0])  # fit internal scaler  dm = rul_datasets.DomainAdaptionDataModule(     rul_datasets.RulDataModule(source, 32, extractor, 20),     rul_datasets.RulDataModule(target, 32, extractor, 20), ) In\u00a0[4]: Copied! <pre>def wave_denoise(noisy, threshold, threshold_func, level, wave_fkt, axis=1):\n    \"\"\"\n    Denoise a signal with wavelets.\n\n    Args:\n        noisy: Noisy signal.\n        threshold: Threshold choosing method.\n        threshold_func: Thresholding method.\n        level: Number of decompositions.\n        wave_fkt: Descriptor of discrete wavelet function.\n        axis: Axis to denoise.\n    Returns:\n        Denoised signal.\n    \"\"\"\n    coeffs = pywt.wavedec(noisy, wave_fkt, level=level, axis=axis)\n    s = wnoisest(coeffs, level=level)\n    sigma_1 = s[level - 1]\n    th = choose_threshold(noisy.reshape(-1), threshold, sigma_1)\n    coeffsd = [coeffs[0]]\n    for i in range(0, level):\n        coeffsd.append(th_denoise(coeffs[1 + i], threshold_func, th))\n    denoised = pywt.waverec(coeffsd, wave_fkt, axis=axis)\n\n    return denoised\n\n\ndef choose_threshold(x, threshold_func, sigma_1):\n    if threshold_func == 'sqtwolog':\n        th = sqtwolog(x, sigma_1)\n    elif threshold_func == 'minimaxi':\n        th = minimaxi(x, sigma_1)\n    elif threshold_func == 'heursure':\n        th = heursure(x, sigma_1)\n    elif threshold_func == 'rigrsure':\n        th = rigrsure(x)\n    else:\n        raise ValueError(\n            f'Invalid value for threshold selection rule {threshold_func}.')\n\n    return th\n\n\ndef sqtwolog(x, sigma_1):\n    return np.sqrt(2 * np.log(len(x))) * sigma_1\n\n\ndef minimaxi(x, sigma_1):\n    l = len(x)\n    if l &lt; 32:\n        th = 0\n    else:\n        th = (0.3936 + 0.1829) * np.log2(l) * sigma_1\n\n    return th\n\n\ndef heursure(x, sigma_1):\n    l = len(x)\n    hth = sqtwolog(x, sigma_1)\n    normsqr = np.dot(x, x)\n    eta = (normsqr ** 2 - l) / l\n    crit = (np.log2(l) ** 1.5) / np.sqrt(l)\n    if eta &lt; crit:\n        th = hth\n    else:\n        sx2 = x ** 2\n        sx2 = np.sort(sx2)\n        cumsumsx2 = np.cumsum(sx2)\n        risks = []\n        for i in range(0, l):\n            risks.append(\n                (l - 2 * (i + 1) + (cumsumsx2[i] + (l - 1 - i) * sx2[i])) / l)\n        mini = np.argmin(risks)\n\n        rth = np.sqrt(sx2[mini])\n        th = min(hth, rth)\n\n    return th\n\n\ndef rigrsure(x):\n    l = len(x)\n    a = np.sort(np.abs(x)) ** 2\n    c = np.linspace(l - 1, 0, l)\n    s = np.cumsum(a) + c * a\n    risk = (l - (2 * np.arange(l)) + s) / l\n    ibest = np.argmin(risk)\n    th = np.sqrt(a[ibest])\n\n    return th\n\n\ndef th_denoise(x, thfkt, th):\n    if thfkt == 'hard':\n        idx = np.abs(x) &gt;= th\n        x[idx] = 0\n    elif thfkt == 'soft':\n        idx = np.abs(x) &gt;= th\n        x[idx] = np.sign(x[idx]) * (np.abs(x[idx]) - th)\n        x[~idx] = 0\n    elif thfkt == 'firm':\n        i = np.abs(x) &lt; th\n        x[i] = np.sign(x[i]) * (np.abs(x[i] ** 9) / th ** 8)\n    elif thfkt == 'exp':\n        i = np.abs(x) &gt; th\n        x[i] = np.sign(x[i]) * (np.abs(x[i]) - 1.5 ** (th - np.abs(x[i]))) * th\n        x[~i] = 0\n    else:\n        raise ValueError(f'Invalid value for thresholding type {thfkt}')\n\n    return x\n\n\ndef wnoisest(coeffs, level=None):\n    if level is None:\n        sig = np.abs(coeffs[-1])\n        stdc = [np.median(sig) / 0.6745]\n    else:\n        stdc = [np.median(np.abs(c)) / 0.6745 for c in coeffs[1:]]\n\n    return stdc\n</pre> def wave_denoise(noisy, threshold, threshold_func, level, wave_fkt, axis=1):     \"\"\"     Denoise a signal with wavelets.      Args:         noisy: Noisy signal.         threshold: Threshold choosing method.         threshold_func: Thresholding method.         level: Number of decompositions.         wave_fkt: Descriptor of discrete wavelet function.         axis: Axis to denoise.     Returns:         Denoised signal.     \"\"\"     coeffs = pywt.wavedec(noisy, wave_fkt, level=level, axis=axis)     s = wnoisest(coeffs, level=level)     sigma_1 = s[level - 1]     th = choose_threshold(noisy.reshape(-1), threshold, sigma_1)     coeffsd = [coeffs[0]]     for i in range(0, level):         coeffsd.append(th_denoise(coeffs[1 + i], threshold_func, th))     denoised = pywt.waverec(coeffsd, wave_fkt, axis=axis)      return denoised   def choose_threshold(x, threshold_func, sigma_1):     if threshold_func == 'sqtwolog':         th = sqtwolog(x, sigma_1)     elif threshold_func == 'minimaxi':         th = minimaxi(x, sigma_1)     elif threshold_func == 'heursure':         th = heursure(x, sigma_1)     elif threshold_func == 'rigrsure':         th = rigrsure(x)     else:         raise ValueError(             f'Invalid value for threshold selection rule {threshold_func}.')      return th   def sqtwolog(x, sigma_1):     return np.sqrt(2 * np.log(len(x))) * sigma_1   def minimaxi(x, sigma_1):     l = len(x)     if l &lt; 32:         th = 0     else:         th = (0.3936 + 0.1829) * np.log2(l) * sigma_1      return th   def heursure(x, sigma_1):     l = len(x)     hth = sqtwolog(x, sigma_1)     normsqr = np.dot(x, x)     eta = (normsqr ** 2 - l) / l     crit = (np.log2(l) ** 1.5) / np.sqrt(l)     if eta &lt; crit:         th = hth     else:         sx2 = x ** 2         sx2 = np.sort(sx2)         cumsumsx2 = np.cumsum(sx2)         risks = []         for i in range(0, l):             risks.append(                 (l - 2 * (i + 1) + (cumsumsx2[i] + (l - 1 - i) * sx2[i])) / l)         mini = np.argmin(risks)          rth = np.sqrt(sx2[mini])         th = min(hth, rth)      return th   def rigrsure(x):     l = len(x)     a = np.sort(np.abs(x)) ** 2     c = np.linspace(l - 1, 0, l)     s = np.cumsum(a) + c * a     risk = (l - (2 * np.arange(l)) + s) / l     ibest = np.argmin(risk)     th = np.sqrt(a[ibest])      return th   def th_denoise(x, thfkt, th):     if thfkt == 'hard':         idx = np.abs(x) &gt;= th         x[idx] = 0     elif thfkt == 'soft':         idx = np.abs(x) &gt;= th         x[idx] = np.sign(x[idx]) * (np.abs(x[idx]) - th)         x[~idx] = 0     elif thfkt == 'firm':         i = np.abs(x) &lt; th         x[i] = np.sign(x[i]) * (np.abs(x[i] ** 9) / th ** 8)     elif thfkt == 'exp':         i = np.abs(x) &gt; th         x[i] = np.sign(x[i]) * (np.abs(x[i]) - 1.5 ** (th - np.abs(x[i]))) * th         x[~i] = 0     else:         raise ValueError(f'Invalid value for thresholding type {thfkt}')      return x   def wnoisest(coeffs, level=None):     if level is None:         sig = np.abs(coeffs[-1])         stdc = [np.median(sig) / 0.6745]     else:         stdc = [np.median(np.abs(c)) / 0.6745 for c in coeffs[1:]]      return stdc In\u00a0[5]: Copied! <pre>runs, _ = source.load_split(\"dev\")\nbearing_1_1 = runs[0][:, :, 0, None]\nbearing_1_1 = wave_denoise(bearing_1_1, \"rigrsure\", \"exp\", 4, \"dmey\", axis=1)\nrms = rul_adapt.approach.tbigru.rms(bearing_1_1)\nmac = rul_adapt.approach.tbigru.mac(bearing_1_1, 100, \"haar\")\n</pre> runs, _ = source.load_split(\"dev\") bearing_1_1 = runs[0][:, :, 0, None] bearing_1_1 = wave_denoise(bearing_1_1, \"rigrsure\", \"exp\", 4, \"dmey\", axis=1) rms = rul_adapt.approach.tbigru.rms(bearing_1_1) mac = rul_adapt.approach.tbigru.mac(bearing_1_1, 100, \"haar\") In\u00a0[6]: Copied! <pre>def smooth(inputs, alpha):\n    \"\"\"Produce smoothed version of input signal.\"\"\"\n    filtered = np.empty_like(inputs)\n    curr = inputs[0]\n    for i, update in enumerate(inputs):\n        curr = alpha * curr + (1 - alpha) * update\n        filtered[i] = curr\n\n    return filtered\n</pre> def smooth(inputs, alpha):     \"\"\"Produce smoothed version of input signal.\"\"\"     filtered = np.empty_like(inputs)     curr = inputs[0]     for i, update in enumerate(inputs):         curr = alpha * curr + (1 - alpha) * update         filtered[i] = curr      return filtered <p>The MAC should first increase to near 1 and then dip down for a brief period. This marks the end of the running-in state. The following steady working state ends after a second brief dip. The end of the steady working state is considered the FTTP.</p> In\u00a0[7]: Copied! <pre>plt.plot(mac)\nplt.plot(smooth(mac, 0.8), c=\"tab:orange\")\nplt.xlabel(\"Time in s\")\nplt.ylabel(\"MAC\")\nplt.ylim((0, 1.2))\nax = plt.twinx(plt.gca())\nax.plot(rms[:, 0], c=\"tab:green\")\nax.set_ylabel(\"RMS\")\nplt.show()\n</pre> plt.plot(mac) plt.plot(smooth(mac, 0.8), c=\"tab:orange\") plt.xlabel(\"Time in s\") plt.ylabel(\"MAC\") plt.ylim((0, 1.2)) ax = plt.twinx(plt.gca()) ax.plot(rms[:, 0], c=\"tab:green\") ax.set_ylabel(\"RMS\") plt.show() <p>We can see the raw and smoothed MAC, as well as the RMS. The plot shows a first dip around 300s and a second dip around 1200s, which would be the first-time-to-predict. Unfortunately, our implementation does not exactly line up with the original results which is why our FTTPs should be regarded with caution.</p> In\u00a0[8]: Copied! <pre>pl.seed_everything(42, workers=True)  # make reproducible\ndm, tbigru, trainer = rul_adapt.construct.get_tbigru(1, 2, max_epochs=1)\n</pre> pl.seed_everything(42, workers=True)  # make reproducible dm, tbigru, trainer = rul_adapt.construct.get_tbigru(1, 2, max_epochs=1) <pre>Global seed set to 42\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</pre> <p>The networks, <code>feature_extractor</code> and <code>regressor</code>, can be accessed as properties of the <code>tbigru</code> object.</p> In\u00a0[9]: Copied! <pre>tbigru.feature_extractor\n</pre> tbigru.feature_extractor Out[9]: <pre>GruExtractor(\n  (_fc_layer): Sequential(\n    (0): Conv1d(30, 15, kernel_size=(1,), stride=(1,))\n    (1): ReLU()\n    (2): Conv1d(15, 5, kernel_size=(1,), stride=(1,))\n    (3): ReLU()\n  )\n  (_gru_layers): GRU(5, 5, bidirectional=True)\n)</pre> <p>Training is done in the PyTorch Lightning fashion. We used the <code>trainer_kwargs</code> to train only one epoch for demonstration purposes.</p> In\u00a0[10]: Copied! <pre>trainer.fit(tbigru, dm)\ntrainer.test(tbigru, dm)\n</pre> trainer.fit(tbigru, dm) trainer.test(tbigru, dm) <pre>\n  | Name               | Type                       | Params\n------------------------------------------------------------------\n0 | train_source_loss  | MeanSquaredError           | 0     \n1 | mmd_loss           | MaximumMeanDiscrepancyLoss | 0     \n2 | evaluator          | AdaptionEvaluator          | 0     \n3 | _feature_extractor | GruExtractor               | 905   \n4 | _regressor         | FullyConnectedHead         | 11    \n------------------------------------------------------------------\n916       Trainable params\n0         Non-trainable params\n916       Total params\n0.004     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>/home/tilman/Programming/rul-adapt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (34) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse        0.6547839045524597\n    test/source/score       0.02461443468928337\n    test/target/rmse                                 0.6114419102668762\n    test/target/score                                0.0323663093149662\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[10]: <pre>[{'test/source/rmse/dataloader_idx_0': 0.6547839045524597,\n  'test/source/score/dataloader_idx_0': 0.02461443468928337},\n {'test/target/rmse/dataloader_idx_1': 0.6114419102668762,\n  'test/target/score/dataloader_idx_1': 0.0323663093149662}]</pre> <p>If you only want to see the hyperparameters, you can use the <code>get_tbigru_config</code> function. This returns an <code>omegeconf.DictConfig</code> which you can modify.</p> In\u00a0[11]: Copied! <pre>one2two_config = rul_adapt.construct.get_tbigru_config(1, 2)\nprint(omegaconf.OmegaConf.to_yaml(one2two_config, resolve=True))\n</pre> one2two_config = rul_adapt.construct.get_tbigru_config(1, 2) print(omegaconf.OmegaConf.to_yaml(one2two_config, resolve=True)) <pre>dm:\n  source:\n    _target_: rul_datasets.FemtoReader\n    fd: 1\n    run_split_dist:\n      dev:\n      - 1\n      - 7\n      val:\n      - 2\n      - 3\n      test:\n      - 6\n    first_time_to_predict:\n    - 1200\n    - 140\n    - 1050\n    - 990\n    - 2300\n    - 1480\n    - 1930\n    norm_rul: true\n  target:\n    _target_: rul_datasets.FemtoReader\n    fd: 2\n    percent_broken: 1.0\n    run_split_dist:\n      dev:\n      - 1\n      - 2\n      val:\n      - 3\n      - 4\n      test:\n      - 6\n    first_time_to_predict:\n    - 160\n    - 210\n    - 450\n    - 200\n    - 530\n    - 340\n    - 40\n    norm_rul: true\n  batch_size: 150\n  feature_extractor:\n    _target_: rul_adapt.approach.tbigru.VibrationFeatureExtractor\n    num_input_features: 2\n    feature_idx:\n    - 0\n    - 1\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    - 9\n    - 10\n    - 11\n    - 12\n    - 13\n    - 16\n    - 17\n    - 20\n    - 21\n    - 22\n    - 23\n    - 40\n    - 41\n    - 43\n    - 44\n    - 48\n    - 50\n    - 52\n    - 54\n    - 56\n    - 57\n    - 58\n    - 59\n  window_size: 20\nfeature_extractor:\n  _convert_: all\n  _target_: rul_adapt.model.GruExtractor\n  input_channels: 30\n  fc_units:\n  - 15\n  - 5\n  gru_units:\n  - 5\n  bidirectional: true\nregressor:\n  _convert_: all\n  _target_: rul_adapt.model.FullyConnectedHead\n  input_channels: 10\n  act_func_on_last_layer: false\n  units:\n  - 1\ndomain_disc:\n  _convert_: all\n  _target_: rul_adapt.model.FullyConnectedHead\n  input_channels: 10\n  act_func_on_last_layer: false\n  units:\n  - 1\ntbigru:\n  _target_: rul_adapt.approach.MmdApproach\n  lr: 0.001\n  mmd_factor: 0.1\n  rul_score_mode: phm12\ntrainer:\n  _target_: pytorch_lightning.Trainer\n  max_epochs: 5000\n</pre> In\u00a0[13]: Copied! <pre>source = rul_datasets.FemtoReader(fd=1, norm_rul=True)\ntarget = rul_datasets.FemtoReader(fd=2, percent_broken=0.8, norm_rul=True)\nsource.prepare_data()\ntarget.prepare_data()\n\nextractor = rul_adapt.approach.VibrationFeatureExtractor(num_input_features=2,\n                                                         feature_idx=[0, 1, 2, 3, 4])\nextractor.fit(source.load_split(\"dev\")[0] + target.load_split(\"dev\")[0])\n\ndm = rul_datasets.DomainAdaptionDataModule(\n    rul_datasets.RulDataModule(source, 32, extractor, window_size=20),\n    rul_datasets.RulDataModule(target, 32, extractor, window_size=20),\n)\n\nfeature_extractor = rul_adapt.model.CnnExtractor(\n    input_channels=5,\n    units=[16, 8],\n    seq_len=20,\n    fc_units=16,\n)\nregressor = rul_adapt.model.FullyConnectedHead(\n    input_channels=16,\n    units=[1],\n    act_func_on_last_layer=False,\n)\n\ntbigru = rul_adapt.approach.MmdApproach(lr=0.001, mmd_factor=0.1)\ntbigru.set_model(feature_extractor, regressor)\n\ntrainer = pl.Trainer(max_epochs=1)\n\ntrainer.fit(tbigru, dm)\ntrainer.test(tbigru, dm)\n</pre> source = rul_datasets.FemtoReader(fd=1, norm_rul=True) target = rul_datasets.FemtoReader(fd=2, percent_broken=0.8, norm_rul=True) source.prepare_data() target.prepare_data()  extractor = rul_adapt.approach.VibrationFeatureExtractor(num_input_features=2,                                                          feature_idx=[0, 1, 2, 3, 4]) extractor.fit(source.load_split(\"dev\")[0] + target.load_split(\"dev\")[0])  dm = rul_datasets.DomainAdaptionDataModule(     rul_datasets.RulDataModule(source, 32, extractor, window_size=20),     rul_datasets.RulDataModule(target, 32, extractor, window_size=20), )  feature_extractor = rul_adapt.model.CnnExtractor(     input_channels=5,     units=[16, 8],     seq_len=20,     fc_units=16, ) regressor = rul_adapt.model.FullyConnectedHead(     input_channels=16,     units=[1],     act_func_on_last_layer=False, )  tbigru = rul_adapt.approach.MmdApproach(lr=0.001, mmd_factor=0.1) tbigru.set_model(feature_extractor, regressor)  trainer = pl.Trainer(max_epochs=1)  trainer.fit(tbigru, dm) trainer.test(tbigru, dm) <pre>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name               | Type                       | Params\n------------------------------------------------------------------\n0 | train_source_loss  | MeanSquaredError           | 0     \n1 | mmd_loss           | MaximumMeanDiscrepancyLoss | 0     \n2 | evaluator          | AdaptionEvaluator          | 0     \n3 | _feature_extractor | CnnExtractor               | 2.7 K \n4 | _regressor         | FullyConnectedHead         | 17    \n------------------------------------------------------------------\n2.7 K     Trainable params\n0         Non-trainable params\n2.7 K     Total params\n0.011     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Testing: 0it [00:00, ?it/s]</pre> <pre>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0             DataLoader 1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    test/source/rmse        0.2802481949329376\n    test/source/score       184.27552795410156\n    test/target/rmse                                 0.2827073335647583\n    test/target/score                                 86.12316131591797\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</pre> Out[13]: <pre>[{'test/source/rmse/dataloader_idx_0': 0.2802481949329376,\n  'test/source/score/dataloader_idx_0': 184.27552795410156},\n {'test/target/rmse/dataloader_idx_1': 0.2827073335647583,\n  'test/target/score/dataloader_idx_1': 86.12316131591797}]</pre>"},{"location":"examples/tbigru/#tbigru","title":"TBiGRU\u00b6","text":""},{"location":"examples/tbigru/#training-preparations","title":"Training preparations\u00b6","text":"<p>Before training, the TBiGRU approach has a feature mining stage and a bearing running state stage that need to be completed. The running state detection is used to determine the first-time-to-predict (FTTP) which is needed to appropriately generate the RUL targets. The RUL Adapt library implements both stages but adds their results to the training configurations for easier an easier replication process (see Reproduce original configurations below).</p>"},{"location":"examples/tbigru/#feature-selection","title":"Feature selection\u00b6","text":"<p>To run the feature selection stage, create a source and target domain reader and feed them to the <code>select_features</code> function. For each input feature the function extracts 30 different features and returns the indices of the top transferable features. We select the appropriate runs as training splits and receive the 30 most transferable features out of 60.</p>"},{"location":"examples/tbigru/#running-state-detection","title":"Running state detection\u00b6","text":"<p>To set the first-time-to-predict (FTTP) for each bearing, the running states must be detected. This approach uses the moving average correlation (MAC) between energy entropies of signals decomposed by maximal overlap wavelet decomposition in a sliding window. We demonstrate this process on Bearing_1_1.</p>"},{"location":"examples/tbigru/#reproduce-original-configurations","title":"Reproduce original configurations\u00b6","text":"<p>You can reproduce the original experiments of Cao et al. by using the <code>get_tbigru</code> constructor function. Known differences to the original paper:</p> <ul> <li>The feature selection is carried out separately for each transfer task instead of jointly for all sub-datasets. This is because the target data should be truncated, as noted above. Therefore, a different sub-dataset needs to be truncated for the feature selection for each task, too.</li> <li>The first-time-to-predict is likely not correct as the process of determining the running state of the bearing could not be recreated faithfully yet.</li> </ul> <p>In this example, we re-create configuration for adaption FEMTO FD001 to FD002. Additional kwargs for the trainer, e.g. accelerator=\"gpu\" for training on a GPU, can be passed to this function, too.</p>"},{"location":"examples/tbigru/#run-your-own-experiments","title":"Run your own experiments\u00b6","text":"<p>You can use the TBiGRU implementation to run your own experiments with different hyperparameters or on different datasets. Here we use a CNN extractor instead of a BiGRU.</p>"}]}